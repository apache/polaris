[{"body":"This guide serves as an introduction to several key entities that can be managed with Apache Polaris (Incubating), describes how to build and deploy Polaris locally, and finally includes examples of how to use Polaris with Apache Spark™.\nPrerequisites This guide covers building Polaris, deploying it locally or via Docker, and interacting with it using the command-line interface and Apache Spark. Before proceeding with Polaris, be sure to satisfy the relevant prerequisites listed here.\nGit To get the latest Polaris code, you’ll need to clone the repository using git. You can install git using homebrew on MacOS:\nbrew install git Please follow instructions from the Git Documentation for instructions on installing Git on other platforms.\nThen, use git to clone the Polaris repo:\ngit clone https://github.com/apache/polaris.git ~/polaris Docker It is recommended to deploy Polaris inside Docker for the Quickstart workflow. Instructions for deploying the Quickstart workflow on the supported Cloud Providers (AWS, Azure, GCP) will be provided only with Docker. However, non-Docker deployment instructions for local deployments can also be followed on Cloud Providers.\nInstructions to install Docker can be found on the Docker website. Ensure that Docker and the Docker Compose plugin are both installed.\nDocker on MacOS Docker can be installed using homebrew:\nbrew install --cask docker There could be a Docker permission issues related to seccomp configuration. To resolve these issues, set the seccomp profile to “unconfined” when running a container. For example:\ndocker run --security-opt seccomp=unconfined apache/polaris:latest Note: Setting the seccomp profile to “unconfined” disables the default system call filtering, which may pose security risks. Use this configuration with caution, especially in production environments.\nDocker on Amazon Linux Docker can be installed using a modification to the CentOS instructions. For example:\nsudo dnf update -y # Remove old version sudo dnf remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # Install dnf plugin sudo dnf -y install dnf-plugins-core # Add CentOS repository sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # Adjust release server version in the path as it will not match with Amazon Linux 2023 sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/docker-ce.repo # Install as usual sudo dnf -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Confirm Docker Installation Once installed, make sure that both Docker and the Docker Compose plugin are installed:\ndocker version docker compose version Also make sure Docker is running and is able to run a sample Docker container:\ndocker run hello-world Java If you plan to build Polaris from source yourself or using this tutorial’s instructions on a Cloud Provider, you will need to satisfy a few prerequisites first.\nPolaris is built using gradle and is compatible with Java 21. We recommend the use of jenv to manage multiple Java versions. For example, to install Java 21 via homebrew and configure it with jenv:\ncd ~/polaris brew install openjdk@21 jenv jenv add $(brew --prefix openjdk@21) jenv local 21 Ensure that java --version and javac both return non-zero responses.\njq Most Polaris Quickstart scripts require jq. You can install jq using homebrew:\nbrew install jq ","categories":"","description":"","excerpt":"This guide serves as an introduction to several key entities that can …","ref":"/in-dev/unreleased/getting-started/install-dependencies/","tags":"","title":"Installing Dependencies"},{"body":"This guide serves as an introduction to several key entities that can be managed with Apache Polaris (Incubating), describes how to build and deploy Polaris locally, and finally includes examples of how to use Polaris with Apache Spark™.\nPrerequisites This guide covers building Polaris, deploying it locally or via Docker, and interacting with it using the command-line interface and Apache Spark. Before proceeding with Polaris, be sure to satisfy the relevant prerequisites listed here.\nGit To get the latest Polaris code, you’ll need to clone the repository using git. You can install git using homebrew on MacOS:\nbrew install git Please follow instructions from the Git Documentation for instructions on installing Git on other platforms.\nThen, use git to clone the Polaris repo:\ncd ~ git clone https://github.com/apache/polaris.git Docker It is recommended to deploy Polaris inside Docker for the Quickstart workflow. Instructions for deploying the Quickstart workflow on the supported Cloud Providers (AWS, Azure, GCP) will be provided only with Docker. However, non-Docker deployment instructions for local deployments can also be followed on Cloud Providers.\nInstructions to install Docker can be found on the Docker website. Ensure that Docker and the Docker Compose plugin are both installed.\nDocker on MacOS Docker can be installed using homebrew:\nbrew install --cask docker There could be a Docker permission issues related to seccomp configuration. To resolve these issues, set the seccomp profile to “unconfined” when running a container. For example:\ndocker run --security-opt seccomp=unconfined apache/polaris:latest Note: Setting the seccomp profile to “unconfined” disables the default system call filtering, which may pose security risks. Use this configuration with caution, especially in production environments.\nDocker on Amazon Linux Docker can be installed using a modification to the CentOS instructions. For example:\nsudo dnf update -y # Remove old version sudo dnf remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # Install dnf plugin sudo dnf -y install dnf-plugins-core # Add CentOS repository sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # Adjust release server version in the path as it will not match with Amazon Linux 2023 sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/docker-ce.repo # Install as usual sudo dnf -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Confirm Docker Installation Once installed, make sure that both Docker and the Docker Compose plugin are installed:\ndocker version docker compose version Also make sure Docker is running and is able to run a sample Docker container:\ndocker run hello-world Java If you plan to build Polaris from source yourself or using this tutorial’s instructions on a Cloud Provider, you will need to satisfy a few prerequisites first.\nPolaris is built using gradle and is compatible with Java 21. We recommend the use of jenv to manage multiple Java versions. For example, to install Java 21 via homebrew and configure it with jenv:\ncd ~/polaris brew install openjdk@21 jenv jenv add $(brew --prefix openjdk@21) jenv local 21 Ensure that java --version and javac both return non-zero responses.\njq Most Polaris Quickstart scripts require jq. Follow the instructions from the jq website to download this tool.\n","categories":"","description":"","excerpt":"This guide serves as an introduction to several key entities that can …","ref":"/releases/1.0.0/getting-started/install-dependencies/","tags":"","title":"Installing Dependencies"},{"body":"This guide serves as an introduction to several key entities that can be managed with Apache Polaris (Incubating), describes how to build and deploy Polaris locally, and finally includes examples of how to use Polaris with Apache Spark™.\nPrerequisites This guide covers building Polaris, deploying it locally or via Docker, and interacting with it using the command-line interface and Apache Spark. Before proceeding with Polaris, be sure to satisfy the relevant prerequisites listed here.\nGit To get the latest Polaris code, you’ll need to clone the repository using git. You can install git using homebrew on MacOS:\nbrew install git Please follow instructions from the Git Documentation for instructions on installing Git on other platforms.\nThen, use git to clone the Polaris repo:\ncd ~ git clone https://github.com/apache/polaris.git Docker It is recommended to deploy Polaris inside Docker for the Quickstart workflow. Instructions for deploying the Quickstart workflow on the supported Cloud Providers (AWS, Azure, GCP) will be provided only with Docker. However, non-Docker deployment instructions for local deployments can also be followed on Cloud Providers.\nInstructions to install Docker can be found on the Docker website. Ensure that Docker and the Docker Compose plugin are both installed.\nDocker on MacOS Docker can be installed using homebrew:\nbrew install --cask docker There could be a Docker permission issues related to seccomp configuration. To resolve these issues, set the seccomp profile to “unconfined” when running a container. For example:\ndocker run --security-opt seccomp=unconfined apache/polaris:latest Note: Setting the seccomp profile to “unconfined” disables the default system call filtering, which may pose security risks. Use this configuration with caution, especially in production environments.\nDocker on Amazon Linux Docker can be installed using a modification to the CentOS instructions. For example:\nsudo dnf update -y # Remove old version sudo dnf remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # Install dnf plugin sudo dnf -y install dnf-plugins-core # Add CentOS repository sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # Adjust release server version in the path as it will not match with Amazon Linux 2023 sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/docker-ce.repo # Install as usual sudo dnf -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Confirm Docker Installation Once installed, make sure that both Docker and the Docker Compose plugin are installed:\ndocker version docker compose version Also make sure Docker is running and is able to run a sample Docker container:\ndocker run hello-world Java If you plan to build Polaris from source yourself or using this tutorial’s instructions on a Cloud Provider, you will need to satisfy a few prerequisites first.\nPolaris is built using gradle and is compatible with Java 21. We recommend the use of jenv to manage multiple Java versions. For example, to install Java 21 via homebrew and configure it with jenv:\ncd ~/polaris brew install openjdk@21 jenv jenv add $(brew --prefix openjdk@21) jenv local 21 Ensure that java --version and javac both return non-zero responses.\njq Most Polaris Quickstart scripts require jq. Follow the instructions from the jq website to download this tool.\n","categories":"","description":"","excerpt":"This guide serves as an introduction to several key entities that can …","ref":"/releases/1.0.1/getting-started/install-dependencies/","tags":"","title":"Installing Dependencies"},{"body":" Upcoming Meetings We have bi-weekly community meeting using Google Meet.\nJoin Polaris Community Sync invite.\nScheduled Meetings Agenda for upcoming Community Meetings\nGoogle Meet Link\nDate Time 2025-08-28\n9:00 AM PST\n18:00 CET\n2025-09-04\n9:00 AM PST\n18:00 CET\n2025-09-18\n9:00 AM PST\n18:00 CET\n2025-10-02\n9:00 AM PST\n18:00 CET\n2025-10-16\n9:00 AM PST\n18:00 CET\n…​\nPast Meetings Date Notes Recording 2025-07-24\nMeeting Notes\nhttps://drive.google.com/file/d/1MZfz_yr1y20BbZf6MSqFidPpSNfBKEzr/view?usp=sharing\n2025-06-26\nMeeting Notes\nhttps://drive.google.com/file/d/15xPXCGVSKmfc5HWI4CYbfiBC5HnUtR16/view?usp=sharing\n2025-06-12\nMeeting Notes\nhttps://drive.google.com/file/d/1QjMpC87ML6kH4EC2Ni5j29W71yBsHigo/view?usp=sharing\n2025-04-17\nMeeting Notes\nhttps://drive.google.com/file/d/1EOxaC7kia7tzvvM0Tlkz8foX6HAWPm_f/view?usp=sharing\n2025-04-03\nMeeting Notes\nhttps://drive.google.com/file/d/1fiiusq6_0De42Mo3HZw8V84SG_W3RmGT/view?usp=sharing\n2025-03-20\nMeeting Notes\nhttps://drive.google.com/file/d/1O2EO7ekFUnfpk2OY7yzP3WybNQ-hG5Zv/view?usp=sharing\n2025-03-06\nMeeting Notes\nhttps://drive.google.com/file/d/1Wopf6oUboprb0wtFTvO-R0ul-EtYC6kH/view?usp=sharing\n2025-02-20\nMeeting Notes\nhttps://drive.google.com/file/d/1HZaGnQV-MhFGDGjgmk9JFDeNq0899AEo/view?usp=sharing\n2025-02-07\nMeeting Notes\nhttps://drive.google.com/file/d/1pRfyRpQGjWglEg8OTanBz2UQAO5ObFOl/view?usp=sharing\n2025-01-23\nMeeting Notes\nhttps://drive.google.com/file/d/1AXy-WkUNP4Fo73ijXYFDk3cRFKiBQ-XI/view?usp=sharing\n2025-01-09\nMeeting Notes\nhttps://drive.google.com/file/d/1p1OFXwIiBXo_qiQoP_T-qNn9tkBXkq_p/view?usp=sharing\n2024-12-12\nMeeting Notes\nhttps://drive.google.com/file/d/1OJiOnn9otN36tgibTMk73wSl01wEak9M/view?usp=sharing\n2024-11-14\nMeeting Notes\nhttps://drive.google.com/file/d/1a2B5c0hychdRuIcNSl2ltEkoH3VcR0J1/view?usp=sharing\n2024-10-31\nMeeting Notes\nhttps://drive.google.com/file/d/1yZkcs8iif2QOFqWhr6rWOKyGJAoIR8aX/view?usp=drive_link\n2024-10-17\nMeeting Notes\n(no recording)\n","categories":"","description":"","excerpt":" Upcoming Meetings We have bi-weekly community meeting using Google …","ref":"/community/meetings/","tags":"","title":"Apache Polaris Community Meetings"},{"body":"This guide serves as a introduction to several key entities that can be managed with Apache Polaris (Incubating), describes how to build and deploy Polaris locally, and finally includes examples of how to use Polaris with Apache Spark™.\nPrerequisites This guide covers building Polaris, deploying it locally or via Docker, and interacting with it using the command-line interface and Apache Spark. Before proceeding with Polaris, be sure to satisfy the relevant prerequisites listed here.\nBuilding and Deploying Polaris To get the latest Polaris code, you’ll need to clone the repository using git. You can install git using homebrew:\nbrew install git Then, use git to clone the Polaris repo:\ncd ~ git clone https://github.com/apache/polaris.git With Docker If you plan to deploy Polaris inside Docker, you’ll need to install docker itself. For example, this can be done using homebrew:\nbrew install --cask docker Once installed, make sure Docker is running.\nFrom Source If you plan to build Polaris from source yourself, you will need to satisfy a few prerequisites first.\nPolaris is built using gradle and is compatible with Java 21. We recommend the use of jenv to manage multiple Java versions. For example, to install Java 21 via homebrew and configure it with jenv:\ncd ~/polaris brew install openjdk@21 jenv jenv add $(brew --prefix openjdk@21) jenv local 21 Connecting to Polaris Polaris is compatible with any Apache Iceberg client that supports the REST API. Depending on the client you plan to use, refer to the prerequisites below.\nWith Spark If you want to connect to Polaris with Apache Spark, you’ll need to start by cloning Spark. As above, make sure git is installed first. You can install it with homebrew:\nbrew install git Then, clone Spark and check out a versioned branch. This guide uses Spark 3.5.\ncd ~ git clone https://github.com/apache/spark.git cd ~/spark git checkout branch-3.5 Deploying Polaris Polaris can be deployed via a lightweight docker image or as a standalone process. Before starting, be sure that you’ve satisfied the relevant prerequisites detailed above.\nDocker Image To start using Polaris in Docker, launch Polaris while Docker is running:\ncd ~/polaris docker compose -f docker-compose.yml up --build Once the polaris-polaris container is up, you can continue to Defining a Catalog.\nBuilding Polaris Run Polaris locally with:\ncd ~/polaris ./gradlew runApp You should see output for some time as Polaris builds and starts up. Eventually, you won’t see any more logs and should see messages that resemble the following:\nINFO [...] [main] [] o.e.j.s.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@... INFO [...] [main] [] o.e.j.server.AbstractConnector: Started application@... INFO [...] [main] [] o.e.j.server.AbstractConnector: Started admin@... INFO [...] [main] [] o.eclipse.jetty.server.Server: Started Server@... At this point, Polaris is running.\nBootstrapping Polaris For this tutorial, we’ll launch an instance of Polaris that stores entities only in-memory. This means that any entities that you define will be destroyed when Polaris is shut down. It also means that Polaris will automatically bootstrap itself with root credentials. For more information on how to configure Polaris for production usage, see the docs.\nWhen Polaris is launched using in-memory mode the root principal credentials can be found in stdout on initial startup. For example:\nrealm: default-realm root principal credentials: \u003cclient-id\u003e:\u003cclient-secret\u003e Be sure to note of these credentials as we’ll be using them below. You can also set these credentials as environment variables for use with the Polaris CLI:\nexport CLIENT_ID=\u003cclient-id\u003e export CLIENT_SECRET=\u003cclient-secret\u003e Defining a Catalog In Polaris, the catalog is the top-level entity that objects like tables and views are organized under. With a Polaris service running, you can create a catalog like so:\ncd ~/polaris ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalogs \\ create \\ --storage-type s3 \\ --default-base-location ${DEFAULT_BASE_LOCATION} \\ --role-arn ${ROLE_ARN} \\ quickstart_catalog This will create a new catalog called quickstart_catalog.\nThe DEFAULT_BASE_LOCATION you provide will be the default location that objects in this catalog should be stored in, and the ROLE_ARN you provide should be a Role ARN with access to read and write data in that location. These credentials will be provided to engines reading data from the catalog once they have authenticated with Polaris using credentials that have access to those resources.\nIf you’re using a storage type other than S3, such as Azure, you’ll provide a different type of credential than a Role ARN. For more details on supported storage types, see the docs.\nAdditionally, if Polaris is running somewhere other than localhost:8181, you can specify the correct hostname and port by providing --host and --port flags. For the full set of options supported by the CLI, please refer to the docs.\nCreating a Principal and Assigning it Privileges With a catalog created, we can create a principal that has access to manage that catalog. For details on how to configure the Polaris CLI, see the section above or refer to the docs.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principals \\ create \\ quickstart_user ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ create \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ create \\ --catalog quickstart_catalog \\ quickstart_catalog_role Be sure to provide the necessary credentials, hostname, and port as before.\nWhen the principals create command completes successfully, it will return the credentials for this new principal. Be sure to note these down for later. For example:\n./polaris ... principals create example {\"clientId\": \"XXXX\", \"clientSecret\": \"YYYY\"} Now, we grant the principal the principal role we created, and grant the catalog role the principal role we created. For more information on these entities, please refer to the linked documentation.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ grant \\ --principal quickstart_user \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ grant \\ --catalog quickstart_catalog \\ --principal-role quickstart_user_role \\ quickstart_catalog_role Now, we’ve linked our principal to the catalog via roles like so:\nIn order to give this principal the ability to interact with the catalog, we must assign some privileges. For the time being, we will give this principal the ability to fully manage content in our new catalog. We can do this with the CLI like so:\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ grant \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT This grants the catalog privileges CATALOG_MANAGE_CONTENT to our catalog role, linking everything together like so:\nCATALOG_MANAGE_CONTENT has create/list/read/write privileges on all entities within the catalog. The same privilege could be granted to a namespace, in which case the principal could create/list/read/write any entity under that namespace.\nUsing Iceberg \u0026 Polaris At this point, we’ve created a principal and granted it the ability to manage a catalog. We can now use an external engine to assume that principal, access our catalog, and store data in that catalog using Apache Iceberg.\nConnecting with Spark To use a Polaris-managed catalog in Apache Spark, we can configure Spark to use the Iceberg catalog REST API.\nThis guide uses Apache Spark 3.5, but be sure to find the appropriate iceberg-spark package for your Spark version. From a local Spark clone on the branch-3.5 branch we can run the following:\nNote: the credentials provided here are those for our principal, not the root credentials.\nbin/spark-shell \\ --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.hadoop:hadoop-aws:3.4.0 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.quickstart_catalog.warehouse=quickstart_catalog \\ --conf spark.sql.catalog.quickstart_catalog.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.quickstart_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.quickstart_catalog.catalog-impl=org.apache.iceberg.rest.RESTCatalog \\ --conf spark.sql.catalog.quickstart_catalog.uri=http://localhost:8181/api/catalog \\ --conf spark.sql.catalog.quickstart_catalog.credential='XXXX:YYYY' \\ --conf spark.sql.catalog.quickstart_catalog.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.quickstart_catalog.token-refresh-enabled=true Replace XXXX and YYYY with the client ID and client secret generated when you created the quickstart_user principal.\nSimilar to the CLI commands above, this configures Spark to use the Polaris running at localhost:8181. If your Polaris server is running elsewhere, but sure to update the configuration appropriately.\nFinally, note that we include the hadoop-aws package here. If your table is using a different filesystem, be sure to include the appropriate dependency.\nOnce the Spark session starts, we can create a namespace and table within the catalog:\nspark.sql(\"USE quickstart_catalog\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS quickstart_namespace\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS quickstart_namespace.schema\") spark.sql(\"USE NAMESPACE quickstart_namespace.schema\") spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS quickstart_table ( id BIGINT, data STRING ) USING ICEBERG \"\"\") We can now use this table like any other:\nspark.sql(\"INSERT INTO quickstart_table VALUES (1, 'some data')\") spark.sql(\"SELECT * FROM quickstart_table\").show(false) . . . +---+---------+ |id |data | +---+---------+ |1 |some data| +---+---------+ If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Spark will lose access to the table:\nspark.sql(\"SELECT * FROM quickstart_table\").show(false) org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated ids '[6, 7]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION ","categories":"","description":"","excerpt":"This guide serves as a introduction to several key entities that can …","ref":"/releases/0.9.0/quickstart/","tags":"","title":"Quick Start"},{"body":"","categories":"","description":"","excerpt":"","ref":"","tags":"","title":"Getting Started"},{"body":"","categories":"","description":"","excerpt":"","ref":"","tags":"","title":"Getting Started"},{"body":"","categories":"","description":"","excerpt":"","ref":"","tags":"","title":"Getting Started"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Apache Polaris Blog"},{"body":"Check out the Quick Start page to get started.\n","categories":"","description":"","excerpt":"Check out the Quick Start page to get started.\n","ref":"/releases/0.9.0/","tags":"","title":"0.9.0"},{"body":"Apache Polaris (Incubating) is a catalog implementation for Apache Iceberg™ tables and is built on the open source Apache Iceberg™ REST protocol.\nWith Polaris, you can provide centralized, secure read and write access to your Iceberg tables across different REST-compatible query engines.\nKey concepts This section introduces key concepts associated with using Apache Polaris (Incubating).\nIn the following diagram, a sample Apache Polaris (Incubating) structure with nested namespaces is shown for Catalog1. No tables or namespaces have been created yet for Catalog2 or Catalog3.\nCatalog In Polaris, you can create one or more catalog resources to organize Iceberg tables.\nConfigure your catalog by setting values in the storage configuration for S3, Azure, or Google Cloud Storage. An Iceberg catalog enables a query engine to manage and organize tables. The catalog forms the first architectural layer in the Apache Iceberg™ table specification and must support the following tasks:\nStoring the current metadata pointer for one or more Iceberg tables. A metadata pointer maps a table name to the location of that table’s current metadata file.\nPerforming atomic operations so that you can update the current metadata pointer for a table to the metadata pointer of a new version of the table.\nTo learn more about Iceberg catalogs, see the Apache Iceberg™ documentation.\nCatalog types A catalog can be one of the following two types:\nInternal: The catalog is managed by Polaris. Tables from this catalog can be read and written in Polaris.\nExternal: The catalog is externally managed by another Iceberg catalog provider (for example, Snowflake, Glue, Dremio Arctic). Tables from this catalog are synced to Polaris. These tables are read-only in Polaris.\nA catalog is configured with a storage configuration that can point to S3, Azure storage, or GCS.\nNamespace You create namespaces to logically group Iceberg tables within a catalog. A catalog can have multiple namespaces. You can also create nested namespaces. Iceberg tables belong to namespaces.\nImportant\nFor the access privileges defined for a catalog to be enforced correctly, the following conditions must be met:\nThe directory only contains the data files that belong to a single table. The directory hierarchy matches the namespace hierarchy for the catalog. For example, if a catalog includes the following items:\nTop-level namespace namespace1 Nested namespace namespace1a A customers table, which is grouped under nested namespace namespace1a An orders table, which is grouped under nested namespace namespace1a The directory hierarchy for the catalog must follow this structure:\n/namespace1/namespace1a/customers/\u003cfiles for the customers table only\u003e /namespace1/namespace1a/orders/\u003cfiles for the orders table only\u003e Storage configuration A storage configuration stores a generated identity and access management (IAM) entity for your cloud storage and is created when you create a catalog. The storage configuration is used to set the values to connect Polaris to your cloud storage. During the catalog creation process, an IAM entity is generated and used to create a trust relationship between the cloud storage provider and Polaris Catalog.\nWhen you create a catalog, you supply the following information about your cloud storage:\nCloud storage provider Information Amazon S3 Default base location for your Amazon S3 bucketLocations for your Amazon S3 bucketS3 role ARNExternal ID (optional) Google Cloud Storage (GCS) Default base location for your GCS bucketLocations for your GCS bucket Azure Default base location for your Microsoft Azure containerLocations for your Microsoft Azure containerAzure tenant ID Example workflow In the following example workflow, Bob creates an Apache Iceberg™ table named Table1 and Alice reads data from Table1.\nBob uses Apache Spark™ to create the Table1 table under the Namespace1 namespace in the Catalog1 catalog and insert values into Table1.\nBob can create Table1 and insert data into it because he is using a service connection with a service principal that has the privileges to perform these actions.\nAlice uses Snowflake to read data from Table1.\nAlice can read data from Table1 because she is using a service connection with a service principal with a catalog integration that has the privileges to perform this action. Alice creates an unmanaged table in Snowflake to read data from Table1.\nSecurity and access control Credential vending To secure interactions with service connections, Polaris vends temporary storage credentials to the query engine during query execution. These credentials allow the query engine to run the query without requiring access to your cloud storage for Iceberg tables. This process is called credential vending.\nAs of now, the following limitation is known regarding Apache Iceberg support:\nremove_orphan_files: Apache Spark can’t use credential vending for this due to a known issue. See apache/iceberg#7914 for details. Identity and access management (IAM) Polaris uses the identity and access management (IAM) entity to securely connect to your storage for accessing table data, Iceberg metadata, and manifest files that store the table schema, partitions, and other metadata. Polaris retains the IAM entity for your storage location.\nAccess control Polaris enforces the access control that you configure across all tables registered with the service and governs security for all queries from query engines in a consistent manner.\nPolaris uses a role-based access control (RBAC) model that lets you centrally configure access for Polaris service principals to catalogs, namespaces, and tables.\nPolaris RBAC uses two different role types to delegate privileges:\nPrincipal roles: Granted to Polaris service principals and analogous to roles in other access control systems that you grant to service principals.\nCatalog roles: Configured with certain privileges on Polaris catalog resources and granted to principal roles.\nFor more information, see Access control.\nLegal Notices Apache®, Apache Iceberg™, Apache Spark™, Apache Flink®, and Flink® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.\n","categories":"","description":"","excerpt":"Apache Polaris (Incubating) is a catalog implementation for Apache …","ref":"/releases/1.0.0/","tags":"","title":"Overview"},{"body":"Apache Polaris (Incubating) is a catalog implementation for Apache Iceberg™ tables and is built on the open source Apache Iceberg™ REST protocol.\nWith Polaris, you can provide centralized, secure read and write access to your Iceberg tables across different REST-compatible query engines.\nKey concepts This section introduces key concepts associated with using Apache Polaris (Incubating).\nIn the following diagram, a sample Apache Polaris (Incubating) structure with nested namespaces is shown for Catalog1. No tables or namespaces have been created yet for Catalog2 or Catalog3.\nCatalog In Polaris, you can create one or more catalog resources to organize Iceberg tables.\nConfigure your catalog by setting values in the storage configuration for S3, Azure, or Google Cloud Storage. An Iceberg catalog enables a query engine to manage and organize tables. The catalog forms the first architectural layer in the Apache Iceberg™ table specification and must support the following tasks:\nStoring the current metadata pointer for one or more Iceberg tables. A metadata pointer maps a table name to the location of that table’s current metadata file.\nPerforming atomic operations so that you can update the current metadata pointer for a table to the metadata pointer of a new version of the table.\nTo learn more about Iceberg catalogs, see the Apache Iceberg™ documentation.\nCatalog types A catalog can be one of the following two types:\nInternal: The catalog is managed by Polaris. Tables from this catalog can be read and written in Polaris.\nExternal: The catalog is externally managed by another Iceberg catalog provider (for example, Snowflake, Glue, Dremio Arctic). Tables from this catalog are synced to Polaris. These tables are read-only in Polaris.\nA catalog is configured with a storage configuration that can point to S3, Azure storage, or GCS.\nNamespace You create namespaces to logically group Iceberg tables within a catalog. A catalog can have multiple namespaces. You can also create nested namespaces. Iceberg tables belong to namespaces.\nImportant\nFor the access privileges defined for a catalog to be enforced correctly, the following conditions must be met:\nThe directory only contains the data files that belong to a single table. The directory hierarchy matches the namespace hierarchy for the catalog. For example, if a catalog includes the following items:\nTop-level namespace namespace1 Nested namespace namespace1a A customers table, which is grouped under nested namespace namespace1a An orders table, which is grouped under nested namespace namespace1a The directory hierarchy for the catalog must follow this structure:\n/namespace1/namespace1a/customers/\u003cfiles for the customers table only\u003e /namespace1/namespace1a/orders/\u003cfiles for the orders table only\u003e Storage configuration A storage configuration stores a generated identity and access management (IAM) entity for your cloud storage and is created when you create a catalog. The storage configuration is used to set the values to connect Polaris to your cloud storage. During the catalog creation process, an IAM entity is generated and used to create a trust relationship between the cloud storage provider and Polaris Catalog.\nWhen you create a catalog, you supply the following information about your cloud storage:\nCloud storage provider Information Amazon S3 Default base location for your Amazon S3 bucketLocations for your Amazon S3 bucketS3 role ARNExternal ID (optional) Google Cloud Storage (GCS) Default base location for your GCS bucketLocations for your GCS bucket Azure Default base location for your Microsoft Azure containerLocations for your Microsoft Azure containerAzure tenant ID Example workflow In the following example workflow, Bob creates an Apache Iceberg™ table named Table1 and Alice reads data from Table1.\nBob uses Apache Spark™ to create the Table1 table under the Namespace1 namespace in the Catalog1 catalog and insert values into Table1.\nBob can create Table1 and insert data into it because he is using a service connection with a service principal that has the privileges to perform these actions.\nAlice uses Snowflake to read data from Table1.\nAlice can read data from Table1 because she is using a service connection with a service principal with a catalog integration that has the privileges to perform this action. Alice creates an unmanaged table in Snowflake to read data from Table1.\nSecurity and access control Credential vending To secure interactions with service connections, Polaris vends temporary storage credentials to the query engine during query execution. These credentials allow the query engine to run the query without requiring access to your cloud storage for Iceberg tables. This process is called credential vending.\nAs of now, the following limitation is known regarding Apache Iceberg support:\nremove_orphan_files: Apache Spark can’t use credential vending for this due to a known issue. See apache/iceberg#7914 for details. Identity and access management (IAM) Polaris uses the identity and access management (IAM) entity to securely connect to your storage for accessing table data, Iceberg metadata, and manifest files that store the table schema, partitions, and other metadata. Polaris retains the IAM entity for your storage location.\nAccess control Polaris enforces the access control that you configure across all tables registered with the service and governs security for all queries from query engines in a consistent manner.\nPolaris uses a role-based access control (RBAC) model that lets you centrally configure access for Polaris service principals to catalogs, namespaces, and tables.\nPolaris RBAC uses two different role types to delegate privileges:\nPrincipal roles: Granted to Polaris service principals and analogous to roles in other access control systems that you grant to service principals.\nCatalog roles: Configured with certain privileges on Polaris catalog resources and granted to principal roles.\nFor more information, see Access control.\nLegal Notices Apache®, Apache Iceberg™, Apache Spark™, Apache Flink®, and Flink® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.\n","categories":"","description":"","excerpt":"Apache Polaris (Incubating) is a catalog implementation for Apache …","ref":"/releases/1.0.1/","tags":"","title":"Overview"},{"body":"","categories":"","description":"","excerpt":"","ref":"/guides/","tags":"","title":"Apache Polaris Guides"},{"body":" [!WARNING] These pages refer to the current state of the main branch, which is still under active development.\nFunctionalities can be changed, removed or added without prior notice.\nApache Polaris (Incubating) is a catalog implementation for Apache Iceberg™ tables and is built on the open source Apache Iceberg™ REST protocol.\nWith Polaris, you can provide centralized, secure read and write access to your Iceberg tables across different REST-compatible query engines.\nKey concepts This section introduces key concepts associated with using Apache Polaris (Incubating).\nIn the following diagram, a sample Apache Polaris (Incubating) structure with nested namespaces is shown for Catalog1. No tables or namespaces have been created yet for Catalog2 or Catalog3.\nCatalog In Polaris, you can create one or more catalog resources to organize Iceberg tables.\nConfigure your catalog by setting values in the storage configuration for S3, Azure, or Google Cloud Storage. An Iceberg catalog enables a query engine to manage and organize tables. The catalog forms the first architectural layer in the Apache Iceberg™ table specification and must support the following tasks:\nStoring the current metadata pointer for one or more Iceberg tables. A metadata pointer maps a table name to the location of that table’s current metadata file.\nPerforming atomic operations so that you can update the current metadata pointer for a table to the metadata pointer of a new version of the table.\nTo learn more about Iceberg catalogs, see the Apache Iceberg™ documentation.\nCatalog types A catalog can be one of the following two types:\nInternal: The catalog is managed by Polaris. Tables from this catalog can be read and written in Polaris.\nExternal: The catalog is externally managed by another Iceberg catalog provider (for example, Snowflake, Glue, Dremio Arctic). Tables from this catalog are synced to Polaris. These tables are read-only in Polaris.\nA catalog is configured with a storage configuration that can point to S3, Azure storage, or GCS.\nNamespace You create namespaces to logically group Iceberg tables within a catalog. A catalog can have multiple namespaces. You can also create nested namespaces. Iceberg tables belong to namespaces.\n[!Important] For the access privileges defined for a catalog to be enforced correctly, the following conditions must be met:\nThe directory only contains the data files that belong to a single table. The directory hierarchy matches the namespace hierarchy for the catalog. For example, if a catalog includes the following items:\nTop-level namespace namespace1 Nested namespace namespace1a A customers table, which is grouped under nested namespace namespace1a An orders table, which is grouped under nested namespace namespace1a The directory hierarchy for the catalog must follow this structure:\n/namespace1/namespace1a/customers/\u003cfiles for the customers table only\u003e /namespace1/namespace1a/orders/\u003cfiles for the orders table only\u003e Storage configuration A storage configuration stores a generated identity and access management (IAM) entity for your cloud storage and is created when you create a catalog. The storage configuration is used to set the values to connect Polaris to your cloud storage. During the catalog creation process, an IAM entity is generated and used to create a trust relationship between the cloud storage provider and Polaris Catalog.\nWhen you create a catalog, you supply the following information about your cloud storage:\nCloud storage provider Information Amazon S3 Default base location for your Amazon S3 bucketLocations for your Amazon S3 bucketS3 role ARNExternal ID (optional) Google Cloud Storage (GCS) Default base location for your GCS bucketLocations for your GCS bucket Azure Default base location for your Microsoft Azure containerLocations for your Microsoft Azure containerAzure tenant ID Example workflow In the following example workflow, Bob creates an Apache Iceberg™ table named Table1 and Alice reads data from Table1.\nBob uses Apache Spark™ to create the Table1 table under the Namespace1 namespace in the Catalog1 catalog and insert values into Table1.\nBob can create Table1 and insert data into it because he is using a service connection with a service principal that has the privileges to perform these actions.\nAlice uses Snowflake to read data from Table1.\nAlice can read data from Table1 because she is using a service connection with a service principal with a catalog integration that has the privileges to perform this action. Alice creates an unmanaged table in Snowflake to read data from Table1.\nSecurity and access control Credential vending To secure interactions with service connections, Polaris vends temporary storage credentials to the query engine during query execution. These credentials allow the query engine to run the query without requiring access to your cloud storage for Iceberg tables. This process is called credential vending.\nAs of now, the following limitation is known regarding Apache Iceberg support:\nremove_orphan_files: Apache Spark can’t use credential vending for this due to a known issue. See apache/iceberg#7914 for details. Identity and access management (IAM) Polaris uses the identity and access management (IAM) entity to securely connect to your storage for accessing table data, Iceberg metadata, and manifest files that store the table schema, partitions, and other metadata. Polaris retains the IAM entity for your storage location.\nAccess control Polaris enforces the access control that you configure across all tables registered with the service and governs security for all queries from query engines in a consistent manner.\nPolaris uses a role-based access control (RBAC) model that lets you centrally configure access for Polaris service principals to catalogs, namespaces, and tables.\nPolaris RBAC uses two different role types to delegate privileges:\nPrincipal roles: Granted to Polaris service principals and analogous to roles in other access control systems that you grant to service principals.\nCatalog roles: Configured with certain privileges on Polaris catalog resources and granted to principal roles.\nFor more information, see Access control.\nLegal Notices Apache®, Apache Iceberg™, Apache Spark™, Apache Flink®, and Flink® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.\n","categories":"","description":"","excerpt":" [!WARNING] These pages refer to the current state of the main branch, …","ref":"/in-dev/unreleased/","tags":"","title":"Overview"},{"body":"Apache Polaris (Incubating) is a catalog implementation for Apache Iceberg™ tables and is built on the open source Apache Iceberg™ REST protocol.\nWith Polaris, you can provide centralized, secure read and write access to your Iceberg tables across different REST-compatible query engines.\nKey concepts This section introduces key concepts associated with using Apache Polaris (Incubating).\nIn the following diagram, a sample Apache Polaris (Incubating) structure with nested namespaces is shown for Catalog1. No tables or namespaces have been created yet for Catalog2 or Catalog3.\nCatalog In Polaris, you can create one or more catalog resources to organize Iceberg tables.\nConfigure your catalog by setting values in the storage configuration for S3, Azure, or Google Cloud Storage. An Iceberg catalog enables a query engine to manage and organize tables. The catalog forms the first architectural layer in the Apache Iceberg™ table specification and must support the following tasks:\nStoring the current metadata pointer for one or more Iceberg tables. A metadata pointer maps a table name to the location of that table’s current metadata file.\nPerforming atomic operations so that you can update the current metadata pointer for a table to the metadata pointer of a new version of the table.\nTo learn more about Iceberg catalogs, see the Apache Iceberg™ documentation.\nCatalog types A catalog can be one of the following two types:\nInternal: The catalog is managed by Polaris. Tables from this catalog can be read and written in Polaris.\nExternal: The catalog is externally managed by another Iceberg catalog provider (for example, Snowflake, Glue, Dremio Arctic). Tables from this catalog are synced to Polaris. These tables are read-only in Polaris. In the current release, only a Snowflake external catalog is provided.\nA catalog is configured with a storage configuration that can point to S3, Azure storage, or GCS.\nNamespace You create namespaces to logically group Iceberg tables within a catalog. A catalog can have multiple namespaces. You can also create nested namespaces. Iceberg tables belong to namespaces.\nApache Iceberg™ tables and catalogs In an internal catalog, an Iceberg table is registered in Polaris, but read and written via query engines. The table data and metadata is stored in your external cloud storage. The table uses Polaris as the Iceberg catalog.\nIf you have tables housed in another Iceberg catalog, you can sync these tables to an external catalog in Polaris. If you sync this catalog to Polaris, it appears as an external catalog in Polaris. Clients connecting to the external catalog can read from or write to these tables. However, clients connecting to Polaris will only be able to read from these tables.\nImportant\nFor the access privileges defined for a catalog to be enforced correctly, the following conditions must be met:\nThe directory only contains the data files that belong to a single table. The directory hierarchy matches the namespace hierarchy for the catalog. For example, if a catalog includes the following items:\nTop-level namespace namespace1 Nested namespace namespace1a A customers table, which is grouped under nested namespace namespace1a An orders table, which is grouped under nested namespace namespace1a The directory hierarchy for the catalog must follow this structure:\n/namespace1/namespace1a/customers/\u003cfiles for the customers table only\u003e /namespace1/namespace1a/orders/\u003cfiles for the orders table only\u003e Service principal A service principal is an entity that you create in Polaris. Each service principal encapsulates credentials that you use to connect to Polaris.\nQuery engines use service principals to connect to catalogs.\nPolaris generates a Client ID and Client Secret pair for each service principal.\nThe following table displays example service principals that you might create in Polaris:\nService connection name Purpose Flink ingestion For Apache Flink® to ingest streaming data into Apache Iceberg™ tables. Spark ETL pipeline For Apache Spark™ to run ETL pipeline jobs on Iceberg tables. Snowflake data pipelines For Snowflake to run data pipelines for transforming data in Apache Iceberg™ tables. Trino BI dashboard For Trino to run BI queries for powering a dashboard. Snowflake AI team For Snowflake to run AI jobs on data in Apache Iceberg™ tables. Service connection A service connection represents a REST-compatible engine (such as Apache Spark™, Apache Flink®, or Trino) that can read from and write to Polaris Catalog. When creating a new service connection, the Polaris administrator grants the service principal that is created with the new service connection either a new or existing principal role. A principal role is a resource in Polaris that you can use to logically group Polaris service principals together and grant privileges on securable objects. For more information, see Principal role. Polaris uses a role-based access control (RBAC) model to grant service principals access to resources. For more information, see Access control. For a diagram of this model, see RBAC model.\nIf the Polaris administrator grants the service principal for the new service connection a new principal role, the service principal doesn’t have any privileges granted to it yet. When securing the catalog that the new service connection will connect to, the Polaris administrator grants privileges to catalog roles and then grants these catalog roles to the new principal role. As a result, the service principal for the new service connection has these privileges. For more information about catalog roles, see Catalog role.\nIf the Polaris administrator grants an existing principal role to the service principal for the new service connection, the service principal has the same privileges granted to the catalog roles that are granted to the existing principal role. If needed, the Polaris administrator can grant additional catalog roles to the existing principal role or remove catalog roles from it to adjust the privileges bestowed to the service principal. For an example of how RBAC works in Polaris, see RBAC example.\nStorage configuration A storage configuration stores a generated identity and access management (IAM) entity for your external cloud storage and is created when you create a catalog. The storage configuration is used to set the values to connect Polaris to your cloud storage. During the catalog creation process, an IAM entity is generated and used to create a trust relationship between the cloud storage provider and Polaris Catalog.\nWhen you create a catalog, you supply the following information about your external cloud storage:\nCloud storage provider Information Amazon S3 Default base location for your Amazon S3 bucketLocations for your Amazon S3 bucketS3 role ARNExternal ID (optional) Google Cloud Storage (GCS) Default base location for your GCS bucketLocations for your GCS bucket Azure Default base location for your Microsoft Azure containerLocations for your Microsoft Azure containerAzure tenant ID Example workflow In the following example workflow, Bob creates an Apache Iceberg™ table named Table1 and Alice reads data from Table1.\nBob uses Apache Spark™ to create the Table1 table under the Namespace1 namespace in the Catalog1 catalog and insert values into Table1.\nBob can create Table1 and insert data into it because he is using a service connection with a service principal that has the privileges to perform these actions.\nAlice uses Snowflake to read data from Table1.\nAlice can read data from Table1 because she is using a service connection with a service principal with a catalog integration that has the privileges to perform this action. Alice creates an unmanaged table in Snowflake to read data from Table1.\nSecurity and access control This section describes security and access control.\nCredential vending To secure interactions with service connections, Polaris vends temporary storage credentials to the query engine during query execution. These credentials allow the query engine to run the query without requiring access to your external cloud storage for Iceberg tables. This process is called credential vending.\nAs of now, the following limitation is known regarding Apache Iceberg support:\nremove_orphan_files: Apache Spark can’t use credential vending for this due to a known issue. See apache/iceberg#7914 for details. Identity and access management (IAM) Polaris uses the identity and access management (IAM) entity to securely connect to your storage for accessing table data, Iceberg metadata, and manifest files that store the table schema, partitions, and other metadata. Polaris retains the IAM entity for your storage location.\nAccess control Polaris enforces the access control that you configure across all tables registered with the service and governs security for all queries from query engines in a consistent manner.\nPolaris uses a role-based access control (RBAC) model that lets you centrally configure access for Polaris service principals to catalogs, namespaces, and tables.\nPolaris RBAC uses two different role types to delegate privileges:\nPrincipal roles: Granted to Polaris service principals and analogous to roles in other access control systems that you grant to service principals.\nCatalog roles: Configured with certain privileges on Polaris catalog resources and granted to principal roles.\nFor more information, see Access control.\nLegal Notices Apache®, Apache Iceberg™, Apache Spark™, Apache Flink®, and Flink® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.\n","categories":"","description":"","excerpt":"Apache Polaris (Incubating) is a catalog implementation for Apache …","ref":"/releases/0.9.0/overview/","tags":"","title":"Overview"},{"body":" Proposals Active\nPast\nPolaris Roadmap Discussed Roadmap\n","categories":"","description":"","excerpt":" Proposals Active\nPast\nPolaris Roadmap Discussed Roadmap\n","ref":"/community/proposals/","tags":"","title":"Apache Polaris Proposals \u0026 Roadmap"},{"body":"Polaris can be deployed via a docker image or as a standalone process. Before starting, be sure that you’ve satisfied the relevant prerequisites detailed in the previous page.\nCommon Setup Before running Polaris, ensure you have completed the following setup steps:\nBuild Polaris cd ~/polaris ./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild --rerun \\ :polaris-admin:assemble \\ :polaris-admin:quarkusAppPartsBuild --rerun \\ -Dquarkus.container-image.build=true For standalone: Omit the -Dquarkus.container-image.tag and -Dquarkus.container-image.build options if you do not need to build a Docker image. Running Polaris with Docker To start using Polaris in Docker and launch Polaris, which is packaged with a Postgres instance, Apache Spark, and Trino.\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ export QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://postgres:5432/POLARIS export QUARKUS_DATASOURCE_USERNAME=postgres export QUARKUS_DATASOURCE_PASSWORD=postgres export CLIENT_ID=root export CLIENT_SECRET=s3cr3t docker compose -p polaris -f getting-started/assets/postgres/docker-compose-postgres.yml \\ -f getting-started/jdbc/docker-compose-bootstrap-db.yml \\ -f getting-started/jdbc/docker-compose.yml up -d You should see output for some time as Polaris, Spark, and Trino build and start up. Eventually, you won’t see any more logs and see some logs relating to Spark, resembling the following:\nspark-sql-1 | Spark Web UI available at http://8bc4de8ed854:4040 spark-sql-1 | Spark master: local[*], Application Id: local-1743745174604 spark-sql-1 | 25/04/04 05:39:38 WARN SparkSQLCLIDriver: WARNING: Directory for Hive history file: /home/spark does not exist. History will not be available during this session. spark-sql-1 | 25/04/04 05:39:39 WARN RESTSessionCatalog: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to http://polaris:8181/api/catalogv1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537 The Docker image pre-configures a sample catalog called quickstart_catalog that uses a local file system.\nRunning Polaris as a Standalone Process You can also start Polaris through Gradle (packaged within the Polaris repository):\nStart the Server Run the following command to start Polaris:\n./gradlew run You should see output for some time as Polaris builds and starts up. Eventually, you won’t see any more logs and should see messages that resemble the following:\nINFO [io.quarkus] [,] [,,,] (main) Apache Polaris Server (incubating) \u003cversion\u003e on JVM (powered by Quarkus \u003cversion\u003e) started in 1.911s. Listening on: http://0.0.0.0:8181. Management interface listening on http://0.0.0.0:8182. INFO [io.quarkus] [,] [,,,] (main) Profile prod activated. INFO [io.quarkus] [,] [,,,] (main) Installed features: [...] At this point, Polaris is running.\nWhen using a Gradle-launched Polaris instance in this tutorial, we’ll launch an instance of Polaris that stores entities only in-memory. This means that any entities that you define will be destroyed when Polaris is shut down. For more information on how to configure Polaris for production usage, see the docs.\nWhen Polaris is run using the ./gradlew run command, the root principal credentials are root and s3cr3t for the CLIENT_ID and CLIENT_SECRET, respectively.\nInstalling Apache Spark and Trino Locally for Testing Apache Spark If you want to connect to Polaris with Apache Spark, you’ll need to start by cloning Spark. As in the prerequisites, make sure git is installed first.\nThen, clone Spark and check out a versioned branch. This guide uses Spark 3.5.\ngit clone --branch branch-3.5 https://github.com/apache/spark.git ~/spark Trino If you want to connect to Polaris with Trino, it is recommended to set up a test instance of Trino using Docker. As in the prerequisites, make sure Docker is installed first\ndocker run --name trino -d -p 8080:8080 trinodb/trino Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\n","categories":"","description":"","excerpt":"Polaris can be deployed via a docker image or as a standalone process. …","ref":"/in-dev/unreleased/getting-started/quickstart/","tags":"","title":"Quickstart"},{"body":"Polaris can be deployed via a docker image or as a standalone process. Before starting, be sure that you’ve satisfied the relevant prerequisites detailed in the previous page.\nCommon Setup Before running Polaris, ensure you have completed the following setup steps:\nBuild Polaris cd ~/polaris ./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild \\ :polaris-admin:assemble --rerun \\ -Dquarkus.container-image.tag=postgres-latest \\ -Dquarkus.container-image.build=true For standalone: Omit the -Dquarkus.container-image.tag and -Dquarkus.container-image.build options if you do not need to build a Docker image. Running Polaris with Docker To start using Polaris in Docker and launch Polaris, which is packaged with a Postgres instance, Apache Spark, and Trino.\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ export QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://postgres:5432/POLARIS export QUARKUS_DATASOURCE_USERNAME=postgres export QUARKUS_DATASOURCE_PASSWORD=postgres export CLIENT_ID=root export CLIENT_SECRET=s3cr3t docker compose -p polaris -f getting-started/assets/postgres/docker-compose-postgres.yml \\ -f getting-started/jdbc/docker-compose-bootstrap-db.yml \\ -f getting-started/jdbc/docker-compose.yml up -d You should see output for some time as Polaris, Spark, and Trino build and start up. Eventually, you won’t see any more logs and see some logs relating to Spark, resembling the following:\nspark-sql-1 | Spark Web UI available at http://8bc4de8ed854:4040 spark-sql-1 | Spark master: local[*], Application Id: local-1743745174604 spark-sql-1 | 25/04/04 05:39:38 WARN SparkSQLCLIDriver: WARNING: Directory for Hive history file: /home/spark does not exist. History will not be available during this session. spark-sql-1 | 25/04/04 05:39:39 WARN RESTSessionCatalog: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to http://polaris:8181/api/catalogv1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537 The Docker image pre-configures a sample catalog called quickstart_catalog that uses a local file system.\nRunning Polaris as a Standalone Process You can also start Polaris through Gradle (packaged within the Polaris repository):\nStart the Server Run the following command to start Polaris:\n./gradlew run You should see output for some time as Polaris builds and starts up. Eventually, you won’t see any more logs and should see messages that resemble the following:\nINFO [io.quarkus] [,] [,,,] (Quarkus Main Thread) polaris-runtime-service \u003cversion\u003e on JVM (powered by Quarkus \u003cversion\u003e) started in 2.656s. Listening on: http://localhost:8181. Management interface listening on http://0.0.0.0:8182. INFO [io.quarkus] [,] [,,,] (Quarkus Main Thread) Profile prod activated. Live Coding activated. INFO [io.quarkus] [,] [,,,] (Quarkus Main Thread) Installed features: [...] At this point, Polaris is running.\nWhen using a Gradle-launched Polaris instance in this tutorial, we’ll launch an instance of Polaris that stores entities only in-memory. This means that any entities that you define will be destroyed when Polaris is shut down. For more information on how to configure Polaris for production usage, see the docs.\nWhen Polaris is run using the ./gradlew run command, the root principal credentials are root and secret for the CLIENT_ID and CLIENT_SECRET, respectively.\nInstalling Apache Spark and Trino Locally for Testing Apache Spark If you want to connect to Polaris with Apache Spark, you’ll need to start by cloning Spark. As in the prerequisites, make sure git is installed first.\nThen, clone Spark and check out a versioned branch. This guide uses Spark 3.5.\ngit clone --branch branch-3.5 https://github.com/apache/spark.git ~/spark Trino If you want to connect to Polaris with Trino, it is recommended to set up a test instance of Trino using Docker. As in the prerequisites, make sure Docker is installed first\ndocker run --name trino -d -p 8080:8080 trinodb/trino Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\n","categories":"","description":"","excerpt":"Polaris can be deployed via a docker image or as a standalone process. …","ref":"/releases/1.0.0/getting-started/quickstart/","tags":"","title":"Quickstart"},{"body":"Polaris can be deployed via a docker image or as a standalone process. Before starting, be sure that you’ve satisfied the relevant prerequisites detailed in the previous page.\nCommon Setup Before running Polaris, ensure you have completed the following setup steps:\nBuild Polaris cd ~/polaris ./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild \\ :polaris-admin:assemble --rerun \\ -Dquarkus.container-image.tag=postgres-latest \\ -Dquarkus.container-image.build=true For standalone: Omit the -Dquarkus.container-image.tag and -Dquarkus.container-image.build options if you do not need to build a Docker image. Running Polaris with Docker To start using Polaris in Docker and launch Polaris, which is packaged with a Postgres instance, Apache Spark, and Trino.\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ export QUARKUS_DATASOURCE_JDBC_URL=jdbc:postgresql://postgres:5432/POLARIS export QUARKUS_DATASOURCE_USERNAME=postgres export QUARKUS_DATASOURCE_PASSWORD=postgres export CLIENT_ID=root export CLIENT_SECRET=s3cr3t docker compose -p polaris -f getting-started/assets/postgres/docker-compose-postgres.yml \\ -f getting-started/jdbc/docker-compose-bootstrap-db.yml \\ -f getting-started/jdbc/docker-compose.yml up -d You should see output for some time as Polaris, Spark, and Trino build and start up. Eventually, you won’t see any more logs and see some logs relating to Spark, resembling the following:\nspark-sql-1 | Spark Web UI available at http://8bc4de8ed854:4040 spark-sql-1 | Spark master: local[*], Application Id: local-1743745174604 spark-sql-1 | 25/04/04 05:39:38 WARN SparkSQLCLIDriver: WARNING: Directory for Hive history file: /home/spark does not exist. History will not be available during this session. spark-sql-1 | 25/04/04 05:39:39 WARN RESTSessionCatalog: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to http://polaris:8181/api/catalogv1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537 The Docker image pre-configures a sample catalog called quickstart_catalog that uses a local file system.\nRunning Polaris as a Standalone Process You can also start Polaris through Gradle (packaged within the Polaris repository):\nStart the Server Run the following command to start Polaris:\n./gradlew run You should see output for some time as Polaris builds and starts up. Eventually, you won’t see any more logs and should see messages that resemble the following:\nINFO [io.quarkus] [,] [,,,] (Quarkus Main Thread) polaris-runtime-service \u003cversion\u003e on JVM (powered by Quarkus \u003cversion\u003e) started in 2.656s. Listening on: http://localhost:8181. Management interface listening on http://0.0.0.0:8182. INFO [io.quarkus] [,] [,,,] (Quarkus Main Thread) Profile prod activated. Live Coding activated. INFO [io.quarkus] [,] [,,,] (Quarkus Main Thread) Installed features: [...] At this point, Polaris is running.\nWhen using a Gradle-launched Polaris instance in this tutorial, we’ll launch an instance of Polaris that stores entities only in-memory. This means that any entities that you define will be destroyed when Polaris is shut down. For more information on how to configure Polaris for production usage, see the docs.\nWhen Polaris is run using the ./gradlew run command, the root principal credentials are root and secret for the CLIENT_ID and CLIENT_SECRET, respectively.\nInstalling Apache Spark and Trino Locally for Testing Apache Spark If you want to connect to Polaris with Apache Spark, you’ll need to start by cloning Spark. As in the prerequisites, make sure git is installed first.\nThen, clone Spark and check out a versioned branch. This guide uses Spark 3.5.\ngit clone --branch branch-3.5 https://github.com/apache/spark.git ~/spark Trino If you want to connect to Polaris with Trino, it is recommended to set up a test instance of Trino using Docker. As in the prerequisites, make sure Docker is installed first\ndocker run --name trino -d -p 8080:8080 trinodb/trino Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\n","categories":"","description":"","excerpt":"Polaris can be deployed via a docker image or as a standalone process. …","ref":"/releases/1.0.1/getting-started/quickstart/","tags":"","title":"Quickstart"},{"body":"Polaris includes a tool for administrators to manage the metastore.\nThe tool must be built with the necessary JDBC drivers to access the metastore database. For example, to build the tool with support for Postgres, run the following:\n./gradlew \\ :polaris-admin:assemble \\ :polaris-admin:quarkusAppPartsBuild --rerun \\ -Dquarkus.container-image.build=true The above command will generate:\nOne Fast-JAR in runtime/admin/build/quarkus-app/quarkus-run.jar Two Docker images named apache/polaris-admin-tool:latest and apache/polaris-admin-tool:\u003cversion\u003e Usage Please make sure the admin tool and Polaris server are with the same version before using it. To run the standalone JAR, use the following command:\njava -jar runtime/admin/build/quarkus-app/quarkus-run.jar --help To run the Docker image, use the following command:\ndocker run apache/polaris-admin-tool:latest --help The basic usage of the Polaris Admin Tool is outlined below:\nUsage: polaris-admin-runner.jar [-hV] [COMMAND] Polaris Admin Tool -h, --help Show this help message and exit. -V, --version Print version information and exit. Commands: help Display help information about the specified command. bootstrap Bootstraps realms and principal credentials. purge Purge principal credentials. Configuration The Polaris Admin Tool must be executed with the same configuration as the Polaris server. The configuration can be done via environment variables or system properties.\nAt a minimum, it is necessary to configure the Polaris Admin Tool to connect to the same database used by the Polaris server.\nSee the metastore documentation for more information on configuring the database connection.\nNote: Polaris will always create schema ‘polaris_schema’ during bootstrap under the configured database.\nBootstrapping Realms and Principal Credentials The bootstrap command is used to bootstrap realms and create the necessary principal credentials for the Polaris server. This command is idempotent and can be run multiple times without causing any issues. If a realm is already bootstrapped, running the bootstrap command again will not have any effect on that realm.\njava -jar runtime/admin/build/quarkus-app/quarkus-run.jar bootstrap --help The basic usage of the bootstrap command is outlined below:\nUsage: polaris-admin-runner.jar bootstrap [-hV] [-c=\u003crealm,clientId, clientSecret\u003e]... -r=\u003crealm\u003e [-r=\u003crealm\u003e]... Bootstraps realms and root principal credentials. -c, --credential=\u003crealm,clientId,clientSecret\u003e Root principal credentials to bootstrap. Must be of the form 'realm,clientId,clientSecret'. -h, --help Show this help message and exit. -r, --realm=\u003crealm\u003e The name of a realm to bootstrap. -V, --version Print version information and exit. For example, to bootstrap the realm1 realm and create its root principal credential with the client ID admin and client secret admin, you can run the following command:\njava -jar runtime/admin/build/quarkus-app/quarkus-run.jar bootstrap -r realm1 -c realm1,admin,admin Purging Realms and Principal Credentials The purge command is used to remove realms and principal credentials from the Polaris server.\n[!WARNING] Running the purge command will remove all data associated with the specified realms! This includes all entities (catalogs, namespaces, tables, views, roles), all principal credentials, grants, and any other data associated with the realms.\njava -jar runtime/admin/build/quarkus-app/quarkus-run.jar purge --help The basic usage of the purge command is outlined below:\nUsage: polaris-admin-runner.jar purge [-hV] -r=\u003crealm\u003e [-r=\u003crealm\u003e]... Purge realms and all associated entities. -h, --help Show this help message and exit. -r, --realm=\u003crealm\u003e The name of a realm to purge. -V, --version Print version information and exit. For example, to purge the realm1 realm, you can run the following command:\njava -jar runtime/admin/build/quarkus-app/quarkus-run.jar purge -r realm1 ","categories":"","description":"","excerpt":"Polaris includes a tool for administrators to manage the metastore. …","ref":"/in-dev/unreleased/admin-tool/","tags":"","title":"Admin Tool"},{"body":"Polaris includes a tool for administrators to manage the metastore.\nThe tool must be built with the necessary JDBC drivers to access the metastore database. For example, to build the tool with support for Postgres, run the following:\n./gradlew \\ :polaris-admin:assemble \\ :polaris-admin:quarkusAppPartsBuild --rerun \\ -Dquarkus.container-image.build=true The above command will generate:\nOne standalone JAR in runtime/admin/build/polaris-admin-*-runner.jar Two distribution archives in runtime/admin/build/distributions Two Docker images named apache/polaris-admin-tool:latest and apache/polaris-admin-tool:\u003cversion\u003e Usage Please make sure the admin tool and Polaris server are with the same version before using it. To run the standalone JAR, use the following command:\njava -jar runtime/admin/build/polaris-admin-*-runner.jar --help To run the Docker image, use the following command:\ndocker run apache/polaris-admin-tool:latest --help The basic usage of the Polaris Admin Tool is outlined below:\nUsage: polaris-admin-runner.jar [-hV] [COMMAND] Polaris Admin Tool -h, --help Show this help message and exit. -V, --version Print version information and exit. Commands: help Display help information about the specified command. bootstrap Bootstraps realms and principal credentials. purge Purge principal credentials. Configuration The Polaris Admin Tool must be executed with the same configuration as the Polaris server. The configuration can be done via environment variables or system properties.\nAt a minimum, it is necessary to configure the Polaris Admin Tool to connect to the same database used by the Polaris server.\nSee the metastore documentation for more information on configuring the database connection.\nNote: Polaris will always create schema ‘polaris_schema’ during bootstrap under the configured database.\nBootstrapping Realms and Principal Credentials The bootstrap command is used to bootstrap realms and create the necessary principal credentials for the Polaris server. This command is idempotent and can be run multiple times without causing any issues. If a realm is already bootstrapped, running the bootstrap command again will not have any effect on that realm.\njava -jar runtime/admin/build/polaris-admin-*-runner.jar bootstrap --help The basic usage of the bootstrap command is outlined below:\nUsage: polaris-admin-runner.jar bootstrap [-hV] [-c=\u003crealm,clientId, clientSecret\u003e]... -r=\u003crealm\u003e [-r=\u003crealm\u003e]... Bootstraps realms and root principal credentials. -c, --credential=\u003crealm,clientId,clientSecret\u003e Root principal credentials to bootstrap. Must be of the form 'realm,clientId,clientSecret'. -h, --help Show this help message and exit. -r, --realm=\u003crealm\u003e The name of a realm to bootstrap. -V, --version Print version information and exit. For example, to bootstrap the realm1 realm and create its root principal credential with the client ID admin and client secret admin, you can run the following command:\njava -jar runtime/admin/build/polaris-admin-*-runner.jar bootstrap -r realm1 -c realm1,admin,admin Purging Realms and Principal Credentials The purge command is used to remove realms and principal credentials from the Polaris server.\nWarning: Running the purge command will remove all data associated with the specified realms! This includes all entities (catalogs, namespaces, tables, views, roles), all principal credentials, grants, and any other data associated with the realms.\njava -jar runtime/admin/build/polaris-admin-*-runner.jar purge --help The basic usage of the purge command is outlined below:\nUsage: polaris-admin-runner.jar purge [-hV] -r=\u003crealm\u003e [-r=\u003crealm\u003e]... Purge realms and all associated entities. -h, --help Show this help message and exit. -r, --realm=\u003crealm\u003e The name of a realm to purge. -V, --version Print version information and exit. For example, to purge the realm1 realm, you can run the following command:\njava -jar runtime/admin/build/polaris-admin-*-runner.jar purge -r realm1 ","categories":"","description":"","excerpt":"Polaris includes a tool for administrators to manage the metastore. …","ref":"/releases/1.0.0/admin-tool/","tags":"","title":"Admin Tool"},{"body":"Polaris includes a tool for administrators to manage the metastore.\nThe tool must be built with the necessary JDBC drivers to access the metastore database. For example, to build the tool with support for Postgres, run the following:\n./gradlew \\ :polaris-admin:assemble \\ :polaris-admin:quarkusAppPartsBuild --rerun \\ -Dquarkus.container-image.build=true The above command will generate:\nOne standalone JAR in runtime/admin/build/polaris-admin-*-runner.jar Two distribution archives in runtime/admin/build/distributions Two Docker images named apache/polaris-admin-tool:latest and apache/polaris-admin-tool:\u003cversion\u003e Usage Please make sure the admin tool and Polaris server are with the same version before using it. To run the standalone JAR, use the following command:\njava -jar runtime/admin/build/polaris-admin-*-runner.jar --help To run the Docker image, use the following command:\ndocker run apache/polaris-admin-tool:latest --help The basic usage of the Polaris Admin Tool is outlined below:\nUsage: polaris-admin-runner.jar [-hV] [COMMAND] Polaris Admin Tool -h, --help Show this help message and exit. -V, --version Print version information and exit. Commands: help Display help information about the specified command. bootstrap Bootstraps realms and principal credentials. purge Purge principal credentials. Configuration The Polaris Admin Tool must be executed with the same configuration as the Polaris server. The configuration can be done via environment variables or system properties.\nAt a minimum, it is necessary to configure the Polaris Admin Tool to connect to the same database used by the Polaris server.\nSee the metastore documentation for more information on configuring the database connection.\nNote: Polaris will always create schema ‘polaris_schema’ during bootstrap under the configured database.\nBootstrapping Realms and Principal Credentials The bootstrap command is used to bootstrap realms and create the necessary principal credentials for the Polaris server. This command is idempotent and can be run multiple times without causing any issues. If a realm is already bootstrapped, running the bootstrap command again will not have any effect on that realm.\njava -jar runtime/admin/build/polaris-admin-*-runner.jar bootstrap --help The basic usage of the bootstrap command is outlined below:\nUsage: polaris-admin-runner.jar bootstrap [-hV] [-c=\u003crealm,clientId, clientSecret\u003e]... -r=\u003crealm\u003e [-r=\u003crealm\u003e]... Bootstraps realms and root principal credentials. -c, --credential=\u003crealm,clientId,clientSecret\u003e Root principal credentials to bootstrap. Must be of the form 'realm,clientId,clientSecret'. -h, --help Show this help message and exit. -r, --realm=\u003crealm\u003e The name of a realm to bootstrap. -V, --version Print version information and exit. For example, to bootstrap the realm1 realm and create its root principal credential with the client ID admin and client secret admin, you can run the following command:\njava -jar runtime/admin/build/polaris-admin-*-runner.jar bootstrap -r realm1 -c realm1,admin,admin Purging Realms and Principal Credentials The purge command is used to remove realms and principal credentials from the Polaris server.\nWarning: Running the purge command will remove all data associated with the specified realms! This includes all entities (catalogs, namespaces, tables, views, roles), all principal credentials, grants, and any other data associated with the realms.\njava -jar runtime/admin/build/polaris-admin-*-runner.jar purge --help The basic usage of the purge command is outlined below:\nUsage: polaris-admin-runner.jar purge [-hV] -r=\u003crealm\u003e [-r=\u003crealm\u003e]... Purge realms and all associated entities. -h, --help Show this help message and exit. -r, --realm=\u003crealm\u003e The name of a realm to purge. -V, --version Print version information and exit. For example, to purge the realm1 realm, you can run the following command:\njava -jar runtime/admin/build/polaris-admin-*-runner.jar purge -r realm1 ","categories":"","description":"","excerpt":"Polaris includes a tool for administrators to manage the metastore. …","ref":"/releases/1.0.1/admin-tool/","tags":"","title":"Admin Tool"},{"body":" Contributor Code of Conduct This is a copy of the Contributor Covenant v2.1. No changes have been made.\nOur Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nOur Standards Examples of behavior that contributes to a positive environment for our community include:\nDemonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include:\nThe use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others’ private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\nScope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nEnforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at private@polaris.apache.org. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.\nEnforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n1. Correction Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n2. Warning Community Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n3. Temporary Ban Community Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n4. Permanent Ban Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\nAttribution This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.\n","categories":"","description":"","excerpt":" Contributor Code of Conduct This is a copy of the Contributor …","ref":"/community/code-of-conduct/","tags":"","title":""},{"body":"In order to help administrators quickly set up and manage their Polaris server, Polaris provides a simple command-line interface (CLI) for common tasks.\nThe basic syntax of the Polaris CLI is outlined below:\npolaris [options] COMMAND ... options: --host --port --base-url --client-id --client-secret --access-token --profile COMMAND must be one of the following:\ncatalogs principals principal-roles catalog-roles namespaces privileges profiles repair Each command supports several subcommands, and some subcommands have actions that come after the subcommand in turn. Finally, arguments follow to form a full invocation. Within a set of named arguments at the end of an invocation ordering is generally not important. Many invocations also have a required positional argument of the type that the command refers to. Again, the ordering of this positional argument relative to named arguments is not important.\nSome example full invocations:\npolaris principals list polaris catalogs delete some_catalog_name polaris catalogs update --property foo=bar some_other_catalog polaris catalogs update another_catalog --property k=v polaris privileges namespace grant --namespace some.schema --catalog fourth_catalog --catalog-role some_catalog_role TABLE_READ_DATA polaris profiles list polaris repair Authentication As outlined above, the Polaris CLI may take credentials using the --client-id and --client-secret options. For example:\npolaris --client-id 4b5ed1ca908c3cc2 --client-secret 07ea8e4edefb9a9e57c247e8d1a4f51c principals ... If --client-id and --client-secret are not provided, the Polaris CLI will try to read the client ID and client secret from environment variables called CLIENT_ID and CLIENT_SECRET respectively. If these flags are not provided and the environment variables are not set, the CLI will fail.\nAlternatively, the --access-token option can be used instead of --client-id and --client-secret, but both authentication methods cannot be used simultaneously.\nAdditionally, the --profile option can be used to specify a saved profile instead of providing authentication details directly. If --profile is not provided, the CLI will check the CLIENT_PROFILE environment variable. Profiles store authentication details and connection settings, simplifying repeated CLI usage.\nIf the --host and --port options are not provided, the CLI will default to communicating with localhost:8181.\nAlternatively, the --base-url option can be used instead of --host and --port, but both options cannot be used simultaneously. This allows specifying arbitrary Polaris URLs, including HTTPS ones, that have additional base prefixes before the /api/*/v1 subpaths.\nPATH These examples assume the Polaris CLI is on the PATH and so can be invoked just by the command polaris. You can add the CLI to your PATH environment variable with a command like the following:\nexport PATH=\"~/polaris:$PATH\" Alternatively, you can run the CLI by providing a path to it, such as with the following invocation:\n~/polaris principals list Commands Each of the commands catalogs, principals, principal-roles, catalog-roles, and privileges is used to manage a different type of entity within Polaris.\nIn addition to these, the profiles command is available for managing stored authentication profiles, allowing login credentials to be configured for reuse. This provides an alternative to passing authentication details with every command.\nTo find details on the options that can be provided to a particular command or subcommand ad-hoc, you may wish to use the --help flag. For example:\npolaris catalogs --help polaris principals create --help polaris profiles --help catalogs The catalogs command is used to create, discover, and otherwise manage catalogs within Polaris.\ncatalogs supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a catalog.\ninput: polaris catalogs create --help options: create Named arguments: --type The type of catalog to create in [INTERNAL, EXTERNAL]. INTERNAL by default. --storage-type (Required) The type of storage to use for the catalog --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --role-arn (Required for S3) A role ARN to use when connecting to S3 --region (Only for S3) The region to use when connecting to S3 --external-id (Only for S3) The external ID to use when connecting to S3 --tenant-id (Required for Azure) A tenant ID to use when connecting to Azure Storage --multi-tenant-app-name (Only for Azure) The app name to use when connecting to Azure Storage --consent-url (Only for Azure) A consent URL granting permissions for the Azure Storage location --service-account (Only for GCS) The service account to use when connecting to GCS --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once --catalog-connection-type The type of external catalog in [ICEBERG, HADOOP]. --iceberg-remote-catalog-name The remote catalog name when federating to an Iceberg REST catalog --hadoop-warehouse The warehouse to use when federating to a HADOOP catalog --catalog-authentication-type The type of authentication in [OAUTH, BEARER, SIGV4] --catalog-service-identity-type The type of service identity in [AWS_IAM] --catalog-service-identity-iam-arn When using the AWS_IAM service identity type, this is the ARN of the IAM user or IAM role Polaris uses to assume roles and then access external resources. --catalog-uri The URI of the external catalog --catalog-token-uri (For authentication type OAUTH) Token server URI --catalog-client-id (For authentication type OAUTH) oauth client id --catalog-client-secret (For authentication type OAUTH) oauth client secret (input-only) --catalog-client-scope (For authentication type OAUTH) oauth scopes to specify when exchanging for a short-lived access token. Multiple can be provided by specifying this option more than once --catalog-bearer-token (For authentication type BEARER) Bearer token (input-only) --catalog-role-arn (For authentication type SIGV4) The aws IAM role arn assumed by polaris userArn when signing requests --catalog-role-session-name (For authentication type SIGV4) The role session name to be used by the SigV4 protocol for signing requests --catalog-external-id (For authentication type SIGV4) An optional external id used to establish a trust relationship with AWS in the trust policy --catalog-signing-region (For authentication type SIGV4) Region to be used by the SigV4 protocol for signing requests --catalog-signing-name (For authentication type SIGV4) The service name to be used by the SigV4 protocol for signing requests, the default signing name is \"execute-api\" is if not provided Positional arguments: catalog Examples polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_data \\ --role-arn ${ROLE_ARN} \\ my_catalog polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_other_data \\ --allowed-location s3://example-bucket/second_location \\ --allowed-location s3://other-bucket/third_location \\ --role-arn ${ROLE_ARN} \\ my_other_catalog polaris catalogs create \\ --storage-type file \\ --default-base-location file:///example/tmp \\ quickstart_catalog delete The delete subcommand is used to delete a catalog.\ninput: polaris catalogs delete --help options: delete Positional arguments: catalog Examples polaris catalogs delete some_catalog get The get subcommand is used to retrieve details about a catalog.\ninput: polaris catalogs get --help options: get Positional arguments: catalog Examples polaris catalogs get some_catalog polaris catalogs get another_catalog list The list subcommand is used to show details about all catalogs, or those that a certain principal role has access to. The principal used to perform this operation must have the CATALOG_LIST privilege.\ninput: polaris catalogs list --help options: list Named arguments: --principal-role The name of a principal role Examples polaris catalogs list polaris catalogs list --principal-role some_user update The update subcommand is used to update a catalog. Currently, this command supports changing the properties of a catalog or updating its storage configuration.\ninput: polaris catalogs update --help options: update Named arguments: --default-base-location A new default base location for the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --region (Only for S3) The region to use when connecting to S3 --set-property A key/value pair such as: tag=value. Merges the specified key/value into an existing properties map by updating the value if the key already exists or creating a new entry if not. Multiple can be provided by specifying this option more than once --remove-property A key to remove from a properties map. If the key already does not exist then no action is takn for the specified key. If properties are also being set in the same update command then the list of removals is applied last. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs update --property tag=new_value my_catalog polaris catalogs update --default-base-location s3://new-bucket/my_data my_catalog Principals The principals command is used to manage principals within Polaris.\nprincipals supports the following subcommands:\ncreate delete get list rotate-credentials update access create The create subcommand is used to create a new principal.\ninput: polaris principals create --help options: create Named arguments: --type The type of principal to create in [SERVICE] --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals create some_user polaris principals create --client-id ${CLIENT_ID} --property admin=true some_admin_user delete The delete subcommand is used to delete a principal.\ninput: polaris principals delete --help options: delete Positional arguments: principal Examples polaris principals delete some_user polaris principals delete some_admin_user get The get subcommand retrieves details about a principal.\ninput: polaris principals get --help options: get Positional arguments: principal Examples polaris principals get some_user polaris principals get some_admin_user list The list subcommand shows details about all principals.\nExamples polaris principals list rotate-credentials The rotate-credentials subcommand is used to update the credentials used by a principal. After this command runs successfully, the new credentials will be printed to stdout.\ninput: polaris principals rotate-credentials --help options: rotate-credentials Positional arguments: principal Examples polaris principals rotate-credentials some_user polaris principals rotate-credentials some_admin_user update The update subcommand is used to update a principal. Currently, this supports rewriting the properties associated with a principal.\ninput: polaris principals update --help options: update Named arguments: --set-property A key/value pair such as: tag=value. Merges the specified key/value into an existing properties map by updating the value if the key already exists or creating a new entry if not. Multiple can be provided by specifying this option more than once --remove-property A key to remove from a properties map. If the key already does not exist then no action is takn for the specified key. If properties are also being set in the same update command then the list of removals is applied last. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals update --property key=value --property other_key=other_value some_user polaris principals update --property are_other_keys_removed=yes some_user access The access subcommand retrieves entities relation about a principal.\ninput: polaris principals access --help options: access Positional arguments: principal Examples polaris principals access quickstart_user Principal Roles The principal-roles command is used to create, discover, and manage principal roles within Polaris. Additionally, this command can identify principals or catalog roles associated with a principal role, and can be used to grant a principal role to a principal.\nprincipal-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new principal role.\ninput: polaris principal-roles create --help options: create Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles create data_engineer polaris principal-roles create --property key=value data_analyst delete The delete subcommand is used to delete a principal role.\ninput: polaris principal-roles delete --help options: delete Positional arguments: principal_role Examples polaris principal-roles delete data_engineer polaris principal-roles delete data_analyst get The get subcommand retrieves details about a principal role.\ninput: polaris principal-roles get --help options: get Positional arguments: principal_role Examples polaris principal-roles get data_engineer polaris principal-roles get data_analyst list The list subcommand is used to print out all principal roles or, alternatively, to list all principal roles associated with a given principal or with a given catalog role.\ninput: polaris principal-roles list --help options: list Named arguments: --catalog-role The name of a catalog role. If provided, show only principal roles assigned to this catalog role. --principal The name of a principal. If provided, show only principal roles assigned to this principal. Examples polaris principal-roles list polaris principal-roles --principal d.knuth polaris principal-roles --catalog-role super_secret_data update The update subcommand is used to update a principal role. Currently, this supports updating the properties tied to a principal role.\ninput: polaris principal-roles update --help options: update Named arguments: --set-property A key/value pair such as: tag=value. Merges the specified key/value into an existing properties map by updating the value if the key already exists or creating a new entry if not. Multiple can be provided by specifying this option more than once --remove-property A key to remove from a properties map. If the key already does not exist then no action is takn for the specified key. If properties are also being set in the same update command then the list of removals is applied last. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles update --property key=value2 data_engineer polaris principal-roles update data_analyst --property key=value3 grant The grant subcommand is used to grant a principal role to a principal.\ninput: polaris principal-roles grant --help options: grant Named arguments: --principal A principal to grant this principal role to Positional arguments: principal_role Examples polaris principal-roles grant --principal d.knuth data_engineer polaris principal-roles grant data_scientist --principal a.ng revoke The revoke subcommand is used to revoke a principal role from a principal.\ninput: polaris principal-roles revoke --help options: revoke Named arguments: --principal A principal to revoke this principal role from Positional arguments: principal_role Examples polaris principal-roles revoke --principal former.employee data_engineer polaris principal-roles revoke data_scientist --principal changed.role Catalog Roles The catalog-roles command is used to create, discover, and manage catalog roles within Polaris. Additionally, this command can be used to grant a catalog role to a principal role.\ncatalog-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new catalog role.\ninput: polaris catalog-roles create --help options: create Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles create --property key=value --catalog some_catalog sales_data polaris catalog-roles create --catalog other_catalog sales_data delete The delete subcommand is used to delete a catalog role.\ninput: polaris catalog-roles delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles delete --catalog some_catalog sales_data polaris catalog-roles delete --catalog other_catalog sales_data get The get subcommand retrieves details about a catalog role.\ninput: polaris catalog-roles get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles get --catalog some_catalog inventory_data polaris catalog-roles get --catalog other_catalog inventory_data list The list subcommand is used to print all catalog roles. Alternatively, if a principal role is provided, only catalog roles associated with that principal are shown.\ninput: polaris catalog-roles list --help options: list Named arguments: --principal-role The name of a principal role Positional arguments: catalog Examples polaris catalog-roles list polaris catalog-roles list --principal-role data_engineer update The update subcommand is used to update a catalog role. Currently, only updating properties associated with the catalog role is supported.\ninput: polaris catalog-roles update --help options: update Named arguments: --catalog The name of an existing catalog --set-property A key/value pair such as: tag=value. Merges the specified key/value into an existing properties map by updating the value if the key already exists or creating a new entry if not. Multiple can be provided by specifying this option more than once --remove-property A key to remove from a properties map. If the key already does not exist then no action is takn for the specified key. If properties are also being set in the same update command then the list of removals is applied last. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles update --property contains_pii=true --catalog some_catalog sales_data polaris catalog-roles update sales_data --catalog some_catalog --property key=value grant The grant subcommand is used to grant a catalog role to a principal role.\ninput: polaris catalog-roles grant --help options: grant Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles grant sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles grant --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role revoke The revoke subcommand is used to revoke a catalog role from a principal role.\ninput: polaris catalog-roles revoke --help options: revoke Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles revoke sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles revoke --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role Namespaces The namespaces command is used to manage namespaces within Polaris.\nnamespaces supports the following subcommands:\ncreate delete get list create The create subcommand is used to create a new namespace.\nWhen creating a namespace with an explicit location, that location must reside within the parent catalog or namespace.\ninput: polaris namespaces create --help options: create Named arguments: --catalog The name of an existing catalog --location If specified, the location at which to store the namespace and entities inside it --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: namespace Examples polaris namespaces create --catalog my_catalog outer polaris namespaces create --catalog my_catalog --location 's3://bucket/outer/inner_SUFFIX' outer.inner delete The delete subcommand is used to delete a namespace.\ninput: polaris namespaces delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces delete outer_namespace.inner_namespace --catalog my_catalog polaris namespaces delete --catalog my_catalog outer_namespace get The get subcommand retrieves details about a namespace.\ninput: polaris namespaces get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces get --catalog some_catalog a.b polaris namespaces get a.b.c --catalog some_catalog list The list subcommand shows details about all namespaces directly within a catalog or, optionally, within some parent prefix in that catalog.\ninput: polaris namespaces list --help options: list Named arguments: --catalog The name of an existing catalog --parent If specified, list namespaces inside this parent namespace Examples polaris namespaces list --catalog my_catalog polaris namespaces list --catalog my_catalog --parent a polaris namespaces list --catalog my_catalog --parent a.b Privileges The privileges command is used to grant various privileges to a catalog role, or to revoke those privileges. Privileges can be on the level of a catalog, a namespace, a table, or a view. For more information on privileges, please refer to the docs.\nNote that when using the privileges command, the user specifies the relevant catalog and catalog role before selecting a subcommand.\nprivileges supports the following subcommands:\nlist catalog namespace table view Each of these subcommands, except list, supports the grant and revoke actions and requires an action to be specified.\nNote that each subcommand’s revoke action always accepts the same options that the corresponding grant action does, but with the addition of the cascade option. cascade is used to revoke all other privileges that depend on the specified privilege.\nlist The list subcommand shows details about all privileges for a catalog role.\ninput: polaris privileges list --help options: list Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Examples polaris privileges list --catalog my_catalog --catalog-role my_role polaris privileges my_role list --catalog-role my_other_role --catalog my_catalog catalog The catalog subcommand manages privileges at the catalog level. grant is used to grant catalog privileges to the specified catalog role, and revoke is used to revoke them.\ninput: polaris privileges catalog --help options: catalog grant Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ catalog \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ TABLE_CREATE polaris privileges \\ catalog \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --cascade \\ TABLE_CREATE namespace The namespace subcommand manages privileges at the namespace level.\ninput: polaris privileges namespace --help options: namespace grant Named arguments: --namespace A period-delimited namespace --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ namespace \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST polaris privileges \\ namespace \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST table The table subcommand manages privileges at the table level.\ninput: polaris privileges table --help options: table grant Named arguments: --namespace A period-delimited namespace --table The name of a table --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --table The name of a table --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ TABLE_DROP polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ --cascade \\ TABLE_DROP view The view subcommand manages privileges at the view level.\ninput: polaris privileges view --help options: view grant Named arguments: --namespace A period-delimited namespace --view The name of a view --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --view The name of a view --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ VIEW_FULL_METADATA polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ --cascade \\ VIEW_FULL_METADATA profiles The profiles command is used to manage stored authentication profiles in Polaris. Profiles allow authentication credentials to be saved and reused, eliminating the need to pass credentials with every command.\nprofiles supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a new authentication profile.\ninput: polaris profiles create --help options: create Positional arguments: profile Examples polaris profiles create dev delete The delete subcommand removes a stored profile.\ninput: polaris profiles delete --help options: delete Positional arguments: profile Examples polaris profiles delete dev get The get subcommand removes a stored profile.\ninput: polaris profiles get --help options: get Positional arguments: profile Examples polaris profiles get dev list The list subcommand displays all stored profiles.\ninput: polaris profiles list --help options: list Examples polaris profiles list update The update subcommand modifies an existing profile.\ninput: polaris profiles update --help options: update Positional arguments: profile Examples polaris profiles update dev repair The repair command is a bash script wrapper used to regenerate Python client code and update necessary dependencies, ensuring the Polaris client remains up-to-date and functional. Please note that this command does not support any options and its usage information is not available via a --help flag.\nExamples This section outlines example code for a few common operations as well as for some more complex ones.\nFor especially complex operations, you may wish to instead directly use the Python API.\nCreating a principal and a catalog polaris principals create my_user polaris catalogs create \\ --type internal \\ --storage-type s3 \\ --default-base-location s3://iceberg-bucket/polaris-base \\ --role-arn arn:aws:iam::111122223333:role/ExampleCorpRole \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-1 \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-2 \\ my_catalog Granting a principal the ability to manage the content of a catalog polaris principal-roles create power_user polaris principal-roles grant --principal my_user power_user polaris catalog-roles create --catalog my_catalog my_catalog_role polaris catalog-roles grant \\ --catalog my_catalog \\ --principal-role power_user \\ my_catalog_role polaris privileges \\ catalog \\ --catalog my_catalog \\ --catalog-role my_catalog_role \\ grant \\ CATALOG_MANAGE_CONTENT Identifying the tables a given principal has been granted explicit access to read Note that some other privileges, such as CATALOG_MANAGE_CONTENT, subsume TABLE_READ_DATA and would not be discovered here.\nprincipal_roles=$(polaris principal-roles list --principal my_principal) for principal_role in ${principal_roles}; do catalog_roles=$(polaris catalog-roles --list --principal-role \"${principal_role}\") for catalog_role in ${catalog_roles}; do grants=$(polaris privileges list --catalog-role \"${catalog_role}\" --catalog \"${catalog}\") for grant in $(echo \"${grants}\" | jq -c '.[] | select(.privilege == \"TABLE_READ_DATA\")'); do echo \"${grant}\" done done done ","categories":"","description":"","excerpt":"In order to help administrators quickly set up and manage their …","ref":"/in-dev/unreleased/command-line-interface/","tags":"","title":"Command Line Interface"},{"body":"In order to help administrators quickly set up and manage their Polaris server, Polaris provides a simple command-line interface (CLI) for common tasks.\nThe basic syntax of the Polaris CLI is outlined below:\npolaris [options] COMMAND ... options: --host --port --client-id --client-secret COMMAND must be one of the following:\ncatalogs principals principal-roles catalog-roles namespaces privileges Each command supports several subcommands, and some subcommands have actions that come after the subcommand in turn. Finally, arguments follow to form a full invocation. Within a set of named arguments at the end of an invocation ordering is generally not important. Many invocations also have a required positional argument of the type that the command refers to. Again, the ordering of this positional argument relative to named arguments is not important.\nSome example full invocations:\npolaris principals list polaris catalogs delete some_catalog_name polaris catalogs update --property foo=bar some_other_catalog polaris catalogs update another_catalog --property k=v polaris privileges namespace grant --namespace some.schema --catalog fourth_catalog --catalog-role some_catalog_role TABLE_READ_DATA Authentication As outlined above, the Polaris CLI may take credentials using the --client-id and --client-secret options. For example:\npolaris --client-id 4b5ed1ca908c3cc2 --client-secret 07ea8e4edefb9a9e57c247e8d1a4f51c principals ... If --client-id and --client-secret are not provided, the Polaris CLI will try to read the client ID and client secret from environment variables called CLIENT_ID and CLIENT_SECRET respectively. If these flags are not provided and the environment variables are not set, the CLI will fail.\nIf the --host and --port options are not provided, the CLI will default to communicating with localhost:8181.\nPATH These examples assume the Polaris CLI is on the PATH and so can be invoked just by the command polaris. You can add the CLI to your PATH environment variable with a command like the following:\nexport PATH=\"~/polaris:$PATH\" Alternatively, you can run the CLI by providing a path to it, such as with the following invocation:\n~/polaris principals list Commands Each of the commands catalogs, principals, principal-roles, catalog-roles, and privileges is used to manage a different type of entity within Polaris.\nTo find details on the options that can be provided to a particular command or subcommand ad-hoc, you may wish to use the --help flag. For example:\npolaris catalogs --help polaris principals create --help catalogs The catalogs command is used to create, discover, and otherwise manage catalogs within Polaris.\ncatalogs supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a catalog.\ninput: polaris catalogs create --help options: create Named arguments: --type The type of catalog to create in [INTERNAL, EXTERNAL]. INTERNAL by default. --storage-type (Required) The type of storage to use for the catalog --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --role-arn (Required for S3) A role ARN to use when connecting to S3 --external-id (Only for S3) The external ID to use when connecting to S3 --tenant-id (Required for Azure) A tenant ID to use when connecting to Azure Storage --multi-tenant-app-name (Only for Azure) The app name to use when connecting to Azure Storage --consent-url (Only for Azure) A consent URL granting permissions for the Azure Storage location --service-account (Only for GCS) The service account to use when connecting to GCS --remote-url (For external catalogs) The remote URL to use --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_data \\ --role-arn ${ROLE_ARN} \\ my_catalog polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_other_data \\ --allowed-location s3://example-bucket/second_location \\ --allowed-location s3://other-bucket/third_location \\ --role-arn ${ROLE_ARN} \\ my_other_catalog delete The delete subcommand is used to delete a catalog.\ninput: polaris catalogs delete --help options: delete Positional arguments: catalog Examples polaris catalogs delete some_catalog get The get subcommand is used to retrieve details about a catalog.\ninput: polaris catalogs get --help options: get Positional arguments: catalog Examples polaris catalogs get some_catalog polaris catalogs get another_catalog list The list subcommand is used to show details about all catalogs, or those that a certain principal role has access to. The principal used to perform this operation must have the CATALOG_LIST privilege.\ninput: polaris catalogs list --help options: list Named arguments: --principal-role The name of a principal role Examples polaris catalogs list polaris catalogs list --principal-role some_user update The update subcommand is used to update a catalog. Currently, this command supports changing the properties of a catalog or updating its storage configuration.\ninput: polaris catalogs update --help options: update Named arguments: --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs update --property tag=new_value my_catalog polaris catalogs update --default-base-location s3://new-bucket/my_data my_catalog Principals The principals command is used to manage principals within Polaris.\nprincipals supports the following subcommands:\ncreate delete get list rotate-credentials update create The create subcommand is used to create a new principal.\ninput: polaris principals create --help options: create Named arguments: --type The type of principal to create in [SERVICE] --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals create some_user polaris principals create --client-id ${CLIENT_ID} --property admin=true some_admin_user delete The delete subcommand is used to delete a principal.\ninput: polaris principals delete --help options: delete Positional arguments: principal Examples polaris principals delete some_user polaris principals delete some_admin_user get The get subcommand retrieves details about a principal.\ninput: polaris principals get --help options: get Positional arguments: principal Examples polaris principals get some_user polaris principals get some_admin_user list The list subcommand shows details about all principals.\nExamples polaris principals list rotate-credentials The rotate-credentials subcommand is used to update the credentials used by a principal. After this command runs successfully, the new credentials will be printed to stdout.\ninput: polaris principals rotate-credentials --help options: rotate-credentials Positional arguments: principal Examples polaris principals rotate-credentials some_user polaris principals rotate-credentials some_admin_user update The update subcommand is used to update a principal. Currently, this supports rewriting the properties associated with a principal.\ninput: polaris principals update --help options: update Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals update --property key=value --property other_key=other_value some_user polaris principals update --property are_other_keys_removed=yes some_user Principal Roles The principal-roles command is used to create, discover, and manage principal roles within Polaris. Additionally, this command can identify principals or catalog roles associated with a principal role, and can be used to grant a principal role to a principal.\nprincipal-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new principal role.\ninput: polaris principal-roles create --help options: create Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles create data_engineer polaris principal-roles create --property key=value data_analyst delete The delete subcommand is used to delete a principal role.\ninput: polaris principal-roles delete --help options: delete Positional arguments: principal_role Examples polaris principal-roles delete data_engineer polaris principal-roles delete data_analyst get The get subcommand retrieves details about a principal role.\ninput: polaris principal-roles get --help options: get Positional arguments: principal_role Examples polaris principal-roles get data_engineer polaris principal-roles get data_analyst list The list subcommand is used to print out all principal roles or, alternatively, to list all principal roles associated with a given principal or with a given catalog role.\ninput: polaris principal-roles list --help options: list Named arguments: --catalog-role The name of a catalog role. If provided, show only principal roles assigned to this catalog role. --principal The name of a principal. If provided, show only principal roles assigned to this principal. Examples polaris principal-roles list polaris principal-roles --principal d.knuth polaris principal-roles --catalog-role super_secret_data update The update subcommand is used to update a principal role. Currently, this supports updating the properties tied to a principal role.\ninput: polaris principal-roles update --help options: update Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles update --property key=value2 data_engineer polaris principal-roles update data_analyst --property key=value3 grant The grant subcommand is used to grant a principal role to a principal.\ninput: polaris principal-roles grant --help options: grant Named arguments: --principal A principal to grant this principal role to Positional arguments: principal_role Examples polaris principal-roles grant --principal d.knuth data_engineer polaris principal-roles grant data_scientist --principal a.ng revoke The revoke subcommand is used to revoke a principal role from a principal.\ninput: polaris principal-roles revoke --help options: revoke Named arguments: --principal A principal to revoke this principal role from Positional arguments: principal_role Examples polaris principal-roles revoke --principal former.employee data_engineer polaris principal-roles revoke data_scientist --principal changed.role Catalog Roles The catalog-roles command is used to create, discover, and manage catalog roles within Polaris. Additionally, this command can be used to grant a catalog role to a principal role.\ncatalog-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new catalog role.\ninput: polaris catalog-roles create --help options: create Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles create --property key=value --catalog some_catalog sales_data polaris catalog-roles create --catalog other_catalog sales_data delete The delete subcommand is used to delete a catalog role.\ninput: polaris catalog-roles delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles delete --catalog some_catalog sales_data polaris catalog-roles delete --catalog other_catalog sales_data get The get subcommand retrieves details about a catalog role.\ninput: polaris catalog-roles get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles get --catalog some_catalog inventory_data polaris catalog-roles get --catalog other_catalog inventory_data list The list subcommand is used to print all catalog roles. Alternatively, if a principal role is provided, only catalog roles associated with that principal are shown.\ninput: polaris catalog-roles list --help options: list Named arguments: --principal-role The name of a principal role Positional arguments: catalog Examples polaris catalog-roles list polaris catalog-roles list --principal-role data_engineer update The update subcommand is used to update a catalog role. Currently, only updating properties associated with the catalog role is supported.\ninput: polaris catalog-roles update --help options: update Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles update --property contains_pii=true --catalog some_catalog sales_data polaris catalog-roles update sales_data --catalog some_catalog --property key=value grant The grant subcommand is used to grant a catalog role to a principal role.\ninput: polaris catalog-roles grant --help options: grant Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles grant sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles grant --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role revoke The revoke subcommand is used to revoke a catalog role from a principal role.\ninput: polaris catalog-roles revoke --help options: revoke Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles revoke sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles revoke --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role Namespaces The namespaces command is used to manage namespaces within Polaris.\nnamespaces supports the following subcommands:\ncreate delete get list create The create subcommand is used to create a new namespace.\nWhen creating a namespace with an explicit location, that location must reside within the parent catalog or namespace.\ninput: polaris namespaces create --help options: create Named arguments: --catalog The name of an existing catalog --location If specified, the location at which to store the namespace and entities inside it --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: namespace Examples polaris namespaces create --catalog my_catalog outer polaris namespaces create --catalog my_catalog --location 's3://bucket/outer/inner_SUFFIX' outer.inner delete The delete subcommand is used to delete a namespace.\ninput: polaris namespaces delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces delete outer_namespace.inner_namespace --catalog my_catalog polaris namespaces delete --catalog my_catalog outer_namespace get The get subcommand retrieves details about a namespace.\ninput: polaris namespaces get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces get --catalog some_catalog a.b polaris namespaces get a.b.c --catalog some_catalog list The list subcommand shows details about all namespaces directly within a catalog or, optionally, within some parent prefix in that catalog.\ninput: polaris namespaces list --help options: list Named arguments: --catalog The name of an existing catalog --parent If specified, list namespaces inside this parent namespace Examples polaris namespaces list --catalog my_catalog polaris namespaces list --catalog my_catalog --parent a polaris namespaces list --catalog my_catalog --parent a.b Privileges The privileges command is used to grant various privileges to a catalog role, or to revoke those privileges. Privileges can be on the level of a catalog, a namespace, a table, or a view. For more information on privileges, please refer to the docs.\nNote that when using the privileges command, the user specifies the relevant catalog and catalog role before selecting a subcommand.\nprivileges supports the following subcommands:\nlist catalog namespace table view Each of these subcommands, except list, supports the grant and revoke actions and requires an action to be specified.\nNote that each subcommand’s revoke action always accepts the same options that the corresponding grant action does, but with the addition of the cascade option. cascade is used to revoke all other privileges that depend on the specified privilege.\nlist The list subcommand shows details about all privileges for a catalog role.\ninput: polaris privileges list --help options: list Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Examples polaris privileges list --catalog my_catalog --catalog-role my_role polaris privileges my_role list --catalog-role my_other_role --catalog my_catalog catalog The catalog subcommand manages privileges at the catalog level. grant is used to grant catalog privileges to the specified catalog role, and revoke is used to revoke them.\ninput: polaris privileges catalog --help options: catalog grant Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ catalog \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ TABLE_CREATE polaris privileges \\ catalog \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --cascade \\ TABLE_CREATE namespace The namespace subcommand manages privileges at the namespace level.\ninput: polaris privileges namespace --help options: namespace grant Named arguments: --namespace A period-delimited namespace --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ namespace \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST polaris privileges \\ namespace \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST table The table subcommand manages privileges at the table level.\ninput: polaris privileges table --help options: table grant Named arguments: --namespace A period-delimited namespace --table The name of a table --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --table The name of a table --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ TABLE_DROP polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ --cascade \\ TABLE_DROP view The view subcommand manages privileges at the view level.\ninput: polaris privileges view --help options: view grant Named arguments: --namespace A period-delimited namespace --view The name of a view --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --view The name of a view --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ VIEW_FULL_METADATA polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ --cascade \\ VIEW_FULL_METADATA Examples This section outlines example code for a few common operations as well as for some more complex ones.\nFor especially complex operations, you may wish to instead directly use the Python API.\nCreating a principal and a catalog polaris principals create my_user polaris catalogs create \\ --type internal \\ --storage-type s3 \\ --default-base-location s3://iceberg-bucket/polaris-base \\ --role-arn arn:aws:iam::111122223333:role/ExampleCorpRole \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-1 \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-2 \\ my_catalog Granting a principal the ability to manage the content of a catalog polaris principal-roles create power_user polaris principal-roles grant --principal my_user power_user polaris catalog-roles create --catalog my_catalog my_catalog_role polaris catalog-roles grant \\ --catalog my_catalog \\ --principal-role power_user \\ my_catalog_role polaris privileges \\ catalog \\ --catalog my_catalog \\ --catalog-role my_catalog_role \\ grant \\ CATALOG_MANAGE_CONTENT Identifying the tables a given principal has been granted explicit access to read Note that some other privileges, such as CATALOG_MANAGE_CONTENT, subsume TABLE_READ_DATA and would not be discovered here.\nprincipal_roles=$(polaris principal-roles list --principal my_principal) for principal_role in ${principal_roles}; do catalog_roles=$(polaris catalog-roles --list --principal-role \"${principal_role}\") for catalog_role in ${catalog_roles}; do grants=$(polaris privileges list --catalog-role \"${catalog_role}\" --catalog \"${catalog}\") for grant in $(echo \"${grants}\" | jq -c '.[] | select(.privilege == \"TABLE_READ_DATA\")'); do echo \"${grant}\" fi done done done ","categories":"","description":"","excerpt":"In order to help administrators quickly set up and manage their …","ref":"/releases/0.9.0/command-line-interface/","tags":"","title":"Apache Polaris (Incubating) CLI"},{"body":"In order to help administrators quickly set up and manage their Polaris server, Polaris provides a simple command-line interface (CLI) for common tasks.\nThe basic syntax of the Polaris CLI is outlined below:\npolaris [options] COMMAND ... options: --host --port --base-url --client-id --client-secret --access-token --profile COMMAND must be one of the following:\ncatalogs principals principal-roles catalog-roles namespaces privileges profiles Each command supports several subcommands, and some subcommands have actions that come after the subcommand in turn. Finally, arguments follow to form a full invocation. Within a set of named arguments at the end of an invocation ordering is generally not important. Many invocations also have a required positional argument of the type that the command refers to. Again, the ordering of this positional argument relative to named arguments is not important.\nSome example full invocations:\npolaris principals list polaris catalogs delete some_catalog_name polaris catalogs update --property foo=bar some_other_catalog polaris catalogs update another_catalog --property k=v polaris privileges namespace grant --namespace some.schema --catalog fourth_catalog --catalog-role some_catalog_role TABLE_READ_DATA polaris profiles list Authentication As outlined above, the Polaris CLI may take credentials using the --client-id and --client-secret options. For example:\npolaris --client-id 4b5ed1ca908c3cc2 --client-secret 07ea8e4edefb9a9e57c247e8d1a4f51c principals ... If --client-id and --client-secret are not provided, the Polaris CLI will try to read the client ID and client secret from environment variables called CLIENT_ID and CLIENT_SECRET respectively. If these flags are not provided and the environment variables are not set, the CLI will fail.\nAlternatively, the --access-token option can be used instead of --client-id and --client-secret, but both authentication methods cannot be used simultaneously.\nAdditionally, the --profile option can be used to specify a saved profile instead of providing authentication details directly. If --profile is not provided, the CLI will check the CLIENT_PROFILE environment variable. Profiles store authentication details and connection settings, simplifying repeated CLI usage.\nIf the --host and --port options are not provided, the CLI will default to communicating with localhost:8181.\nAlternatively, the --base-url option can be used instead of --host and --port, but both options cannot be used simultaneously. This allows specifying arbitrary Polaris URLs, including HTTPS ones, that have additional base prefixes before the /api/*/v1 subpaths.\nPATH These examples assume the Polaris CLI is on the PATH and so can be invoked just by the command polaris. You can add the CLI to your PATH environment variable with a command like the following:\nexport PATH=\"~/polaris:$PATH\" Alternatively, you can run the CLI by providing a path to it, such as with the following invocation:\n~/polaris principals list Commands Each of the commands catalogs, principals, principal-roles, catalog-roles, and privileges is used to manage a different type of entity within Polaris.\nIn addition to these, the profiles command is available for managing stored authentication profiles, allowing login credentials to be configured for reuse. This provides an alternative to passing authentication details with every command.\nTo find details on the options that can be provided to a particular command or subcommand ad-hoc, you may wish to use the --help flag. For example:\npolaris catalogs --help polaris principals create --help polaris profiles --help catalogs The catalogs command is used to create, discover, and otherwise manage catalogs within Polaris.\ncatalogs supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a catalog.\ninput: polaris catalogs create --help options: create Named arguments: --type The type of catalog to create in [INTERNAL, EXTERNAL]. INTERNAL by default. --storage-type (Required) The type of storage to use for the catalog --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --role-arn (Required for S3) A role ARN to use when connecting to S3 --external-id (Only for S3) The external ID to use when connecting to S3 --tenant-id (Required for Azure) A tenant ID to use when connecting to Azure Storage --multi-tenant-app-name (Only for Azure) The app name to use when connecting to Azure Storage --consent-url (Only for Azure) A consent URL granting permissions for the Azure Storage location --service-account (Only for GCS) The service account to use when connecting to GCS --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_data \\ --role-arn ${ROLE_ARN} \\ my_catalog polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_other_data \\ --allowed-location s3://example-bucket/second_location \\ --allowed-location s3://other-bucket/third_location \\ --role-arn ${ROLE_ARN} \\ my_other_catalog polaris catalogs create \\ --storage-type file \\ --default-base-location file:///example/tmp \\ quickstart_catalog delete The delete subcommand is used to delete a catalog.\ninput: polaris catalogs delete --help options: delete Positional arguments: catalog Examples polaris catalogs delete some_catalog get The get subcommand is used to retrieve details about a catalog.\ninput: polaris catalogs get --help options: get Positional arguments: catalog Examples polaris catalogs get some_catalog polaris catalogs get another_catalog list The list subcommand is used to show details about all catalogs, or those that a certain principal role has access to. The principal used to perform this operation must have the CATALOG_LIST privilege.\ninput: polaris catalogs list --help options: list Named arguments: --principal-role The name of a principal role Examples polaris catalogs list polaris catalogs list --principal-role some_user update The update subcommand is used to update a catalog. Currently, this command supports changing the properties of a catalog or updating its storage configuration.\ninput: polaris catalogs update --help options: update Named arguments: --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs update --property tag=new_value my_catalog polaris catalogs update --default-base-location s3://new-bucket/my_data my_catalog Principals The principals command is used to manage principals within Polaris.\nprincipals supports the following subcommands:\ncreate delete get list rotate-credentials update access create The create subcommand is used to create a new principal.\ninput: polaris principals create --help options: create Named arguments: --type The type of principal to create in [SERVICE] --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals create some_user polaris principals create --client-id ${CLIENT_ID} --property admin=true some_admin_user delete The delete subcommand is used to delete a principal.\ninput: polaris principals delete --help options: delete Positional arguments: principal Examples polaris principals delete some_user polaris principals delete some_admin_user get The get subcommand retrieves details about a principal.\ninput: polaris principals get --help options: get Positional arguments: principal Examples polaris principals get some_user polaris principals get some_admin_user list The list subcommand shows details about all principals.\nExamples polaris principals list rotate-credentials The rotate-credentials subcommand is used to update the credentials used by a principal. After this command runs successfully, the new credentials will be printed to stdout.\ninput: polaris principals rotate-credentials --help options: rotate-credentials Positional arguments: principal Examples polaris principals rotate-credentials some_user polaris principals rotate-credentials some_admin_user update The update subcommand is used to update a principal. Currently, this supports rewriting the properties associated with a principal.\ninput: polaris principals update --help options: update Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals update --property key=value --property other_key=other_value some_user polaris principals update --property are_other_keys_removed=yes some_user access The access subcommand retrieves entities relation about a principal.\ninput: polaris principals access --help options: access Positional arguments: principal Examples polaris principals access quickstart_user Principal Roles The principal-roles command is used to create, discover, and manage principal roles within Polaris. Additionally, this command can identify principals or catalog roles associated with a principal role, and can be used to grant a principal role to a principal.\nprincipal-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new principal role.\ninput: polaris principal-roles create --help options: create Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles create data_engineer polaris principal-roles create --property key=value data_analyst delete The delete subcommand is used to delete a principal role.\ninput: polaris principal-roles delete --help options: delete Positional arguments: principal_role Examples polaris principal-roles delete data_engineer polaris principal-roles delete data_analyst get The get subcommand retrieves details about a principal role.\ninput: polaris principal-roles get --help options: get Positional arguments: principal_role Examples polaris principal-roles get data_engineer polaris principal-roles get data_analyst list The list subcommand is used to print out all principal roles or, alternatively, to list all principal roles associated with a given principal or with a given catalog role.\ninput: polaris principal-roles list --help options: list Named arguments: --catalog-role The name of a catalog role. If provided, show only principal roles assigned to this catalog role. --principal The name of a principal. If provided, show only principal roles assigned to this principal. Examples polaris principal-roles list polaris principal-roles --principal d.knuth polaris principal-roles --catalog-role super_secret_data update The update subcommand is used to update a principal role. Currently, this supports updating the properties tied to a principal role.\ninput: polaris principal-roles update --help options: update Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles update --property key=value2 data_engineer polaris principal-roles update data_analyst --property key=value3 grant The grant subcommand is used to grant a principal role to a principal.\ninput: polaris principal-roles grant --help options: grant Named arguments: --principal A principal to grant this principal role to Positional arguments: principal_role Examples polaris principal-roles grant --principal d.knuth data_engineer polaris principal-roles grant data_scientist --principal a.ng revoke The revoke subcommand is used to revoke a principal role from a principal.\ninput: polaris principal-roles revoke --help options: revoke Named arguments: --principal A principal to revoke this principal role from Positional arguments: principal_role Examples polaris principal-roles revoke --principal former.employee data_engineer polaris principal-roles revoke data_scientist --principal changed.role Catalog Roles The catalog-roles command is used to create, discover, and manage catalog roles within Polaris. Additionally, this command can be used to grant a catalog role to a principal role.\ncatalog-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new catalog role.\ninput: polaris catalog-roles create --help options: create Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles create --property key=value --catalog some_catalog sales_data polaris catalog-roles create --catalog other_catalog sales_data delete The delete subcommand is used to delete a catalog role.\ninput: polaris catalog-roles delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles delete --catalog some_catalog sales_data polaris catalog-roles delete --catalog other_catalog sales_data get The get subcommand retrieves details about a catalog role.\ninput: polaris catalog-roles get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles get --catalog some_catalog inventory_data polaris catalog-roles get --catalog other_catalog inventory_data list The list subcommand is used to print all catalog roles. Alternatively, if a principal role is provided, only catalog roles associated with that principal are shown.\ninput: polaris catalog-roles list --help options: list Named arguments: --principal-role The name of a principal role Positional arguments: catalog Examples polaris catalog-roles list polaris catalog-roles list --principal-role data_engineer update The update subcommand is used to update a catalog role. Currently, only updating properties associated with the catalog role is supported.\ninput: polaris catalog-roles update --help options: update Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles update --property contains_pii=true --catalog some_catalog sales_data polaris catalog-roles update sales_data --catalog some_catalog --property key=value grant The grant subcommand is used to grant a catalog role to a principal role.\ninput: polaris catalog-roles grant --help options: grant Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles grant sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles grant --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role revoke The revoke subcommand is used to revoke a catalog role from a principal role.\ninput: polaris catalog-roles revoke --help options: revoke Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles revoke sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles revoke --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role Namespaces The namespaces command is used to manage namespaces within Polaris.\nnamespaces supports the following subcommands:\ncreate delete get list create The create subcommand is used to create a new namespace.\nWhen creating a namespace with an explicit location, that location must reside within the parent catalog or namespace.\ninput: polaris namespaces create --help options: create Named arguments: --catalog The name of an existing catalog --location If specified, the location at which to store the namespace and entities inside it --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: namespace Examples polaris namespaces create --catalog my_catalog outer polaris namespaces create --catalog my_catalog --location 's3://bucket/outer/inner_SUFFIX' outer.inner delete The delete subcommand is used to delete a namespace.\ninput: polaris namespaces delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces delete outer_namespace.inner_namespace --catalog my_catalog polaris namespaces delete --catalog my_catalog outer_namespace get The get subcommand retrieves details about a namespace.\ninput: polaris namespaces get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces get --catalog some_catalog a.b polaris namespaces get a.b.c --catalog some_catalog list The list subcommand shows details about all namespaces directly within a catalog or, optionally, within some parent prefix in that catalog.\ninput: polaris namespaces list --help options: list Named arguments: --catalog The name of an existing catalog --parent If specified, list namespaces inside this parent namespace Examples polaris namespaces list --catalog my_catalog polaris namespaces list --catalog my_catalog --parent a polaris namespaces list --catalog my_catalog --parent a.b Privileges The privileges command is used to grant various privileges to a catalog role, or to revoke those privileges. Privileges can be on the level of a catalog, a namespace, a table, or a view. For more information on privileges, please refer to the docs.\nNote that when using the privileges command, the user specifies the relevant catalog and catalog role before selecting a subcommand.\nprivileges supports the following subcommands:\nlist catalog namespace table view Each of these subcommands, except list, supports the grant and revoke actions and requires an action to be specified.\nNote that each subcommand’s revoke action always accepts the same options that the corresponding grant action does, but with the addition of the cascade option. cascade is used to revoke all other privileges that depend on the specified privilege.\nlist The list subcommand shows details about all privileges for a catalog role.\ninput: polaris privileges list --help options: list Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Examples polaris privileges list --catalog my_catalog --catalog-role my_role polaris privileges my_role list --catalog-role my_other_role --catalog my_catalog catalog The catalog subcommand manages privileges at the catalog level. grant is used to grant catalog privileges to the specified catalog role, and revoke is used to revoke them.\ninput: polaris privileges catalog --help options: catalog grant Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ catalog \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ TABLE_CREATE polaris privileges \\ catalog \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --cascade \\ TABLE_CREATE namespace The namespace subcommand manages privileges at the namespace level.\ninput: polaris privileges namespace --help options: namespace grant Named arguments: --namespace A period-delimited namespace --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ namespace \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST polaris privileges \\ namespace \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST table The table subcommand manages privileges at the table level.\ninput: polaris privileges table --help options: table grant Named arguments: --namespace A period-delimited namespace --table The name of a table --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --table The name of a table --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ TABLE_DROP polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ --cascade \\ TABLE_DROP view The view subcommand manages privileges at the view level.\ninput: polaris privileges view --help options: view grant Named arguments: --namespace A period-delimited namespace --view The name of a view --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --view The name of a view --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ VIEW_FULL_METADATA polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ --cascade \\ VIEW_FULL_METADATA profiles The profiles command is used to manage stored authentication profiles in Polaris. Profiles allow authentication credentials to be saved and reused, eliminating the need to pass credentials with every command.\nprofiles supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a new authentication profile.\ninput: polaris profiles create --help options: create Positional arguments: profile Examples polaris profiles create dev delete The delete subcommand removes a stored profile.\ninput: polaris profiles delete --help options: delete Positional arguments: profile Examples polaris profiles delete dev get The get subcommand removes a stored profile.\ninput: polaris profiles get --help options: get Positional arguments: profile Examples polaris profiles get dev list The list subcommand displays all stored profiles.\ninput: polaris profiles list --help options: list Examples polaris profiles list update The update subcommand modifies an existing profile.\ninput: polaris profiles update --help options: update Positional arguments: profile Examples polaris profiles update dev Examples This section outlines example code for a few common operations as well as for some more complex ones.\nFor especially complex operations, you may wish to instead directly use the Python API.\nCreating a principal and a catalog polaris principals create my_user polaris catalogs create \\ --type internal \\ --storage-type s3 \\ --default-base-location s3://iceberg-bucket/polaris-base \\ --role-arn arn:aws:iam::111122223333:role/ExampleCorpRole \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-1 \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-2 \\ my_catalog Granting a principal the ability to manage the content of a catalog polaris principal-roles create power_user polaris principal-roles grant --principal my_user power_user polaris catalog-roles create --catalog my_catalog my_catalog_role polaris catalog-roles grant \\ --catalog my_catalog \\ --principal-role power_user \\ my_catalog_role polaris privileges \\ catalog \\ --catalog my_catalog \\ --catalog-role my_catalog_role \\ grant \\ CATALOG_MANAGE_CONTENT Identifying the tables a given principal has been granted explicit access to read Note that some other privileges, such as CATALOG_MANAGE_CONTENT, subsume TABLE_READ_DATA and would not be discovered here.\nprincipal_roles=$(polaris principal-roles list --principal my_principal) for principal_role in ${principal_roles}; do catalog_roles=$(polaris catalog-roles --list --principal-role \"${principal_role}\") for catalog_role in ${catalog_roles}; do grants=$(polaris privileges list --catalog-role \"${catalog_role}\" --catalog \"${catalog}\") for grant in $(echo \"${grants}\" | jq -c '.[] | select(.privilege == \"TABLE_READ_DATA\")'); do echo \"${grant}\" fi done done done ","categories":"","description":"","excerpt":"In order to help administrators quickly set up and manage their …","ref":"/releases/1.0.0/command-line-interface/","tags":"","title":"Command Line Interface"},{"body":"In order to help administrators quickly set up and manage their Polaris server, Polaris provides a simple command-line interface (CLI) for common tasks.\nThe basic syntax of the Polaris CLI is outlined below:\npolaris [options] COMMAND ... options: --host --port --base-url --client-id --client-secret --access-token --profile COMMAND must be one of the following:\ncatalogs principals principal-roles catalog-roles namespaces privileges profiles Each command supports several subcommands, and some subcommands have actions that come after the subcommand in turn. Finally, arguments follow to form a full invocation. Within a set of named arguments at the end of an invocation ordering is generally not important. Many invocations also have a required positional argument of the type that the command refers to. Again, the ordering of this positional argument relative to named arguments is not important.\nSome example full invocations:\npolaris principals list polaris catalogs delete some_catalog_name polaris catalogs update --property foo=bar some_other_catalog polaris catalogs update another_catalog --property k=v polaris privileges namespace grant --namespace some.schema --catalog fourth_catalog --catalog-role some_catalog_role TABLE_READ_DATA polaris profiles list Authentication As outlined above, the Polaris CLI may take credentials using the --client-id and --client-secret options. For example:\npolaris --client-id 4b5ed1ca908c3cc2 --client-secret 07ea8e4edefb9a9e57c247e8d1a4f51c principals ... If --client-id and --client-secret are not provided, the Polaris CLI will try to read the client ID and client secret from environment variables called CLIENT_ID and CLIENT_SECRET respectively. If these flags are not provided and the environment variables are not set, the CLI will fail.\nAlternatively, the --access-token option can be used instead of --client-id and --client-secret, but both authentication methods cannot be used simultaneously.\nAdditionally, the --profile option can be used to specify a saved profile instead of providing authentication details directly. If --profile is not provided, the CLI will check the CLIENT_PROFILE environment variable. Profiles store authentication details and connection settings, simplifying repeated CLI usage.\nIf the --host and --port options are not provided, the CLI will default to communicating with localhost:8181.\nAlternatively, the --base-url option can be used instead of --host and --port, but both options cannot be used simultaneously. This allows specifying arbitrary Polaris URLs, including HTTPS ones, that have additional base prefixes before the /api/*/v1 subpaths.\nPATH These examples assume the Polaris CLI is on the PATH and so can be invoked just by the command polaris. You can add the CLI to your PATH environment variable with a command like the following:\nexport PATH=\"~/polaris:$PATH\" Alternatively, you can run the CLI by providing a path to it, such as with the following invocation:\n~/polaris principals list Commands Each of the commands catalogs, principals, principal-roles, catalog-roles, and privileges is used to manage a different type of entity within Polaris.\nIn addition to these, the profiles command is available for managing stored authentication profiles, allowing login credentials to be configured for reuse. This provides an alternative to passing authentication details with every command.\nTo find details on the options that can be provided to a particular command or subcommand ad-hoc, you may wish to use the --help flag. For example:\npolaris catalogs --help polaris principals create --help polaris profiles --help catalogs The catalogs command is used to create, discover, and otherwise manage catalogs within Polaris.\ncatalogs supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a catalog.\ninput: polaris catalogs create --help options: create Named arguments: --type The type of catalog to create in [INTERNAL, EXTERNAL]. INTERNAL by default. --storage-type (Required) The type of storage to use for the catalog --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --role-arn (Required for S3) A role ARN to use when connecting to S3 --external-id (Only for S3) The external ID to use when connecting to S3 --tenant-id (Required for Azure) A tenant ID to use when connecting to Azure Storage --multi-tenant-app-name (Only for Azure) The app name to use when connecting to Azure Storage --consent-url (Only for Azure) A consent URL granting permissions for the Azure Storage location --service-account (Only for GCS) The service account to use when connecting to GCS --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_data \\ --role-arn ${ROLE_ARN} \\ my_catalog polaris catalogs create \\ --storage-type s3 \\ --default-base-location s3://example-bucket/my_other_data \\ --allowed-location s3://example-bucket/second_location \\ --allowed-location s3://other-bucket/third_location \\ --role-arn ${ROLE_ARN} \\ my_other_catalog polaris catalogs create \\ --storage-type file \\ --default-base-location file:///example/tmp \\ quickstart_catalog delete The delete subcommand is used to delete a catalog.\ninput: polaris catalogs delete --help options: delete Positional arguments: catalog Examples polaris catalogs delete some_catalog get The get subcommand is used to retrieve details about a catalog.\ninput: polaris catalogs get --help options: get Positional arguments: catalog Examples polaris catalogs get some_catalog polaris catalogs get another_catalog list The list subcommand is used to show details about all catalogs, or those that a certain principal role has access to. The principal used to perform this operation must have the CATALOG_LIST privilege.\ninput: polaris catalogs list --help options: list Named arguments: --principal-role The name of a principal role Examples polaris catalogs list polaris catalogs list --principal-role some_user update The update subcommand is used to update a catalog. Currently, this command supports changing the properties of a catalog or updating its storage configuration.\ninput: polaris catalogs update --help options: update Named arguments: --default-base-location (Required) Default base location of the catalog --allowed-location An allowed location for files tracked by the catalog. Multiple locations can be provided by specifying this option more than once. --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog Examples polaris catalogs update --property tag=new_value my_catalog polaris catalogs update --default-base-location s3://new-bucket/my_data my_catalog Principals The principals command is used to manage principals within Polaris.\nprincipals supports the following subcommands:\ncreate delete get list rotate-credentials update access create The create subcommand is used to create a new principal.\ninput: polaris principals create --help options: create Named arguments: --type The type of principal to create in [SERVICE] --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals create some_user polaris principals create --client-id ${CLIENT_ID} --property admin=true some_admin_user delete The delete subcommand is used to delete a principal.\ninput: polaris principals delete --help options: delete Positional arguments: principal Examples polaris principals delete some_user polaris principals delete some_admin_user get The get subcommand retrieves details about a principal.\ninput: polaris principals get --help options: get Positional arguments: principal Examples polaris principals get some_user polaris principals get some_admin_user list The list subcommand shows details about all principals.\nExamples polaris principals list rotate-credentials The rotate-credentials subcommand is used to update the credentials used by a principal. After this command runs successfully, the new credentials will be printed to stdout.\ninput: polaris principals rotate-credentials --help options: rotate-credentials Positional arguments: principal Examples polaris principals rotate-credentials some_user polaris principals rotate-credentials some_admin_user update The update subcommand is used to update a principal. Currently, this supports rewriting the properties associated with a principal.\ninput: polaris principals update --help options: update Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal Examples polaris principals update --property key=value --property other_key=other_value some_user polaris principals update --property are_other_keys_removed=yes some_user access The access subcommand retrieves entities relation about a principal.\ninput: polaris principals access --help options: access Positional arguments: principal Examples polaris principals access quickstart_user Principal Roles The principal-roles command is used to create, discover, and manage principal roles within Polaris. Additionally, this command can identify principals or catalog roles associated with a principal role, and can be used to grant a principal role to a principal.\nprincipal-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new principal role.\ninput: polaris principal-roles create --help options: create Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles create data_engineer polaris principal-roles create --property key=value data_analyst delete The delete subcommand is used to delete a principal role.\ninput: polaris principal-roles delete --help options: delete Positional arguments: principal_role Examples polaris principal-roles delete data_engineer polaris principal-roles delete data_analyst get The get subcommand retrieves details about a principal role.\ninput: polaris principal-roles get --help options: get Positional arguments: principal_role Examples polaris principal-roles get data_engineer polaris principal-roles get data_analyst list The list subcommand is used to print out all principal roles or, alternatively, to list all principal roles associated with a given principal or with a given catalog role.\ninput: polaris principal-roles list --help options: list Named arguments: --catalog-role The name of a catalog role. If provided, show only principal roles assigned to this catalog role. --principal The name of a principal. If provided, show only principal roles assigned to this principal. Examples polaris principal-roles list polaris principal-roles --principal d.knuth polaris principal-roles --catalog-role super_secret_data update The update subcommand is used to update a principal role. Currently, this supports updating the properties tied to a principal role.\ninput: polaris principal-roles update --help options: update Named arguments: --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: principal_role Examples polaris principal-roles update --property key=value2 data_engineer polaris principal-roles update data_analyst --property key=value3 grant The grant subcommand is used to grant a principal role to a principal.\ninput: polaris principal-roles grant --help options: grant Named arguments: --principal A principal to grant this principal role to Positional arguments: principal_role Examples polaris principal-roles grant --principal d.knuth data_engineer polaris principal-roles grant data_scientist --principal a.ng revoke The revoke subcommand is used to revoke a principal role from a principal.\ninput: polaris principal-roles revoke --help options: revoke Named arguments: --principal A principal to revoke this principal role from Positional arguments: principal_role Examples polaris principal-roles revoke --principal former.employee data_engineer polaris principal-roles revoke data_scientist --principal changed.role Catalog Roles The catalog-roles command is used to create, discover, and manage catalog roles within Polaris. Additionally, this command can be used to grant a catalog role to a principal role.\ncatalog-roles supports the following subcommands:\ncreate delete get list update grant revoke create The create subcommand is used to create a new catalog role.\ninput: polaris catalog-roles create --help options: create Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles create --property key=value --catalog some_catalog sales_data polaris catalog-roles create --catalog other_catalog sales_data delete The delete subcommand is used to delete a catalog role.\ninput: polaris catalog-roles delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles delete --catalog some_catalog sales_data polaris catalog-roles delete --catalog other_catalog sales_data get The get subcommand retrieves details about a catalog role.\ninput: polaris catalog-roles get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: catalog_role Examples polaris catalog-roles get --catalog some_catalog inventory_data polaris catalog-roles get --catalog other_catalog inventory_data list The list subcommand is used to print all catalog roles. Alternatively, if a principal role is provided, only catalog roles associated with that principal are shown.\ninput: polaris catalog-roles list --help options: list Named arguments: --principal-role The name of a principal role Positional arguments: catalog Examples polaris catalog-roles list polaris catalog-roles list --principal-role data_engineer update The update subcommand is used to update a catalog role. Currently, only updating properties associated with the catalog role is supported.\ninput: polaris catalog-roles update --help options: update Named arguments: --catalog The name of an existing catalog --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: catalog_role Examples polaris catalog-roles update --property contains_pii=true --catalog some_catalog sales_data polaris catalog-roles update sales_data --catalog some_catalog --property key=value grant The grant subcommand is used to grant a catalog role to a principal role.\ninput: polaris catalog-roles grant --help options: grant Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles grant sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles grant --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role revoke The revoke subcommand is used to revoke a catalog role from a principal role.\ninput: polaris catalog-roles revoke --help options: revoke Named arguments: --catalog The name of an existing catalog --principal-role The name of a catalog role Positional arguments: catalog_role Examples polaris catalog-roles revoke sensitive_data --catalog some_catalog --principal-role power_user polaris catalog-roles revoke --catalog sales_data contains_cc_info_catalog_role --principal-role financial_analyst_role Namespaces The namespaces command is used to manage namespaces within Polaris.\nnamespaces supports the following subcommands:\ncreate delete get list create The create subcommand is used to create a new namespace.\nWhen creating a namespace with an explicit location, that location must reside within the parent catalog or namespace.\ninput: polaris namespaces create --help options: create Named arguments: --catalog The name of an existing catalog --location If specified, the location at which to store the namespace and entities inside it --property A key/value pair such as: tag=value. Multiple can be provided by specifying this option more than once Positional arguments: namespace Examples polaris namespaces create --catalog my_catalog outer polaris namespaces create --catalog my_catalog --location 's3://bucket/outer/inner_SUFFIX' outer.inner delete The delete subcommand is used to delete a namespace.\ninput: polaris namespaces delete --help options: delete Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces delete outer_namespace.inner_namespace --catalog my_catalog polaris namespaces delete --catalog my_catalog outer_namespace get The get subcommand retrieves details about a namespace.\ninput: polaris namespaces get --help options: get Named arguments: --catalog The name of an existing catalog Positional arguments: namespace Examples polaris namespaces get --catalog some_catalog a.b polaris namespaces get a.b.c --catalog some_catalog list The list subcommand shows details about all namespaces directly within a catalog or, optionally, within some parent prefix in that catalog.\ninput: polaris namespaces list --help options: list Named arguments: --catalog The name of an existing catalog --parent If specified, list namespaces inside this parent namespace Examples polaris namespaces list --catalog my_catalog polaris namespaces list --catalog my_catalog --parent a polaris namespaces list --catalog my_catalog --parent a.b Privileges The privileges command is used to grant various privileges to a catalog role, or to revoke those privileges. Privileges can be on the level of a catalog, a namespace, a table, or a view. For more information on privileges, please refer to the docs.\nNote that when using the privileges command, the user specifies the relevant catalog and catalog role before selecting a subcommand.\nprivileges supports the following subcommands:\nlist catalog namespace table view Each of these subcommands, except list, supports the grant and revoke actions and requires an action to be specified.\nNote that each subcommand’s revoke action always accepts the same options that the corresponding grant action does, but with the addition of the cascade option. cascade is used to revoke all other privileges that depend on the specified privilege.\nlist The list subcommand shows details about all privileges for a catalog role.\ninput: polaris privileges list --help options: list Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Examples polaris privileges list --catalog my_catalog --catalog-role my_role polaris privileges my_role list --catalog-role my_other_role --catalog my_catalog catalog The catalog subcommand manages privileges at the catalog level. grant is used to grant catalog privileges to the specified catalog role, and revoke is used to revoke them.\ninput: polaris privileges catalog --help options: catalog grant Named arguments: --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ catalog \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ TABLE_CREATE polaris privileges \\ catalog \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --cascade \\ TABLE_CREATE namespace The namespace subcommand manages privileges at the namespace level.\ninput: polaris privileges namespace --help options: namespace grant Named arguments: --namespace A period-delimited namespace --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ namespace \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST polaris privileges \\ namespace \\ revoke \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ TABLE_LIST table The table subcommand manages privileges at the table level.\ninput: polaris privileges table --help options: table grant Named arguments: --namespace A period-delimited namespace --table The name of a table --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --table The name of a table --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ TABLE_DROP polaris privileges \\ table \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b \\ --table t \\ --cascade \\ TABLE_DROP view The view subcommand manages privileges at the view level.\ninput: polaris privileges view --help options: view grant Named arguments: --namespace A period-delimited namespace --view The name of a view --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege revoke Named arguments: --namespace A period-delimited namespace --view The name of a view --cascade When revoking privileges, additionally revoke privileges that depend on the specified privilege --catalog The name of an existing catalog --catalog-role The name of a catalog role Positional arguments: privilege Examples polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ VIEW_FULL_METADATA polaris privileges \\ view \\ grant \\ --catalog my_catalog \\ --catalog-role catalog_role \\ --namespace a.b.c \\ --view v \\ --cascade \\ VIEW_FULL_METADATA profiles The profiles command is used to manage stored authentication profiles in Polaris. Profiles allow authentication credentials to be saved and reused, eliminating the need to pass credentials with every command.\nprofiles supports the following subcommands:\ncreate delete get list update create The create subcommand is used to create a new authentication profile.\ninput: polaris profiles create --help options: create Positional arguments: profile Examples polaris profiles create dev delete The delete subcommand removes a stored profile.\ninput: polaris profiles delete --help options: delete Positional arguments: profile Examples polaris profiles delete dev get The get subcommand removes a stored profile.\ninput: polaris profiles get --help options: get Positional arguments: profile Examples polaris profiles get dev list The list subcommand displays all stored profiles.\ninput: polaris profiles list --help options: list Examples polaris profiles list update The update subcommand modifies an existing profile.\ninput: polaris profiles update --help options: update Positional arguments: profile Examples polaris profiles update dev Examples This section outlines example code for a few common operations as well as for some more complex ones.\nFor especially complex operations, you may wish to instead directly use the Python API.\nCreating a principal and a catalog polaris principals create my_user polaris catalogs create \\ --type internal \\ --storage-type s3 \\ --default-base-location s3://iceberg-bucket/polaris-base \\ --role-arn arn:aws:iam::111122223333:role/ExampleCorpRole \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-1 \\ --allowed-location s3://iceberg-bucket/polaris-alt-location-2 \\ my_catalog Granting a principal the ability to manage the content of a catalog polaris principal-roles create power_user polaris principal-roles grant --principal my_user power_user polaris catalog-roles create --catalog my_catalog my_catalog_role polaris catalog-roles grant \\ --catalog my_catalog \\ --principal-role power_user \\ my_catalog_role polaris privileges \\ catalog \\ --catalog my_catalog \\ --catalog-role my_catalog_role \\ grant \\ CATALOG_MANAGE_CONTENT Identifying the tables a given principal has been granted explicit access to read Note that some other privileges, such as CATALOG_MANAGE_CONTENT, subsume TABLE_READ_DATA and would not be discovered here.\nprincipal_roles=$(polaris principal-roles list --principal my_principal) for principal_role in ${principal_roles}; do catalog_roles=$(polaris catalog-roles --list --principal-role \"${principal_role}\") for catalog_role in ${catalog_roles}; do grants=$(polaris privileges list --catalog-role \"${catalog_role}\" --catalog \"${catalog}\") for grant in $(echo \"${grants}\" | jq -c '.[] | select(.privilege == \"TABLE_READ_DATA\")'); do echo \"${grant}\" fi done done done ","categories":"","description":"","excerpt":"In order to help administrators quickly set up and manage their …","ref":"/releases/1.0.1/command-line-interface/","tags":"","title":"Command Line Interface"},{"body":" Learn, Connect and Collaborate\nSlack\nPublic chat, open to everybody\nCommunity Meetings\nUpcoming, live and recorded Community Meetings\nProposals and Roadmap\nProposals for important features and Polaris Roadmap\nGitHub\nDevelopment takes place here!\nCode of Conduct\nCode of Conduct\nChat Bylaws\nA few rules around our public chat as a collaboration tool for the project.\nContribution Guidelines\nHow to contribute to Apache Polaris\nAll Mailing Lists\nDevelopment oriented content\nSubscribe to dev@\nMailing List Archives\nNotifications about GitHub issues and PRs\nSubscribe to issues@\nMailing List Archives\nNotifications about Git commits\nSubscribe to commits@\nMailing List Archives\nTeam\nAjantha Bhat\nCommitter\nDremio\nAlex Dutra\nCommitter\nDremio\nAnna Filippova\nCommitter\nSnowflake\nAnoop Johnson\nPPMC Member\nGoogle\nAshvin Agrawal\nPPMC Member\nMicrosoft\nBertrand Delacretaz\nMentor\nDennis Huo\nPPMC Member\nSnowflake\nDmitri Bourlatchkov\nPPMC Member\nDremio\nEric Maynard\nCommitter\nSnowflake\nHolden Karau\nMentor\nJack Ye\nPPMC Member\nLanceDB\nJB Onofre\nPPMC Member \u0026 Mentor\nDremio\nJohn Roesler\nPPMC Member\nConfluent\nJonas Jiang\nCommitter\nSnowflake\nKent Yao\nMentor\nMichael Collado\nCommitter\nSnowflake\nPierre Laporte\nCommitter\nDremio\nPrashant Singh\nCommitter\nSnowflake\nRobert Stupp\nPPMC Member\nDremio\nRussell Spitzer\nPPMC\nSnowflake\nRyan Blue\nMentor\nTyler Akidau\nPPMC Member\nSnowflake\nYong Zheng\nCommitter\nYufei Gu\nPPMC Member\nSnowflake\nYuya Ebihara\nCommitter\nStarburst\n","categories":"","description":"","excerpt":" Learn, Connect and Collaborate\nSlack\nPublic chat, open to everybody …","ref":"/community/","tags":"","title":"Apache Polaris Community"},{"body":"We will now demonstrate how to deploy Polaris locally, as well as with all supported Cloud Providers: Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP).\nLocally, Polaris can be deployed using both Docker and local build. On the cloud, this tutorial will deploy Polaris using Docker only - but local builds can also be executed.\n","categories":"","description":"","excerpt":"We will now demonstrate how to deploy Polaris locally, as well as with …","ref":"/in-dev/unreleased/getting-started/deploying-polaris/","tags":"","title":"Deploying Polaris on Cloud Providers"},{"body":"We will now demonstrate how to deploy Polaris locally, as well as with all supported Cloud Providers: Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP).\nLocally, Polaris can be deployed using both Docker and local build. On the cloud, this tutorial will deploy Polaris using Docker only - but local builds can also be executed.\n","categories":"","description":"","excerpt":"We will now demonstrate how to deploy Polaris locally, as well as with …","ref":"/releases/1.0.0/getting-started/deploying-polaris/","tags":"","title":"Deploying Polaris on Cloud Providers"},{"body":"We will now demonstrate how to deploy Polaris locally, as well as with all supported Cloud Providers: Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP).\nLocally, Polaris can be deployed using both Docker and local build. On the cloud, this tutorial will deploy Polaris using Docker only - but local builds can also be executed.\n","categories":"","description":"","excerpt":"We will now demonstrate how to deploy Polaris locally, as well as with …","ref":"/releases/1.0.1/getting-started/deploying-polaris/","tags":"","title":"Deploying Polaris on Cloud Providers"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start an Amazon RDS for PostgreSQL instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nThere must be at least two subnets created in the VPC and region in which your EC2 instance reside. The span of subnets MUST include at least 2 availability zones (AZs) within the same region. Your EC2 instance must be enabled with IMDSv1 or IMDSv2 with 2+ hop limit. The AWS identity that you will use to run this script must have the following AWS permissions: “ec2:DescribeInstances” “rds:CreateDBInstance” “rds:DescribeDBInstances” “rds:CreateDBSubnetGroup” “sts:AssumeRole” on the same role as the Instance Profile role of the EC2 instance on which you are running this script. Additionally, you should ensure that the Instance Profile contains a trust policy that allows the role to trust itself to be assumed. chmod +x getting-started/assets/cloud_providers/deploy-aws.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-aws.sh Next Steps Congrats, you now have a running instance of1 Polaris! For details on how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/in-dev/unreleased/getting-started/deploying-polaris/quickstart-deploy-aws/","tags":"","title":"Deploying Polaris on Amazon Web Services (AWS)"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start an Amazon RDS for PostgreSQL instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nThere must be at least two subnets created in the VPC and region in which your EC2 instance reside. The span of subnets MUST include at least 2 availability zones (AZs) within the same region. Your EC2 instance must be enabled with IMDSv1 or IMDSv2 with 2+ hop limit. The AWS identity that you will use to run this script must have the following AWS permissions: “ec2:DescribeInstances” “rds:CreateDBInstance” “rds:DescribeDBInstances” “rds:CreateDBSubnetGroup” “sts:AssumeRole” on the same role as the Instance Profile role of the EC2 instance on which you are running this script. Additionally, you should ensure that the Instance Profile contains a trust policy that allows the role to trust itself to be assumed. chmod +x getting-started/assets/cloud_providers/deploy-aws.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-aws.sh Next Steps Congrats, you now have a running instance of1 Polaris! For details on how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/releases/1.0.0/getting-started/deploying-polaris/quickstart-deploy-aws/","tags":"","title":"Deploying Polaris on Amazon Web Services (AWS)"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start an Amazon RDS for PostgreSQL instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nThere must be at least two subnets created in the VPC and region in which your EC2 instance reside. The span of subnets MUST include at least 2 availability zones (AZs) within the same region. Your EC2 instance must be enabled with IMDSv1 or IMDSv2 with 2+ hop limit. The AWS identity that you will use to run this script must have the following AWS permissions: “ec2:DescribeInstances” “rds:CreateDBInstance” “rds:DescribeDBInstances” “rds:CreateDBSubnetGroup” “sts:AssumeRole” on the same role as the Instance Profile role of the EC2 instance on which you are running this script. Additionally, you should ensure that the Instance Profile contains a trust policy that allows the role to trust itself to be assumed. chmod +x getting-started/assets/cloud_providers/deploy-aws.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-aws.sh Next Steps Congrats, you now have a running instance of1 Polaris! For details on how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/releases/1.0.1/getting-started/deploying-polaris/quickstart-deploy-aws/","tags":"","title":"Deploying Polaris on Amazon Web Services (AWS)"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start an Azure Database for PostgreSQL - Flexible Server instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nInstall the AZ CLI, if it is not already installed on the Azure VM. Instructions to download the AZ CLI can be found here. You must be logged into the AZ CLI. Please run az account show to ensure that you are logged in prior to running this script. Assign a System-Assigned Managed Identity to the Azure VM. chmod +x getting-started/assets/cloud_providers/deploy-azure.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-azure.sh Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/in-dev/unreleased/getting-started/deploying-polaris/quickstart-deploy-azure/","tags":"","title":"Deploying Polaris on Azure"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start an Azure Database for PostgreSQL - Flexible Server instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nInstall the AZ CLI, if it is not already installed on the Azure VM. Instructions to download the AZ CLI can be found here. You must be logged into the AZ CLI. Please run az account show to ensure that you are logged in prior to running this script. Assign a System-Assigned Managed Identity to the Azure VM. chmod +x getting-started/assets/cloud_providers/deploy-azure.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-azure.sh Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/releases/1.0.0/getting-started/deploying-polaris/quickstart-deploy-azure/","tags":"","title":"Deploying Polaris on Azure"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start an Azure Database for PostgreSQL - Flexible Server instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nInstall the AZ CLI, if it is not already installed on the Azure VM. Instructions to download the AZ CLI can be found here. You must be logged into the AZ CLI. Please run az account show to ensure that you are logged in prior to running this script. Assign a System-Assigned Managed Identity to the Azure VM. chmod +x getting-started/assets/cloud_providers/deploy-azure.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-azure.sh Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/releases/1.0.1/getting-started/deploying-polaris/quickstart-deploy-azure/","tags":"","title":"Deploying Polaris on Azure"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start a Cloud SQL for PostgreSQL instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nInstall the gcloud CLI, if it is not already installed on the GCP VM. Instructions to download the gcloud CLI can be found here. Ensure the Cloud SQL Admin API has been enabled in your project and that your VM’s Principal has access to the correct role: roles/cloudsql.admin. Ensure the VM’s Principal has access to at least Read-only scope on Compute Engine: compute.readonly. chmod +x getting-started/assets/cloud_providers/deploy-gcp.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-gcp.sh Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/in-dev/unreleased/getting-started/deploying-polaris/quickstart-deploy-gcp/","tags":"","title":"Deploying Polaris on Google Cloud Platform (GCP)"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start a Cloud SQL for PostgreSQL instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nInstall the gcloud CLI, if it is not already installed on the GCP VM. Instructions to download the gcloud CLI can be found here. Ensure the Cloud SQL Admin API has been enabled in your project and that your VM’s Principal has access to the correct role: roles/cloudsql.admin. Ensure the VM’s Principal has access to at least Read-only scope on Compute Engine: compute.readonly. chmod +x getting-started/assets/cloud_providers/deploy-gcp.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-gcp.sh Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/releases/1.0.0/getting-started/deploying-polaris/quickstart-deploy-gcp/","tags":"","title":"Deploying Polaris on Google Cloud Platform (GCP)"},{"body":"Build and launch Polaris using the AWS Startup Script at the location provided in the command below. This script will start a Cloud SQL for PostgreSQL instance, which will be used as the backend Postgres instance holding all Polaris data. Additionally, Polaris will be bootstrapped to use this database and Docker containers will be spun up for Spark SQL and Trino.\nThe requirements to run the script below are:\nInstall the gcloud CLI, if it is not already installed on the GCP VM. Instructions to download the gcloud CLI can be found here. Ensure the Cloud SQL Admin API has been enabled in your project and that your VM’s Principal has access to the correct role: roles/cloudsql.admin. Ensure the VM’s Principal has access to at least Read-only scope on Compute Engine: compute.readonly. chmod +x getting-started/assets/cloud_providers/deploy-gcp.sh export ASSETS_PATH=$(pwd)/getting-started/assets/ export CLIENT_ID=root export CLIENT_SECRET=s3cr3t ./getting-started/assets/cloud_providers/deploy-gcp.sh Next Steps Congrats, you now have a running instance of Polaris! For further information regarding how to use Polaris, check out the Using Polaris page.\nCleanup Instructions To shut down the Polaris server, run the following commands:\nexport ASSETS_PATH=$(pwd)/getting-started/assets/ docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml down To deploy Polaris in a production setting, please review further recommendations at the Configuring Polaris for Production page.\n","categories":"","description":"","excerpt":"Build and launch Polaris using the AWS Startup Script at the location …","ref":"/releases/1.0.1/getting-started/deploying-polaris/quickstart-deploy-gcp/","tags":"","title":"Deploying Polaris on Google Cloud Platform (GCP)"},{"body":"In this guide we walk through setting up a simple Polaris Server with local MinIO storage.\nSimilar configurations are expected to work with other S3-compatible systems that also have the STS API.\nSetup Clone the Polaris source repository, then build a docker image for Polaris.\n./gradlew :polaris-server:assemble -Dquarkus.container-image.build=true Start MinIO with Polaris using the docker compose example.\ndocker compose -f getting-started/minio/docker-compose.yml up The compose script will start MinIO on default ports (API on 9000, UI on 9001) plus a Polaris Server pre-configured to that MinIO instance.\nIn this example the root principal has its password set to s3cr3t.\nConnecting from Spark Start Spark.\nexport AWS_REGION=us-west-2 bin/spark-sql \\ --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.apache.iceberg:iceberg-aws-bundle:1.9.0 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.polaris=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.polaris.type=rest \\ --conf spark.sql.catalog.polaris.uri=http://localhost:8181/api/catalog \\ --conf spark.sql.catalog.polaris.token-refresh-enabled=false \\ --conf spark.sql.catalog.polaris.warehouse=quickstart_catalog \\ --conf spark.sql.catalog.polaris.scope=PRINCIPAL_ROLE:ALL \\ --conf spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.polaris.credential=root:s3cr3t Note: AWS_REGION is required by the AWS SDK used by Spark, but the value is irrelevant in this case.\nCreate a table in Spark.\nuse polaris; create namespace ns; create table ns.t1 as select 'abc'; select * from ns.t1; Connecting from MinIO client mc alias set pol http://localhost:9000 minio_root m1n1opwd mc ls pol/bucket123/ns/t1 [2025-08-13 18:52:38 EDT] 0B data/ [2025-08-13 18:52:38 EDT] 0B metadata/ Note: the values of minio_root, m1n1opwd and bucket123 are defined in the docker compose file.\nNotes on Storage Configuation In this example the Polaris Catalog is defined as (excluding uninteresting properties):\n{ \"name\": \"quickstart_catalog\", \"storageConfigInfo\": { \"endpoint\": \"http://localhost:9000\", \"endpointInternal\": \"http://minio:9000\", \"pathStyleAccess\": true, \"storageType\": \"S3\", \"allowedLocations\": [ \"s3://bucket123\" ] } } Note that the roleArn parameter, which is required for AWS storage, does not need to be set for MinIO.\nNote the two endpoint values. endpointInternal is used by the Polaris Server, while endpoint is communicated to clients (such as Spark) in Iceberg REST API responses. This distinction allows the system to work smoothly when the clients and the server have different views of the network (in this example the host name minio is resolvable only inside the docker compose environment).\n","categories":"","description":"","excerpt":"In this guide we walk through setting up a simple Polaris Server with …","ref":"/in-dev/unreleased/getting-started/minio/","tags":"","title":"Deploying Polaris on MinIO"},{"body":"This page explains what a realm is and what it is used for in Polaris.\nWhat is it? A realm in Polaris serves as logical partitioning mechanism within the catalog system. This isolation allows for multitenancy, enabling different teams, environments or organizations to operate independently within the same Polaris deployment.\nKey Characteristics Isolation: Each realm encapsulates its own set of resources, ensuring that operations, policies in one realm do not affect others.\nAuthentication Context: When configuring Polaris, principals credentials are associated with a specific realm. This allows for the separation of security concerns across different realms.\nConfiguration Scope: Realm identifiers are used in various configurations, such as connection strings feature configurations, etc.\nAn example of this is:\njdbc:postgresql://localhost:5432/{realm}\nThis ensures that each realm’s data is stored separately.\nHow is it used in the system? RealmContext: It is a key concept used to identify and resolve the context in which operations are performed. For example DefaultRealmContextResolver, a realm is resolved from request headers, and operations are performed based on the resolved realm identifier.\nAuthentication and Authorization: For example, in DefaultAuthenticator, RealmContext is used to provide context about the current security domain, which is used to retrieve the correct PolarisMetastoreManager that manages all Polaris entities and associated grant records metadata for authorization.\nIsolation: In methods like createEntityManagerFactory(@Nonnull RealmContext realmContext) from PolarisEclipseLinkPersistenceUnit interface, the realm context influence how resources are created or managed based on the security policies of that realm. An example of this is the way a realm name can be used to create a database connection url so that you have one database instance per realm, when applicable. Or it can be more granular and applied at primary key level (within the same database instance).\n","categories":"","description":"","excerpt":"This page explains what a realm is and what it is used for in Polaris. …","ref":"/in-dev/unreleased/realm/","tags":"","title":"Realm"},{"body":"This page explains what a realm is and what it is used for in Polaris.\nWhat is it? A realm in Polaris serves as logical partitioning mechanism within the catalog system. This isolation allows for multitenancy, enabling different teams, environments or organizations to operate independently within the same Polaris deployment.\nKey Characteristics Isolation: Each realm encapsulates its own set of resources, ensuring that operations, policies in one realm do not affect others.\nAuthentication Context: When configuring Polaris, principals credentials are associated with a specific realm. This allows for the separation of security concerns across different realms.\nConfiguration Scope: Realm identifiers are used in various configurations, such as connection strings feature configurations, etc.\nAn example of this is:\njdbc:postgresql://localhost:5432/{realm} This ensures that each realm’s data is stored separately.\nHow is it used in the system? RealmContext: It is a key concept used to identify and resolve the context in which operations are performed. For example DefaultRealmContextResolver, a realm is resolved from request headers, and operations are performed based on the resolved realm identifier.\nAuthentication and Authorization: For example, in BasePolarisAuthenticator, RealmContext is used to provide context about the current security domain, which is used to retrieve the correct PolarisMetastoreManager that manages all Polaris entities and associated grant records metadata for authorization.\nIsolation: In methods like createEntityManagerFactory(@Nonnull RealmContext realmContext) from PolarisEclipseLinkPersistenceUnit interface, the realm context influence how resources are created or managed based on the security policies of that realm. An example of this is the way a realm name can be used to create a database connection url so that you have one database instance per realm, when applicable. Or it can be more granular and applied at primary key level (within the same database instance).\n","categories":"","description":"","excerpt":"This page explains what a realm is and what it is used for in Polaris. …","ref":"/releases/1.0.0/realm/","tags":"","title":"Realm"},{"body":"This page explains what a realm is and what it is used for in Polaris.\nWhat is it? A realm in Polaris serves as logical partitioning mechanism within the catalog system. This isolation allows for multitenancy, enabling different teams, environments or organizations to operate independently within the same Polaris deployment.\nKey Characteristics Isolation: Each realm encapsulates its own set of resources, ensuring that operations, policies in one realm do not affect others.\nAuthentication Context: When configuring Polaris, principals credentials are associated with a specific realm. This allows for the separation of security concerns across different realms.\nConfiguration Scope: Realm identifiers are used in various configurations, such as connection strings feature configurations, etc.\nAn example of this is:\njdbc:postgresql://localhost:5432/{realm} This ensures that each realm’s data is stored separately.\nHow is it used in the system? RealmContext: It is a key concept used to identify and resolve the context in which operations are performed. For example DefaultRealmContextResolver, a realm is resolved from request headers, and operations are performed based on the resolved realm identifier.\nAuthentication and Authorization: For example, in BasePolarisAuthenticator, RealmContext is used to provide context about the current security domain, which is used to retrieve the correct PolarisMetastoreManager that manages all Polaris entities and associated grant records metadata for authorization.\nIsolation: In methods like createEntityManagerFactory(@Nonnull RealmContext realmContext) from PolarisEclipseLinkPersistenceUnit interface, the realm context influence how resources are created or managed based on the security policies of that realm. An example of this is the way a realm name can be used to create a database connection url so that you have one database instance per realm, when applicable. Or it can be more granular and applied at primary key level (within the same database instance).\n","categories":"","description":"","excerpt":"This page explains what a realm is and what it is used for in Polaris. …","ref":"/releases/1.0.1/realm/","tags":"","title":"Realm"},{"body":" Apache Polaris - Project Chat Bylaws Apache Polaris is currently undergoing Incubation at the Apache Software Foundation.\nMotivation Apache Polaris uses public Slack workspace (join here).\nA few rules shall ensure that the chat conforms to the rules and best practices of the Apache Software Foundation and serves well as a collaboration tool for the project.\nOrganizations and other open-source projects that contribute continuously and significantly to Polaris are welcome, but shall not use the Polaris chat as a vehicle for their own marketing without explicit approval of the project (P)PMC.\nCode of Conduct The Apache Software Foundation’s Code of Conduct applies to the Polaris project public chat.\nGovernance The Polaris project’s chat tool is not provided by the ASF - the Polaris project (P)PMC members govern and monitor the chat service. Everybody is welcome to join the Polaris project chat. Invites are not needed. Polaris project (P)PMC members have “administrator” privileges on the Slack chat. Polaris project committers are granted “moderator” or “administrator” privileges for moderation purposes. This bylaws document shall be published on the project’s web site and linked from relevant public channels. Only (P)PMC members are allowed to create new channels. The number of channels shall be limited to #general (user discussions), #dev (development discussions), #announcements (release announcements), and #noise (GitHub feed). Only (P)PMC members are allowed to notify a large number of users (aka @here). 3rd Parties Definition: “3rd party” means any (other) open-source project or any commercial vendor or any other organization.\n3rd Party Channels 3rd parties who contribute to or use the Polaris project may ask the (P)PMC to get a dedicated public channel, for example #vendor- or #project-. Promotion of 3rd party content is only permitted in these 3rd party channels, and only if the content primarily covers Polaris. Commercial advertisements in any form are prohibited. Interaction with users of 3rd party OSS projects or commercial products should be directed to these 3rd party channels. Content moderation Any content which violates the Apache “Code of Conduct” will be removed. Any commercial advertisements will be removed. All other content will not be moderated. ","categories":"","description":"","excerpt":" Apache Polaris - Project Chat Bylaws Apache Polaris is currently …","ref":"/community/chat-bylaws/","tags":"","title":""},{"body":"This page documents various entities that can be managed in Apache Polaris (Incubating).\nCatalog A catalog is a top-level entity in Polaris that may contain other entities like namespaces and tables. These map directly to Apache Iceberg catalogs.\nFor information on managing catalogs with the REST API or for more information on what data can be associated with a catalog, see the CreateCatalogRequest OpenAPI.\nStorage Type All catalogs in Polaris are associated with a storage type. Valid Storage Types are S3, Azure, and GCS. The FILE type is also additionally available for testing. Each of these types relates to a different storage provider where data within the catalog may reside. Depending on the storage type, various other configurations may be set for a catalog including credentials to be used when accessing data inside the catalog.\nFor details on how to use Storage Types in the REST API, see the StorageConfigInfo OpenAPI.\nFor usage examples of storage types, see docs.\nNamespace A namespace is a logical entity that resides within a catalog and can contain other entities such as tables or views. Some other systems may refer to namespaces as schemas or databases.\nIn Polaris, namespaces can be nested. For example, a.b.c.d.e.f.g is a valid namespace. b is said to reside within a, and so on.\nFor information on managing namespaces with the REST API or for more information on what data can be associated with a namespace, see the CreateNamespaceRequest OpenAPI.\nTable Polaris tables are entities that map to Apache Iceberg tables, Delta tables, or Hudi tables.\nFor information on managing tables with the REST API or for more information on what data can be associated with a table, see the CreateTableRequest OpenAPI.\nView Polaris views are entities that map to Apache Iceberg views.\nFor information on managing views with the REST API or for more information on what data can be associated with a view, see the CreateViewRequest OpenAPI.\nPrincipal Polaris principals are unique identities that can be used to represent users or services. Each principal may have one or more principal roles assigned to it for the purpose of accessing catalogs and the entities within them.\nFor information on managing principals with the REST API or for more information on what data can be associated with a principal, see the CreatePrincipalRequest OpenAPI.\nPrincipal Role Polaris principal roles are labels that may be granted to principals. Each principal may have one or more principal roles, and the same principal role may be granted to multiple principals. Principal roles may be assigned based on the persona or responsibilities of a given principal, or on how that principal will need to access different entities within Polaris.\nFor information on managing principal roles with the REST API or for more information on what data can be associated with a principal role, see the CreatePrincipalRoleRequest OpenAPI\nCatalog Role Polaris catalog roles are labels that may be granted to catalogs. Each catalog may have one or more catalog roles, and the same catalog role may be granted to multiple catalogs. Catalog roles may be assigned based on the nature of data that will reside in a catalog, or by the groups of users and services that might need to access that data.\nEach catalog role may have multiple privileges granted to it, and each catalog role can be granted to one or more principal roles. This is the mechanism by which principals are granted access to entities inside a catalog such as namespaces and tables.\nPolicy Polaris policy is a set of rules governing actions on specified resources under predefined conditions. Polaris support policy for Iceberg table compaction, snapshot expiry, row-level access control, and custom policy definitions.\nPolicy can be applied at catalog level, namespace level, or table level. Policy inheritance can be achieved by attaching one to a higher-level scope, such as namespace or catalog. As a result, tables registered under those entities do not need to be declared individually for the same policy. If a table or a namespace requires a different policy, user can assign a different policy, hence overriding policy of the same type declared at the higher level entities.\nPrivilege Polaris privileges are granted to catalog roles in order to grant principals with a given principal role some degree of access to catalogs with a given catalog role. When a privilege is granted to a catalog role, any principal roles granted that catalog role receive the privilege. In turn, any principals who are granted that principal role receive it.\nA privilege can be scoped to any entity inside a catalog, including the catalog itself.\nFor a list of supported privileges for each privilege class, see the OpenAPI (TablePrivilege, ViewPrivilege, NamespacePrivilege, CatalogPrivilege).\n","categories":"","description":"","excerpt":"This page documents various entities that can be managed in Apache …","ref":"/in-dev/unreleased/entities/","tags":"","title":"Entities"},{"body":"This page documents various entities that can be managed in Apache Polaris (Incubating).\nCatalog A catalog is a top-level entity in Polaris that may contain other entities like namespaces and tables. These map directly to Apache Iceberg catalogs.\nFor information on managing catalogs with the REST API or for more information on what data can be associated with a catalog, see the API docs.\nStorage Type All catalogs in Polaris are associated with a storage type. Valid Storage Types are S3, Azure, and GCS. The FILE type is also additionally available for testing. Each of these types relates to a different storage provider where data within the catalog may reside. Depending on the storage type, various other configurations may be set for a catalog including credentials to be used when accessing data inside the catalog.\nFor details on how to use Storage Types in the REST API, see the API docs.\nNamespace A namespace is a logical entity that resides within a catalog and can contain other entities such as tables or views. Some other systems may refer to namespaces as schemas or databases.\nIn Polaris, namespaces can be nested. For example, a.b.c.d.e.f.g is a valid namespace. b is said to reside within a, and so on.\nFor information on managing namespaces with the REST API or for more information on what data can be associated with a namespace, see the API docs.\nTable Polaris tables are entities that map to Apache Iceberg tables.\nFor information on managing tables with the REST API or for more information on what data can be associated with a table, see the API docs.\nView Polaris views are entities that map to Apache Iceberg views.\nFor information on managing views with the REST API or for more information on what data can be associated with a view, see the API docs.\nPrincipal Polaris principals are unique identities that can be used to represent users or services. Each principal may have one or more principal roles assigned to it for the purpose of accessing catalogs and the entities within them.\nFor information on managing principals with the REST API or for more information on what data can be associated with a principal, see the API docs.\nPrincipal Role Polaris principal roles are labels that may be granted to principals. Each principal may have one or more principal roles, and the same principal role may be granted to multiple principals. Principal roles may be assigned based on the persona or responsibilities of a given principal, or on how that principal will need to access different entities within Polaris.\nFor information on managing principal roles with the REST API or for more information on what data can be associated with a principal role, see the API docs.\nCatalog Role Polaris catalog roles are labels that may be granted to catalogs. Each catalog may have one or more catalog roles, and the same catalog role may be granted to multiple catalogs. Catalog roles may be assigned based on the nature of data that will reside in a catalog, or by the groups of users and services that might need to access that data.\nEach catalog role may have multiple privileges granted to it, and each catalog role can be granted to one or more principal roles. This is the mechanism by which principals are granted access to entities inside a catalog such as namespaces and tables.\nPrivilege Polaris privileges are granted to catalog roles in order to grant principals with a given principal role some degree of access to catalogs with a given catalog role. When a privilege is granted to a catalog role, any principal roles granted that catalog role receive the privilege. In turn, any principals who are granted that principal role receive it.\nA privilege can be scoped to any entity inside a catalog, including the catalog itself.\nFor a list of supported privileges for each privilege class, see the API docs:\nTable Privileges View Privileges Namespace Privileges Catalog Privileges ","categories":"","description":"","excerpt":"This page documents various entities that can be managed in Apache …","ref":"/releases/0.9.0/entities/","tags":"","title":"Entities"},{"body":"This page documents various entities that can be managed in Apache Polaris (Incubating).\nCatalog A catalog is a top-level entity in Polaris that may contain other entities like namespaces and tables. These map directly to Apache Iceberg catalogs.\nFor information on managing catalogs with the REST API or for more information on what data can be associated with a catalog, see the API docs.\nStorage Type All catalogs in Polaris are associated with a storage type. Valid Storage Types are S3, Azure, and GCS. The FILE type is also additionally available for testing. Each of these types relates to a different storage provider where data within the catalog may reside. Depending on the storage type, various other configurations may be set for a catalog including credentials to be used when accessing data inside the catalog.\nFor details on how to use Storage Types in the REST API, see the API docs.\nFor usage examples of storage types, see docs.\nNamespace A namespace is a logical entity that resides within a catalog and can contain other entities such as tables or views. Some other systems may refer to namespaces as schemas or databases.\nIn Polaris, namespaces can be nested. For example, a.b.c.d.e.f.g is a valid namespace. b is said to reside within a, and so on.\nFor information on managing namespaces with the REST API or for more information on what data can be associated with a namespace, see the API docs.\nTable Polaris tables are entities that map to Apache Iceberg tables, Delta tables, or Hudi tables.\nFor information on managing tables with the REST API or for more information on what data can be associated with a table, see the API docs.\nView Polaris views are entities that map to Apache Iceberg views.\nFor information on managing views with the REST API or for more information on what data can be associated with a view, see the API docs.\nPrincipal Polaris principals are unique identities that can be used to represent users or services. Each principal may have one or more principal roles assigned to it for the purpose of accessing catalogs and the entities within them.\nFor information on managing principals with the REST API or for more information on what data can be associated with a principal, see the API docs.\nPrincipal Role Polaris principal roles are labels that may be granted to principals. Each principal may have one or more principal roles, and the same principal role may be granted to multiple principals. Principal roles may be assigned based on the persona or responsibilities of a given principal, or on how that principal will need to access different entities within Polaris.\nFor information on managing principal roles with the REST API or for more information on what data can be associated with a principal role, see the API docs.\nCatalog Role Polaris catalog roles are labels that may be granted to catalogs. Each catalog may have one or more catalog roles, and the same catalog role may be granted to multiple catalogs. Catalog roles may be assigned based on the nature of data that will reside in a catalog, or by the groups of users and services that might need to access that data.\nEach catalog role may have multiple privileges granted to it, and each catalog role can be granted to one or more principal roles. This is the mechanism by which principals are granted access to entities inside a catalog such as namespaces and tables.\nPolicy Polaris policy is a set of rules governing actions on specified resources under predefined conditions. Polaris support policy for Iceberg table compaction, snapshot expiry, row-level access control, and custom policy definitions.\nPolicy can be applied at catalog level, namespace level, or table level. Policy inheritance can be achieved by attaching one to a higher-level scope, such as namespace or catalog. As a result, tables registered under those entities do not need to be declared individually for the same policy. If a table or a namespace requires a different policy, user can assign a different policy, hence overriding policy of the same type declared at the higher level entities.\nPrivilege Polaris privileges are granted to catalog roles in order to grant principals with a given principal role some degree of access to catalogs with a given catalog role. When a privilege is granted to a catalog role, any principal roles granted that catalog role receive the privilege. In turn, any principals who are granted that principal role receive it.\nA privilege can be scoped to any entity inside a catalog, including the catalog itself.\nFor a list of supported privileges for each privilege class, see the API docs:\nTable Privileges View Privileges Namespace Privileges Catalog Privileges ","categories":"","description":"","excerpt":"This page documents various entities that can be managed in Apache …","ref":"/releases/1.0.0/entities/","tags":"","title":"Entities"},{"body":"This page documents various entities that can be managed in Apache Polaris (Incubating).\nCatalog A catalog is a top-level entity in Polaris that may contain other entities like namespaces and tables. These map directly to Apache Iceberg catalogs.\nFor information on managing catalogs with the REST API or for more information on what data can be associated with a catalog, see the API docs.\nStorage Type All catalogs in Polaris are associated with a storage type. Valid Storage Types are S3, Azure, and GCS. The FILE type is also additionally available for testing. Each of these types relates to a different storage provider where data within the catalog may reside. Depending on the storage type, various other configurations may be set for a catalog including credentials to be used when accessing data inside the catalog.\nFor details on how to use Storage Types in the REST API, see the API docs.\nFor usage examples of storage types, see docs.\nNamespace A namespace is a logical entity that resides within a catalog and can contain other entities such as tables or views. Some other systems may refer to namespaces as schemas or databases.\nIn Polaris, namespaces can be nested. For example, a.b.c.d.e.f.g is a valid namespace. b is said to reside within a, and so on.\nFor information on managing namespaces with the REST API or for more information on what data can be associated with a namespace, see the API docs.\nTable Polaris tables are entities that map to Apache Iceberg tables, Delta tables, or Hudi tables.\nFor information on managing tables with the REST API or for more information on what data can be associated with a table, see the API docs.\nView Polaris views are entities that map to Apache Iceberg views.\nFor information on managing views with the REST API or for more information on what data can be associated with a view, see the API docs.\nPrincipal Polaris principals are unique identities that can be used to represent users or services. Each principal may have one or more principal roles assigned to it for the purpose of accessing catalogs and the entities within them.\nFor information on managing principals with the REST API or for more information on what data can be associated with a principal, see the API docs.\nPrincipal Role Polaris principal roles are labels that may be granted to principals. Each principal may have one or more principal roles, and the same principal role may be granted to multiple principals. Principal roles may be assigned based on the persona or responsibilities of a given principal, or on how that principal will need to access different entities within Polaris.\nFor information on managing principal roles with the REST API or for more information on what data can be associated with a principal role, see the API docs.\nCatalog Role Polaris catalog roles are labels that may be granted to catalogs. Each catalog may have one or more catalog roles, and the same catalog role may be granted to multiple catalogs. Catalog roles may be assigned based on the nature of data that will reside in a catalog, or by the groups of users and services that might need to access that data.\nEach catalog role may have multiple privileges granted to it, and each catalog role can be granted to one or more principal roles. This is the mechanism by which principals are granted access to entities inside a catalog such as namespaces and tables.\nPolicy Polaris policy is a set of rules governing actions on specified resources under predefined conditions. Polaris support policy for Iceberg table compaction, snapshot expiry, row-level access control, and custom policy definitions.\nPolicy can be applied at catalog level, namespace level, or table level. Policy inheritance can be achieved by attaching one to a higher-level scope, such as namespace or catalog. As a result, tables registered under those entities do not need to be declared individually for the same policy. If a table or a namespace requires a different policy, user can assign a different policy, hence overriding policy of the same type declared at the higher level entities.\nPrivilege Polaris privileges are granted to catalog roles in order to grant principals with a given principal role some degree of access to catalogs with a given catalog role. When a privilege is granted to a catalog role, any principal roles granted that catalog role receive the privilege. In turn, any principals who are granted that principal role receive it.\nA privilege can be scoped to any entity inside a catalog, including the catalog itself.\nFor a list of supported privileges for each privilege class, see the API docs:\nTable Privileges View Privileges Namespace Privileges Catalog Privileges ","categories":"","description":"","excerpt":"This page documents various entities that can be managed in Apache …","ref":"/releases/1.0.1/entities/","tags":"","title":"Entities"},{"body":"Setup Ensure your CLIENT_ID \u0026 CLIENT_SECRET variables are already defined, as they were required for starting the Polaris server earlier.\nexport CLIENT_ID=YOUR_CLIENT_ID export CLIENT_SECRET=YOUR_CLIENT_SECRET Defining a Catalog In Polaris, the catalog is the top-level entity that objects like tables and views are organized under. With a Polaris service running, you can create a catalog like so:\ncd ~/polaris ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalogs \\ create \\ --storage-type s3 \\ --default-base-location ${DEFAULT_BASE_LOCATION} \\ --role-arn ${ROLE_ARN} \\ quickstart_catalog This will create a new catalog called quickstart_catalog. If you are using one of the Getting Started locally-built Docker images, we have already created a catalog named quickstart_catalog for you.\nThe DEFAULT_BASE_LOCATION you provide will be the default location that objects in this catalog should be stored in, and the ROLE_ARN you provide should be a Role ARN with access to read and write data in that location. These credentials will be provided to engines reading data from the catalog once they have authenticated with Polaris using credentials that have access to those resources.\nIf you’re using a storage type other than S3, such as Azure, you’ll provide a different type of credential than a Role ARN. For more details on supported storage types, see the docs.\nAdditionally, if Polaris is running somewhere other than localhost:8181, you can specify the correct hostname and port by providing --host and --port flags. For the full set of options supported by the CLI, please refer to the docs.\nCreating a Principal and Assigning it Privileges With a catalog created, we can create a principal that has access to manage that catalog. For details on how to configure the Polaris CLI, see the section above or refer to the docs.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principals \\ create \\ quickstart_user ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ create \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ create \\ --catalog quickstart_catalog \\ quickstart_catalog_role Be sure to provide the necessary credentials, hostname, and port as before.\nWhen the principals create command completes successfully, it will return the credentials for this new principal. Export them for future use. For example:\n./polaris ... principals create example {\"clientId\": \"XXXX\", \"clientSecret\": \"YYYY\"} export USER_CLIENT_ID=XXXX export USER_CLIENT_SECRET=YYYY Now, we grant the principal the principal role we created, and grant the catalog role the principal role we created. For more information on these entities, please refer to the linked documentation.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ grant \\ --principal quickstart_user \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ grant \\ --catalog quickstart_catalog \\ --principal-role quickstart_user_role \\ quickstart_catalog_role Now, we’ve linked our principal to the catalog via roles like so:\nIn order to give this principal the ability to interact with the catalog, we must assign some privileges. For the time being, we will give this principal the ability to fully manage content in our new catalog. We can do this with the CLI like so:\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ grant \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT This grants the catalog privileges CATALOG_MANAGE_CONTENT to our catalog role, linking everything together like so:\nCATALOG_MANAGE_CONTENT has create/list/read/write privileges on all entities within the catalog. The same privilege could be granted to a namespace, in which case the principal could create/list/read/write any entity under that namespace.\nUsing Iceberg \u0026 Polaris At this point, we’ve created a principal and granted it the ability to manage a catalog. We can now use an external engine to assume that principal, access our catalog, and store data in that catalog using Apache Iceberg. Polaris is compatible with any Apache Iceberg client that supports the REST API. Depending on the client you plan to use, refer to the respective examples below.\nConnecting with Spark Using a Local Build of Spark To use a Polaris-managed catalog in Apache Spark, we can configure Spark to use the Iceberg catalog REST API.\nThis guide uses Apache Spark 3.5, but be sure to find the appropriate iceberg-spark package for your Spark version. From a local Spark clone on the branch-3.5 branch we can run the following:\nNote: the credentials provided here are those for our principal, not the root credentials.\nbin/spark-sql \\ --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.quickstart_catalog.warehouse=quickstart_catalog \\ --conf spark.sql.catalog.quickstart_catalog.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.quickstart_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.quickstart_catalog.catalog-impl=org.apache.iceberg.rest.RESTCatalog \\ --conf spark.sql.catalog.quickstart_catalog.uri=http://localhost:8181/api/catalog \\ --conf spark.sql.catalog.quickstart_catalog.credential=${USER_CLIENT_ID}:${USER_CLIENT_SECRET} \\ --conf spark.sql.catalog.quickstart_catalog.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.quickstart_catalog.token-refresh-enabled=true \\ --conf spark.sql.catalog.quickstart_catalog.client.region=us-west-2 Similar to the CLI commands above, this configures Spark to use the Polaris running at localhost:8181. If your Polaris server is running elsewhere, but sure to update the configuration appropriately.\nFinally, note that we include the iceberg-aws-bundle package here. If your table is using a different filesystem, be sure to include the appropriate dependency.\nUsing Spark SQL from a Docker container Refresh the Docker container with the user’s credentials:\ndocker compose -p polaris -f getting-started/eclipselink/docker-compose.yml stop spark-sql docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml rm -f spark-sql docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml up -d --no-deps spark-sql Attach to the running spark-sql container:\ndocker attach $(docker ps -q --filter name=spark-sql) Sample Commands Once the Spark session starts, we can create a namespace and table within the catalog:\nUSE quickstart_catalog; CREATE NAMESPACE IF NOT EXISTS quickstart_namespace; CREATE NAMESPACE IF NOT EXISTS quickstart_namespace.schema; USE NAMESPACE quickstart_namespace.schema; CREATE TABLE IF NOT EXISTS quickstart_table (id BIGINT, data STRING) USING ICEBERG; We can now use this table like any other:\nINSERT INTO quickstart_table VALUES (1, 'some data'); SELECT * FROM quickstart_table; . . . +---+---------+ |id |data | +---+---------+ |1 |some data| +---+---------+ If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Spark will lose access to the table:\nINSERT INTO quickstart_table VALUES (1, 'some data'); org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated grants via '[quickstart_catalog_role, quickstart_user_role]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION Connecting with Trino Refresh the Docker container with the user’s credentials:\ndocker compose -p polaris -f getting-started/eclipselink/docker-compose.yml stop trino docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml rm -f trino docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml up -d --no-deps trino Attach to the running Trino container:\ndocker exec -it $(docker ps -q --filter name=trino) trino You may not see Trino’s prompt immediately, type ENTER to see it. A few commands that you can try:\nSHOW CATALOGS; SHOW SCHEMAS FROM iceberg; CREATE SCHEMA iceberg.quickstart_schema; CREATE TABLE iceberg.quickstart_schema.quickstart_table AS SELECT 1 x; SELECT * FROM iceberg.quickstart_schema.quickstart_table; If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Trino will lose access to the table:\nSELECT * FROM iceberg.quickstart_schema.quickstart_table; org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated grants via '[quickstart_catalog_role, quickstart_user_role]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION Connecting with PyIceberg Using Credentials from pyiceberg.catalog import load_catalog catalog = load_catalog( type='rest', uri='http://localhost:8181/api/catalog', warehouse='quickstart_catalog', scope=\"PRINCIPAL_ROLE:ALL\", credential=f\"{CLIENT_ID}:{CLIENT_SECRET}\", ) If the load_catalog function is used with credentials, then PyIceberg will automatically request an authorization token from the v1/oauth/tokens endpoint, and will later use this token to prove its identity to the Polaris Catalog.\nUsing a Token from pyiceberg.catalog import load_catalog import requests # Step 1: Get OAuth token response = requests.post( \"http://localhost:8181/api/catalog/v1/oauth/tokens\", auth =(CLIENT_ID, CLIENT_SECRET), data = { \"grant_type\": \"client_credentials\", \"scope\": \"PRINCIPAL_ROLE:ALL\" }) token = response.json()[\"access_token\"] # Step 2: Load the catalog using the token catalog = load_catalog( type='rest', uri='http://localhost:8181/api/catalog', warehouse='quickstart_catalog', token=token, ) It is possible to use load_catalog function by providing an authorization token directly. This method is useful when using an external identity provider (e.g. Google Identity).\nConnecting Using REST APIs To access Polaris from the host machine, first request an access token:\nexport POLARIS_TOKEN=$(curl -s http://polaris:8181/api/catalog/v1/oauth/tokens \\ --resolve polaris:8181:127.0.0.1 \\ --user ${CLIENT_ID}:${CLIENT_SECRET} \\ -d 'grant_type=client_credentials' \\ -d 'scope=PRINCIPAL_ROLE:ALL' | jq -r .access_token) Then, use the access token in the Authorization header when accessing Polaris:\ncurl -v http://127.0.0.1:8181/api/management/v1/principal-roles -H \"Authorization: Bearer $POLARIS_TOKEN\" curl -v http://127.0.0.1:8181/api/management/v1/catalogs/quickstart_catalog -H \"Authorization: Bearer $POLARIS_TOKEN\" Next Steps Visit Configuring Polaris for Production. A Getting Started experience for using Spark with Jupyter Notebooks is documented here. To shut down a locally-deployed Polaris server and clean up all related Docker containers, run the command listed below. Cloud Deployments have their respective termination commands on their Deployment page, while Polaris running on Gradle will terminate when the Gradle process terminates. docker compose -p polaris \\ -f getting-started/assets/postgres/docker-compose-postgres.yml \\ -f getting-started/jdbc/docker-compose-bootstrap-db.yml \\ -f getting-started/jdbc/docker-compose.yml \\ down ","categories":"","description":"","excerpt":"Setup Ensure your CLIENT_ID \u0026 CLIENT_SECRET variables are already …","ref":"/in-dev/unreleased/getting-started/using-polaris/","tags":"","title":"Using Polaris"},{"body":"Setup Ensure your CLIENT_ID \u0026 CLIENT_SECRET variables are already defined, as they were required for starting the Polaris server earlier.\nexport CLIENT_ID=YOUR_CLIENT_ID export CLIENT_SECRET=YOUR_CLIENT_SECRET Defining a Catalog In Polaris, the catalog is the top-level entity that objects like tables and views are organized under. With a Polaris service running, you can create a catalog like so:\ncd ~/polaris ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalogs \\ create \\ --storage-type s3 \\ --default-base-location ${DEFAULT_BASE_LOCATION} \\ --role-arn ${ROLE_ARN} \\ quickstart_catalog This will create a new catalog called quickstart_catalog. If you are using one of the Getting Started locally-built Docker images, we have already created a catalog named quickstart_catalog for you.\nThe DEFAULT_BASE_LOCATION you provide will be the default location that objects in this catalog should be stored in, and the ROLE_ARN you provide should be a Role ARN with access to read and write data in that location. These credentials will be provided to engines reading data from the catalog once they have authenticated with Polaris using credentials that have access to those resources.\nIf you’re using a storage type other than S3, such as Azure, you’ll provide a different type of credential than a Role ARN. For more details on supported storage types, see the docs.\nAdditionally, if Polaris is running somewhere other than localhost:8181, you can specify the correct hostname and port by providing --host and --port flags. For the full set of options supported by the CLI, please refer to the docs.\nCreating a Principal and Assigning it Privileges With a catalog created, we can create a principal that has access to manage that catalog. For details on how to configure the Polaris CLI, see the section above or refer to the docs.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principals \\ create \\ quickstart_user ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ create \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ create \\ --catalog quickstart_catalog \\ quickstart_catalog_role Be sure to provide the necessary credentials, hostname, and port as before.\nWhen the principals create command completes successfully, it will return the credentials for this new principal. Export them for future use. For example:\n./polaris ... principals create example {\"clientId\": \"XXXX\", \"clientSecret\": \"YYYY\"} export USER_CLIENT_ID=XXXX export USER_CLIENT_SECRET=YYYY Now, we grant the principal the principal role we created, and grant the catalog role the principal role we created. For more information on these entities, please refer to the linked documentation.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ grant \\ --principal quickstart_user \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ grant \\ --catalog quickstart_catalog \\ --principal-role quickstart_user_role \\ quickstart_catalog_role Now, we’ve linked our principal to the catalog via roles like so:\nIn order to give this principal the ability to interact with the catalog, we must assign some privileges. For the time being, we will give this principal the ability to fully manage content in our new catalog. We can do this with the CLI like so:\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ grant \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT This grants the catalog privileges CATALOG_MANAGE_CONTENT to our catalog role, linking everything together like so:\nCATALOG_MANAGE_CONTENT has create/list/read/write privileges on all entities within the catalog. The same privilege could be granted to a namespace, in which case the principal could create/list/read/write any entity under that namespace.\nUsing Iceberg \u0026 Polaris At this point, we’ve created a principal and granted it the ability to manage a catalog. We can now use an external engine to assume that principal, access our catalog, and store data in that catalog using Apache Iceberg. Polaris is compatible with any Apache Iceberg client that supports the REST API. Depending on the client you plan to use, refer to the respective examples below.\nConnecting with Spark Using a Local Build of Spark To use a Polaris-managed catalog in Apache Spark, we can configure Spark to use the Iceberg catalog REST API.\nThis guide uses Apache Spark 3.5, but be sure to find the appropriate iceberg-spark package for your Spark version. From a local Spark clone on the branch-3.5 branch we can run the following:\nNote: the credentials provided here are those for our principal, not the root credentials.\nbin/spark-sql \\ --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.apache.iceberg:iceberg-aws-bundle:1.9.0 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.quickstart_catalog.warehouse=quickstart_catalog \\ --conf spark.sql.catalog.quickstart_catalog.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.quickstart_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.quickstart_catalog.catalog-impl=org.apache.iceberg.rest.RESTCatalog \\ --conf spark.sql.catalog.quickstart_catalog.uri=http://localhost:8181/api/catalog \\ --conf spark.sql.catalog.quickstart_catalog.credential='${USER_CLIENT_ID}:${USER_CLIENT_SECRET}' \\ --conf spark.sql.catalog.quickstart_catalog.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.quickstart_catalog.token-refresh-enabled=true \\ --conf spark.sql.catalog.quickstart_catalog.client.region=us-west-2 Similar to the CLI commands above, this configures Spark to use the Polaris running at localhost:8181. If your Polaris server is running elsewhere, but sure to update the configuration appropriately.\nFinally, note that we include the iceberg-aws-bundle package here. If your table is using a different filesystem, be sure to include the appropriate dependency.\nUsing Spark SQL from a Docker container Refresh the Docker container with the user’s credentials:\ndocker compose -p polaris -f getting-started/eclipselink/docker-compose.yml stop spark-sql docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml rm -f spark-sql docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml up -d --no-deps spark-sql Attach to the running spark-sql container:\ndocker attach $(docker ps -q --filter name=spark-sql) Sample Commands Once the Spark session starts, we can create a namespace and table within the catalog:\nUSE quickstart_catalog; CREATE NAMESPACE IF NOT EXISTS quickstart_namespace; CREATE NAMESPACE IF NOT EXISTS quickstart_namespace.schema; USE NAMESPACE quickstart_namespace.schema; CREATE TABLE IF NOT EXISTS quickstart_table (id BIGINT, data STRING) USING ICEBERG; We can now use this table like any other:\nINSERT INTO quickstart_table VALUES (1, 'some data'); SELECT * FROM quickstart_table; . . . +---+---------+ |id |data | +---+---------+ |1 |some data| +---+---------+ If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Spark will lose access to the table:\nINSERT INTO quickstart_table VALUES (1, 'some data'); org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated grants via '[quickstart_catalog_role, quickstart_user_role]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION Connecting with Trino Refresh the Docker container with the user’s credentials:\ndocker compose -p polaris -f getting-started/eclipselink/docker-compose.yml stop trino docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml rm -f trino docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml up -d --no-deps trino Attach to the running Trino container:\ndocker exec -it $(docker ps -q --filter name=trino) trino You may not see Trino’s prompt immediately, type ENTER to see it. A few commands that you can try:\nSHOW CATALOGS; SHOW SCHEMAS FROM iceberg; CREATE SCHEMA iceberg.quickstart_schema; CREATE TABLE iceberg.quickstart_schema.quickstart_table AS SELECT 1 x; SELECT * FROM iceberg.quickstart_schema.quickstart_table; If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Trino will lose access to the table:\nSELECT * FROM iceberg.quickstart_schema.quickstart_table; org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated grants via '[quickstart_catalog_role, quickstart_user_role]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION Connecting Using REST APIs To access Polaris from the host machine, first request an access token:\nexport POLARIS_TOKEN=$(curl -s http://polaris:8181/api/catalog/v1/oauth/tokens \\ --resolve polaris:8181:127.0.0.1 \\ --user ${CLIENT_ID}:${CLIENT_SECRET} \\ -d 'grant_type=client_credentials' \\ -d 'scope=PRINCIPAL_ROLE:ALL' | jq -r .access_token) Then, use the access token in the Authorization header when accessing Polaris:\ncurl -v http://127.0.0.1:8181/api/management/v1/principal-roles -H \"Authorization: Bearer $POLARIS_TOKEN\" curl -v http://127.0.0.1:8181/api/management/v1/catalogs/quickstart_catalog -H \"Authorization: Bearer $POLARIS_TOKEN\" Next Steps Visit Configuring Polaris for Production. A Getting Started experience for using Spark with Jupyter Notebooks is documented here. To shut down a locally-deployed Polaris server and clean up all related Docker containers, run the command listed below. Cloud Deployments have their respective termination commands on their Deployment page, while Polaris running on Gradle will terminate when the Gradle process terminates. docker compose -p polaris -f getting-started/assets/postgres/docker-compose-postgres.yml -f getting-started/jdbc/docker-compose-bootstrap-db.yml -f getting-started/jdbc/docker-compose.yml down ","categories":"","description":"","excerpt":"Setup Ensure your CLIENT_ID \u0026 CLIENT_SECRET variables are already …","ref":"/releases/1.0.0/getting-started/using-polaris/","tags":"","title":"Using Polaris"},{"body":"Setup Ensure your CLIENT_ID \u0026 CLIENT_SECRET variables are already defined, as they were required for starting the Polaris server earlier.\nexport CLIENT_ID=YOUR_CLIENT_ID export CLIENT_SECRET=YOUR_CLIENT_SECRET Defining a Catalog In Polaris, the catalog is the top-level entity that objects like tables and views are organized under. With a Polaris service running, you can create a catalog like so:\ncd ~/polaris ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalogs \\ create \\ --storage-type s3 \\ --default-base-location ${DEFAULT_BASE_LOCATION} \\ --role-arn ${ROLE_ARN} \\ quickstart_catalog This will create a new catalog called quickstart_catalog. If you are using one of the Getting Started locally-built Docker images, we have already created a catalog named quickstart_catalog for you.\nThe DEFAULT_BASE_LOCATION you provide will be the default location that objects in this catalog should be stored in, and the ROLE_ARN you provide should be a Role ARN with access to read and write data in that location. These credentials will be provided to engines reading data from the catalog once they have authenticated with Polaris using credentials that have access to those resources.\nIf you’re using a storage type other than S3, such as Azure, you’ll provide a different type of credential than a Role ARN. For more details on supported storage types, see the docs.\nAdditionally, if Polaris is running somewhere other than localhost:8181, you can specify the correct hostname and port by providing --host and --port flags. For the full set of options supported by the CLI, please refer to the docs.\nCreating a Principal and Assigning it Privileges With a catalog created, we can create a principal that has access to manage that catalog. For details on how to configure the Polaris CLI, see the section above or refer to the docs.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principals \\ create \\ quickstart_user ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ create \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ create \\ --catalog quickstart_catalog \\ quickstart_catalog_role Be sure to provide the necessary credentials, hostname, and port as before.\nWhen the principals create command completes successfully, it will return the credentials for this new principal. Export them for future use. For example:\n./polaris ... principals create example {\"clientId\": \"XXXX\", \"clientSecret\": \"YYYY\"} export USER_CLIENT_ID=XXXX export USER_CLIENT_SECRET=YYYY Now, we grant the principal the principal role we created, and grant the catalog role the principal role we created. For more information on these entities, please refer to the linked documentation.\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ principal-roles \\ grant \\ --principal quickstart_user \\ quickstart_user_role ./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ catalog-roles \\ grant \\ --catalog quickstart_catalog \\ --principal-role quickstart_user_role \\ quickstart_catalog_role Now, we’ve linked our principal to the catalog via roles like so:\nIn order to give this principal the ability to interact with the catalog, we must assign some privileges. For the time being, we will give this principal the ability to fully manage content in our new catalog. We can do this with the CLI like so:\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ grant \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT This grants the catalog privileges CATALOG_MANAGE_CONTENT to our catalog role, linking everything together like so:\nCATALOG_MANAGE_CONTENT has create/list/read/write privileges on all entities within the catalog. The same privilege could be granted to a namespace, in which case the principal could create/list/read/write any entity under that namespace.\nUsing Iceberg \u0026 Polaris At this point, we’ve created a principal and granted it the ability to manage a catalog. We can now use an external engine to assume that principal, access our catalog, and store data in that catalog using Apache Iceberg. Polaris is compatible with any Apache Iceberg client that supports the REST API. Depending on the client you plan to use, refer to the respective examples below.\nConnecting with Spark Using a Local Build of Spark To use a Polaris-managed catalog in Apache Spark, we can configure Spark to use the Iceberg catalog REST API.\nThis guide uses Apache Spark 3.5, but be sure to find the appropriate iceberg-spark package for your Spark version. From a local Spark clone on the branch-3.5 branch we can run the following:\nNote: the credentials provided here are those for our principal, not the root credentials.\nbin/spark-sql \\ --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.apache.iceberg:iceberg-aws-bundle:1.9.0 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.quickstart_catalog.warehouse=quickstart_catalog \\ --conf spark.sql.catalog.quickstart_catalog.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.quickstart_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.quickstart_catalog.catalog-impl=org.apache.iceberg.rest.RESTCatalog \\ --conf spark.sql.catalog.quickstart_catalog.uri=http://localhost:8181/api/catalog \\ --conf spark.sql.catalog.quickstart_catalog.credential='${USER_CLIENT_ID}:${USER_CLIENT_SECRET}' \\ --conf spark.sql.catalog.quickstart_catalog.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.quickstart_catalog.token-refresh-enabled=true \\ --conf spark.sql.catalog.quickstart_catalog.client.region=us-west-2 Similar to the CLI commands above, this configures Spark to use the Polaris running at localhost:8181. If your Polaris server is running elsewhere, but sure to update the configuration appropriately.\nFinally, note that we include the iceberg-aws-bundle package here. If your table is using a different filesystem, be sure to include the appropriate dependency.\nUsing Spark SQL from a Docker container Refresh the Docker container with the user’s credentials:\ndocker compose -p polaris -f getting-started/eclipselink/docker-compose.yml stop spark-sql docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml rm -f spark-sql docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml up -d --no-deps spark-sql Attach to the running spark-sql container:\ndocker attach $(docker ps -q --filter name=spark-sql) Sample Commands Once the Spark session starts, we can create a namespace and table within the catalog:\nUSE quickstart_catalog; CREATE NAMESPACE IF NOT EXISTS quickstart_namespace; CREATE NAMESPACE IF NOT EXISTS quickstart_namespace.schema; USE NAMESPACE quickstart_namespace.schema; CREATE TABLE IF NOT EXISTS quickstart_table (id BIGINT, data STRING) USING ICEBERG; We can now use this table like any other:\nINSERT INTO quickstart_table VALUES (1, 'some data'); SELECT * FROM quickstart_table; . . . +---+---------+ |id |data | +---+---------+ |1 |some data| +---+---------+ If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Spark will lose access to the table:\nINSERT INTO quickstart_table VALUES (1, 'some data'); org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated grants via '[quickstart_catalog_role, quickstart_user_role]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION Connecting with Trino Refresh the Docker container with the user’s credentials:\ndocker compose -p polaris -f getting-started/eclipselink/docker-compose.yml stop trino docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml rm -f trino docker compose -p polaris -f getting-started/eclipselink/docker-compose.yml up -d --no-deps trino Attach to the running Trino container:\ndocker exec -it $(docker ps -q --filter name=trino) trino You may not see Trino’s prompt immediately, type ENTER to see it. A few commands that you can try:\nSHOW CATALOGS; SHOW SCHEMAS FROM iceberg; CREATE SCHEMA iceberg.quickstart_schema; CREATE TABLE iceberg.quickstart_schema.quickstart_table AS SELECT 1 x; SELECT * FROM iceberg.quickstart_schema.quickstart_table; If at any time access is revoked…\n./polaris \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ privileges \\ catalog \\ revoke \\ --catalog quickstart_catalog \\ --catalog-role quickstart_catalog_role \\ CATALOG_MANAGE_CONTENT Trino will lose access to the table:\nSELECT * FROM iceberg.quickstart_schema.quickstart_table; org.apache.iceberg.exceptions.ForbiddenException: Forbidden: Principal 'quickstart_user' with activated PrincipalRoles '[]' and activated grants via '[quickstart_catalog_role, quickstart_user_role]' is not authorized for op LOAD_TABLE_WITH_READ_DELEGATION Connecting Using REST APIs To access Polaris from the host machine, first request an access token:\nexport POLARIS_TOKEN=$(curl -s http://polaris:8181/api/catalog/v1/oauth/tokens \\ --resolve polaris:8181:127.0.0.1 \\ --user ${CLIENT_ID}:${CLIENT_SECRET} \\ -d 'grant_type=client_credentials' \\ -d 'scope=PRINCIPAL_ROLE:ALL' | jq -r .access_token) Then, use the access token in the Authorization header when accessing Polaris:\ncurl -v http://127.0.0.1:8181/api/management/v1/principal-roles -H \"Authorization: Bearer $POLARIS_TOKEN\" curl -v http://127.0.0.1:8181/api/management/v1/catalogs/quickstart_catalog -H \"Authorization: Bearer $POLARIS_TOKEN\" Next Steps Visit Configuring Polaris for Production. A Getting Started experience for using Spark with Jupyter Notebooks is documented here. To shut down a locally-deployed Polaris server and clean up all related Docker containers, run the command listed below. Cloud Deployments have their respective termination commands on their Deployment page, while Polaris running on Gradle will terminate when the Gradle process terminates. docker compose -p polaris -f getting-started/assets/postgres/docker-compose-postgres.yml -f getting-started/jdbc/docker-compose-bootstrap-db.yml -f getting-started/jdbc/docker-compose.yml down ","categories":"","description":"","excerpt":"Setup Ensure your CLIENT_ID \u0026 CLIENT_SECRET variables are already …","ref":"/releases/1.0.1/getting-started/using-polaris/","tags":"","title":"Using Polaris"},{"body":"The Polaris Policy framework empowers organizations to centrally define, manage, and enforce fine-grained governance, lifecycle, and operational rules across all data resources in the catalog.\nWith the policy API, you can:\nCreate and manage policies Attach policies to specific resources (catalogs, namespaces, tables, or views) Check applicable policies for any given resource What is a Policy? A policy in Apache Polaris is a structured entity that defines rules governing actions on specified resources under predefined conditions. Each policy contains:\nName: A unique identifier within a namespace Type: Determines the semantics and expected format of the policy content Description: Explains the purpose of the policy Content: Contains the actual rules defining the policy behavior Version: An automatically tracked revision number Inheritable: Whether the policy can be inherited by child resources, decided by its type Policy Types Polaris supports several predefined system policy types (prefixed with system.):\nPolicy Type Purpose JSON-Schema Applies To system.data-compaction Defines rules for data file compaction operations data-compaction/2025-02-03.json Iceberg table, namespace, catalog system.metadata-compaction Defines rules for metadata file compaction operations metadata-compaction/2025-02-03.json Iceberg table, namespace, catalog system.orphan-file-removal Defines rules for removing orphaned files orphan-file-removal/2025-02-03.json Iceberg table, namespace, catalog system.snapshot-expiry Defines rules for snapshot expiration snapshot-expiry/2025-02-03.json Iceberg table, namespace, catalog Support for additional predefined system policy types and custom policy type definitions is in progress. For more details, please refer to the roadmap.\nPolicy Inheritance The entity hierarchy in Polaris is structured as follows:\nCatalog | Namespace | +-----------+----------+ | | | Iceberg Iceberg Generic Table View Table Policies can be attached at any level, and inheritance flows from catalog down to namespace, then to tables and views.\nPolicies can be inheritable or non-inheritable:\nInheritable policies: Apply to the target resource and all its applicable child resources Non-inheritable policies: Apply only to the specific target resource The inheritance follows an override mechanism:\nTable-level policies override namespace and catalog policies Namespace-level policies override parent namespace and catalog policies [!IMPORTANT] Because an override completely replaces the same policy type at higher levels, only one instance of a given policy type can be attached to (and therefore affect) a resource.\nWorking with Policies Creating a Policy To create a policy, you need to provide a name, type, and optionally a description and content:\nPOST /polaris/v1/{prefix}/namespaces/{namespace}/policies { \"name\": \"compaction-policy\", \"type\": \"system.data-compaction\", \"description\": \"Policy for optimizing table storage\", \"content\": \"{\\\"version\\\": \\\"2025-02-03\\\", \\\"enable\\\": true, \\\"config\\\": {\\\"target_file_size_bytes\\\": 134217728}}\" } The policy content is validated against a schema specific to its type. Here are a few policy content examples:\nData Compaction Policy { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"target_file_size_bytes\": 134217728, \"compaction_strategy\": \"bin-pack\", \"max-concurrent-file-group-rewrites\": 5 } } Orphan File Removal Policy { \"version\": \"2025-02-03\", \"enable\": true, \"max_orphan_file_age_in_days\": 30, \"locations\": [\"s3://my-bucket/my-table-location\"], \"config\": { \"prefix_mismatch_mode\": \"ignore\" } } Attaching Policies to Resources Policies can be attached to different resource levels:\nCatalog level: Applies to the entire catalog Namespace level: Applies to a specific namespace Table-like level: Applies to individual tables or views Example of attaching a policy to a table:\nPUT /polaris/v1/{prefix}/namespaces/{namespace}/policies/{policy-name}/mappings { \"target\": { \"type\": \"table-like\", \"path\": [\"NS1\", \"NS2\", \"test_table_1\"] } } For inheritable policies, only one policy of a given type can be attached to a resource. For non-inheritable policies, multiple policies of the same type can be attached.\nRetrieving Applicable Policies A user can view applicable policies on a resource (e.g., table, namespace, or catalog) as long as they have read permission on that resource.\nHere is an example to find all policies that apply to a specific resource (including inherited policies):\nGET /polaris/v1/catalog/applicable-policies?namespace=finance%1Fquarterly\u0026target-name=transactions Sample response:\n{ \"policies\": [ { \"name\": \"snapshot-expiry-policy\", \"type\": \"system.snapshot-expiry\", \"appliedAt\": \"namespace\", \"content\": { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"min_snapshot_to_keep\": 1, \"max_snapshot_age_days\": 2, \"max_ref_age_days\": 3 } } }, { \"name\": \"compaction-policy\", \"type\": \"system.data-compaction\", \"appliedAt\": \"catalog\", \"content\": { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"target_file_size_bytes\": 134217728 } } } ] } API Reference For the complete and up-to-date API specification, see the policy-api.yaml.\n","categories":"","description":"","excerpt":"The Polaris Policy framework empowers organizations to centrally …","ref":"/in-dev/unreleased/policy/","tags":"","title":"Policy"},{"body":"The Polaris Policy framework empowers organizations to centrally define, manage, and enforce fine-grained governance, lifecycle, and operational rules across all data resources in the catalog.\nWith the policy API, you can:\nCreate and manage policies Attach policies to specific resources (catalogs, namespaces, tables, or views) Check applicable policies for any given resource What is a Policy? A policy in Apache Polaris is a structured entity that defines rules governing actions on specified resources under predefined conditions. Each policy contains:\nName: A unique identifier within a namespace Type: Determines the semantics and expected format of the policy content Description: Explains the purpose of the policy Content: Contains the actual rules defining the policy behavior Version: An automatically tracked revision number Inheritable: Whether the policy can be inherited by child resources, decided by its type Policy Types Polaris supports several predefined system policy types (prefixed with system.):\nPolicy Type Purpose JSON-Schema Applies To system.data-compaction Defines rules for data file compaction operations data-compaction/2025-02-03.json Iceberg table, namespace, catalog system.metadata-compaction Defines rules for metadata file compaction operations metadata-compaction/2025-02-03.json Iceberg table, namespace, catalog system.orphan-file-removal Defines rules for removing orphaned files orphan-file-removal/2025-02-03.json Iceberg table, namespace, catalog system.snapshot-expiry Defines rules for snapshot expiration snapshot-expiry/2025-02-03.json Iceberg table, namespace, catalog Support for additional predefined system policy types and custom policy type definitions is in progress. For more details, please refer to the roadmap.\nPolicy Inheritance The entity hierarchy in Polaris is structured as follows:\nCatalog | Namespace | +-----------+----------+ | | | Iceberg Iceberg Generic Table View Table Policies can be attached at any level, and inheritance flows from catalog down to namespace, then to tables and views.\nPolicies can be inheritable or non-inheritable:\nInheritable policies: Apply to the target resource and all its applicable child resources Non-inheritable policies: Apply only to the specific target resource The inheritance follows an override mechanism:\nTable-level policies override namespace and catalog policies Namespace-level policies override parent namespace and catalog policies Important: Because an override completely replaces the same policy type at higher levels, only one instance of a given policy type can be attached to (and therefore affect) a resource.\nWorking with Policies Creating a Policy To create a policy, you need to provide a name, type, and optionally a description and content:\nPOST /polaris/v1/{prefix}/namespaces/{namespace}/policies { \"name\": \"compaction-policy\", \"type\": \"system.data-compaction\", \"description\": \"Policy for optimizing table storage\", \"content\": \"{\\\"version\\\": \\\"2025-02-03\\\", \\\"enable\\\": true, \\\"config\\\": {\\\"target_file_size_bytes\\\": 134217728}}\" } The policy content is validated against a schema specific to its type. Here are a few policy content examples:\nData Compaction Policy { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"target_file_size_bytes\": 134217728, \"compaction_strategy\": \"bin-pack\", \"max-concurrent-file-group-rewrites\": 5 } } Orphan File Removal Policy { \"version\": \"2025-02-03\", \"enable\": true, \"max_orphan_file_age_in_days\": 30, \"locations\": [\"s3://my-bucket/my-table-location\"], \"config\": { \"prefix_mismatch_mode\": \"ignore\" } } Attaching Policies to Resources Policies can be attached to different resource levels:\nCatalog level: Applies to the entire catalog Namespace level: Applies to a specific namespace Table-like level: Applies to individual tables or views Example of attaching a policy to a table:\nPUT /polaris/v1/{prefix}/namespaces/{namespace}/policies/{policy-name}/mappings { \"target\": { \"type\": \"table-like\", \"path\": [\"NS1\", \"NS2\", \"test_table_1\"] } } For inheritable policies, only one policy of a given type can be attached to a resource. For non-inheritable policies, multiple policies of the same type can be attached.\nRetrieving Applicable Policies A user can view applicable policies on a resource (e.g., table, namespace, or catalog) as long as they have read permission on that resource.\nHere is an example to find all policies that apply to a specific resource (including inherited policies):\nGET /polaris/v1/catalog/applicable-policies?namespace=finance%1Fquarterly\u0026target-name=transactions Sample response:\n{ \"policies\": [ { \"name\": \"snapshot-expiry-policy\", \"type\": \"system.snapshot-expiry\", \"appliedAt\": \"namespace\", \"content\": { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"min_snapshot_to_keep\": 1, \"max_snapshot_age_days\": 2, \"max_ref_age_days\": 3 } } }, { \"name\": \"compaction-policy\", \"type\": \"system.data-compaction\", \"appliedAt\": \"catalog\", \"content\": { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"target_file_size_bytes\": 134217728 } } } ] } API Reference For the complete and up-to-date API specification, see the policy-api.yaml.\n","categories":"","description":"","excerpt":"The Polaris Policy framework empowers organizations to centrally …","ref":"/releases/1.0.0/policy/","tags":"","title":"Policy"},{"body":"The Polaris Policy framework empowers organizations to centrally define, manage, and enforce fine-grained governance, lifecycle, and operational rules across all data resources in the catalog.\nWith the policy API, you can:\nCreate and manage policies Attach policies to specific resources (catalogs, namespaces, tables, or views) Check applicable policies for any given resource What is a Policy? A policy in Apache Polaris is a structured entity that defines rules governing actions on specified resources under predefined conditions. Each policy contains:\nName: A unique identifier within a namespace Type: Determines the semantics and expected format of the policy content Description: Explains the purpose of the policy Content: Contains the actual rules defining the policy behavior Version: An automatically tracked revision number Inheritable: Whether the policy can be inherited by child resources, decided by its type Policy Types Polaris supports several predefined system policy types (prefixed with system.):\nPolicy Type Purpose JSON-Schema Applies To system.data-compaction Defines rules for data file compaction operations data-compaction/2025-02-03.json Iceberg table, namespace, catalog system.metadata-compaction Defines rules for metadata file compaction operations metadata-compaction/2025-02-03.json Iceberg table, namespace, catalog system.orphan-file-removal Defines rules for removing orphaned files orphan-file-removal/2025-02-03.json Iceberg table, namespace, catalog system.snapshot-expiry Defines rules for snapshot expiration snapshot-expiry/2025-02-03.json Iceberg table, namespace, catalog Support for additional predefined system policy types and custom policy type definitions is in progress. For more details, please refer to the roadmap.\nPolicy Inheritance The entity hierarchy in Polaris is structured as follows:\nCatalog | Namespace | +-----------+----------+ | | | Iceberg Iceberg Generic Table View Table Policies can be attached at any level, and inheritance flows from catalog down to namespace, then to tables and views.\nPolicies can be inheritable or non-inheritable:\nInheritable policies: Apply to the target resource and all its applicable child resources Non-inheritable policies: Apply only to the specific target resource The inheritance follows an override mechanism:\nTable-level policies override namespace and catalog policies Namespace-level policies override parent namespace and catalog policies Important: Because an override completely replaces the same policy type at higher levels, only one instance of a given policy type can be attached to (and therefore affect) a resource.\nWorking with Policies Creating a Policy To create a policy, you need to provide a name, type, and optionally a description and content:\nPOST /polaris/v1/{prefix}/namespaces/{namespace}/policies { \"name\": \"compaction-policy\", \"type\": \"system.data-compaction\", \"description\": \"Policy for optimizing table storage\", \"content\": \"{\\\"version\\\": \\\"2025-02-03\\\", \\\"enable\\\": true, \\\"config\\\": {\\\"target_file_size_bytes\\\": 134217728}}\" } The policy content is validated against a schema specific to its type. Here are a few policy content examples:\nData Compaction Policy { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"target_file_size_bytes\": 134217728, \"compaction_strategy\": \"bin-pack\", \"max-concurrent-file-group-rewrites\": 5 } } Orphan File Removal Policy { \"version\": \"2025-02-03\", \"enable\": true, \"max_orphan_file_age_in_days\": 30, \"locations\": [\"s3://my-bucket/my-table-location\"], \"config\": { \"prefix_mismatch_mode\": \"ignore\" } } Attaching Policies to Resources Policies can be attached to different resource levels:\nCatalog level: Applies to the entire catalog Namespace level: Applies to a specific namespace Table-like level: Applies to individual tables or views Example of attaching a policy to a table:\nPUT /polaris/v1/{prefix}/namespaces/{namespace}/policies/{policy-name}/mappings { \"target\": { \"type\": \"table-like\", \"path\": [\"NS1\", \"NS2\", \"test_table_1\"] } } For inheritable policies, only one policy of a given type can be attached to a resource. For non-inheritable policies, multiple policies of the same type can be attached.\nRetrieving Applicable Policies A user can view applicable policies on a resource (e.g., table, namespace, or catalog) as long as they have read permission on that resource.\nHere is an example to find all policies that apply to a specific resource (including inherited policies):\nGET /polaris/v1/catalog/applicable-policies?namespace=finance%1Fquarterly\u0026target-name=transactions Sample response:\n{ \"policies\": [ { \"name\": \"snapshot-expiry-policy\", \"type\": \"system.snapshot-expiry\", \"appliedAt\": \"namespace\", \"content\": { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"min_snapshot_to_keep\": 1, \"max_snapshot_age_days\": 2, \"max_ref_age_days\": 3 } } }, { \"name\": \"compaction-policy\", \"type\": \"system.data-compaction\", \"appliedAt\": \"catalog\", \"content\": { \"version\": \"2025-02-03\", \"enable\": true, \"config\": { \"target_file_size_bytes\": 134217728 } } } ] } API Reference For the complete and up-to-date API specification, see the policy-api.yaml.\n","categories":"","description":"","excerpt":"The Polaris Policy framework empowers organizations to centrally …","ref":"/releases/1.0.1/policy/","tags":"","title":"Policy"},{"body":"The Generic Table in Apache Polaris is designed to provide support for non-Iceberg tables across different table formats includes delta, csv etc. It currently provides the following capabilities:\nCreate a generic table under a namespace Load a generic table Drop a generic table List all generic tables under a namespace NOTE The current generic table is in beta release. Please use it with caution and report any issue if encountered.\nWhat is a Generic Table? A generic table in Polaris is an entity that defines the following fields:\nname (required): A unique identifier for the table within a namespace format (required): The format for the generic table, i.e. “delta”, “csv” base-location (optional): Table base location in URI format. For example: s3:///path/to/table The table base location is a location that includes all files for the table A table with multiple disjoint locations (i.e. containing files that are outside the configured base location) is not compliant with the current generic table support in Polaris. If no location is provided, clients or users are responsible for managing the location. properties (optional): Properties for the generic table passed on creation. Currently, there is no reserved property key defined. The property definition and interpretation is delegated to client or engine implementations. doc (optional): Comment or description for the table Generic Table API Vs. Iceberg Table API Generic Table provides a different set of APIs to operate on the generic table entities while Iceberg APIs operates on the Iceberg table entities.\nOperations Iceberg Table API Generic Table API Create Table Create an Iceberg table Create a generic table Load Table Load an Iceberg table. If the table to load is a generic table, you need to call the Generic Table loadTable API, otherwise a TableNotFoundException will be thrown Load a generic table. Similarly, try to load an Iceberg table through Generic Table API will thrown a TableNotFoundException. Drop Table Drop an Iceberg table. Similar as load table, if the table to drop is a Generic table, a tableNotFoundException will be thrown. Drop a generic table. Drop an Iceberg table through Generic table endpoint will thrown an TableNotFound Exception List Table List all Iceberg tables List all generic tables Note that generic table shares the same namespace with Iceberg tables, the table name has to be unique under the same namespace. Furthermore, since there is currently no support for Update Generic Table, any update to the existing table requires a drop and re-create.\nWorking with Generic Table There are two ways to work with Polaris Generic Tables today:\nDirectly communicate with Polaris through REST API calls using tools such as curl. Details will be described in the later section. Use the Spark client provided if you are working with Spark. Please refer to Polaris Spark Client for detailed instructions. Create a Generic Table To create a generic table, you need to provide the corresponding fields as described in What is a Generic Table.\nThe REST API for creating a generic Table is POST /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables, and the request body looks like the following:\n{ \"name\": \"\u003ctable_name\u003e\", \"format\": \"\u003ctable_format\u003e\", \"base-location\": \"\u003ctable_base_location\u003e\", \"doc\": \"\u003ccomment or description for table\u003e\", \"properties\": { \"\u003cproperty-key\u003e\": \"\u003cproperty-value\u003e\" } } Here is an example to create a generic table with name delta_table and format as delta under a namespace delta_ns for catalog delta_catalog using curl:\ncurl -X POST http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"delta_table\", \"format\": \"delta\", \"base-location\": \"s3://\u003cmy-bucket\u003e/path/to/table\", \"doc\": \"delta table example\", \"properties\": { \"key1\": \"value1\" } }' Load a Generic Table The REST endpoint for load a generic table is GET /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/{generic-table}.\nHere is an example to load the table delta_table using curl:\ncurl -X GET http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/delta_table And the response looks like the following:\n{ \"table\": { \"name\": \"delta_table\", \"format\": \"delta\", \"base-location\": \"s3://\u003cmy-bucket\u003e/path/to/table\", \"doc\": \"delta table example\", \"properties\": { \"key1\": \"value1\" } } } List Generic Tables The REST endpoint for listing the generic tables under a given namespace is GET /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/.\nFollowing curl command lists all tables under namespace delta_namespace:\ncurl -X GET http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/ Example Response:\n{ \"identifiers\": [ { \"namespace\": [\"delta_ns\"], \"name\": \"delta_table\" } ], \"next-page-token\": null } Drop a Generic Table The drop generic table REST endpoint is DELETE /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/{generic-table}\nThe following curl call drops the table delat_table:\ncurl -X DELETE http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/{generic-table} API Reference For the complete and up-to-date API specification, see the Catalog API Spec.\nLimitations Current limitations of Generic Table support:\nLimited spec information. Currently, there is no spec for information like Schema, Partition etc. No commit coordination or update capability provided at the catalog service level. Therefore, the catalog itself is unaware of anything about the underlying table except some of the loosely defined metadata. It is the responsibility of the engine (and plugins used by the engine) to determine exactly how loading or commiting data should look like based on the metadata. For example, with the delta support, th delta log serialization, deserialization and update all happens at client side.\n","categories":"","description":"","excerpt":"The Generic Table in Apache Polaris is designed to provide support for …","ref":"/in-dev/unreleased/generic-table/","tags":"","title":"Generic Table (Beta)"},{"body":"The Generic Table in Apache Polaris is designed to provide support for non-Iceberg tables across different table formats includes delta, csv etc. It currently provides the following capabilities:\nCreate a generic table under a namespace Load a generic table Drop a generic table List all generic tables under a namespace NOTE The current generic table is in beta release. Please use it with caution and report any issue if encountered.\nWhat is a Generic Table? A generic table in Polaris is an entity that defines the following fields:\nname (required): A unique identifier for the table within a namespace format (required): The format for the generic table, i.e. “delta”, “csv” base-location (optional): Table base location in URI format. For example: s3:///path/to/table The table base location is a location that includes all files for the table A table with multiple disjoint locations (i.e. containing files that are outside the configured base location) is not compliant with the current generic table support in Polaris. If no location is provided, clients or users are responsible for managing the location. properties (optional): Properties for the generic table passed on creation. Currently, there is no reserved property key defined. The property definition and interpretation is delegated to client or engine implementations. doc (optional): Comment or description for the table Generic Table API Vs. Iceberg Table API Generic Table provides a different set of APIs to operate on the generic table entities while Iceberg APIs operates on the Iceberg table entities.\nOperations Iceberg Table API Generic Table API Create Table Create an Iceberg table Create a generic table Load Table Load an Iceberg table. If the table to load is a generic table, you need to call the Generic Table loadTable API, otherwise a TableNotFoundException will be thrown Load a generic table. Similarly, try to load an Iceberg table through Generic Table API will thrown a TableNotFoundException. Drop Table Drop an Iceberg table. Similar as load table, if the table to drop is a Generic table, a tableNotFoundException will be thrown. Drop a generic table. Drop an Iceberg table through Generic table endpoint will thrown an TableNotFound Exception List Table List all Iceberg tables List all generic tables Note that generic table shares the same namespace with Iceberg tables, the table name has to be unique under the same namespace. Furthermore, since there is currently no support for Update Generic Table, any update to the existing table requires a drop and re-create.\nWorking with Generic Table There are two ways to work with Polaris Generic Tables today:\nDirectly communicate with Polaris through REST API calls using tools such as curl. Details will be described in the later section. Use the Spark client provided if you are working with Spark. Please refer to Polaris Spark Client for detailed instructions. Create a Generic Table To create a generic table, you need to provide the corresponding fields as described in What is a Generic Table.\nThe REST API for creating a generic Table is POST /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables, and the request body looks like the following:\n{ \"name\": \"\u003ctable_name\u003e\", \"format\": \"\u003ctable_format\u003e\", \"base-location\": \"\u003ctable_base_location\u003e\", \"doc\": \"\u003ccomment or description for table\u003e\", \"properties\": { \"\u003cproperty-key\u003e\": \"\u003cproperty-value\u003e\" } } Here is an example to create a generic table with name delta_table and format as delta under a namespace delta_ns for catalog delta_catalog using curl:\ncurl -X POST http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"delta_table\", \"format\": \"delta\", \"base-location\": \"s3://\u003cmy-bucket\u003e/path/to/table\", \"doc\": \"delta table example\", \"properties\": { \"key1\": \"value1\" } }' Load a Generic Table The REST endpoint for load a generic table is GET /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/{generic-table}.\nHere is an example to load the table delta_table using curl:\ncurl -X GET http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/delta_table And the response looks like the following:\n{ \"table\": { \"name\": \"delta_table\", \"format\": \"delta\", \"base-location\": \"s3://\u003cmy-bucket\u003e/path/to/table\", \"doc\": \"delta table example\", \"properties\": { \"key1\": \"value1\" } } } List Generic Tables The REST endpoint for listing the generic tables under a given namespace is GET /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/.\nFollowing curl command lists all tables under namespace delta_namespace:\ncurl -X GET http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/ Example Response:\n{ \"identifiers\": [ { \"namespace\": [\"delta_ns\"], \"name\": \"delta_table\" } ], \"next-page-token\": null } Drop a Generic Table The drop generic table REST endpoint is DELETE /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/{generic-table}\nThe following curl call drops the table delat_table:\ncurl -X DELETE http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/{generic-table} API Reference For the complete and up-to-date API specification, see the Catalog API Spec.\nLimitations Current limitations of Generic Table support:\nLimited spec information. Currently, there is no spec for information like Schema, Partition etc. No commit coordination or update capability provided at the catalog service level. Therefore, the catalog itself is unaware of anything about the underlying table except some of the loosely defined metadata. It is the responsibility of the engine (and plugins used by the engine) to determine exactly how loading or commiting data should look like based on the metadata. For example, with the delta support, th delta log serialization, deserialization and update all happens at client side.\n","categories":"","description":"","excerpt":"The Generic Table in Apache Polaris is designed to provide support for …","ref":"/releases/1.0.0/generic-table/","tags":"","title":"Generic Table (Beta)"},{"body":"The Generic Table in Apache Polaris is designed to provide support for non-Iceberg tables across different table formats includes delta, csv etc. It currently provides the following capabilities:\nCreate a generic table under a namespace Load a generic table Drop a generic table List all generic tables under a namespace NOTE The current generic table is in beta release. Please use it with caution and report any issue if encountered.\nWhat is a Generic Table? A generic table in Polaris is an entity that defines the following fields:\nname (required): A unique identifier for the table within a namespace format (required): The format for the generic table, i.e. “delta”, “csv” base-location (optional): Table base location in URI format. For example: s3:///path/to/table The table base location is a location that includes all files for the table A table with multiple disjoint locations (i.e. containing files that are outside the configured base location) is not compliant with the current generic table support in Polaris. If no location is provided, clients or users are responsible for managing the location. properties (optional): Properties for the generic table passed on creation. Currently, there is no reserved property key defined. The property definition and interpretation is delegated to client or engine implementations. doc (optional): Comment or description for the table Generic Table API Vs. Iceberg Table API Generic Table provides a different set of APIs to operate on the generic table entities while Iceberg APIs operates on the Iceberg table entities.\nOperations Iceberg Table API Generic Table API Create Table Create an Iceberg table Create a generic table Load Table Load an Iceberg table. If the table to load is a generic table, you need to call the Generic Table loadTable API, otherwise a TableNotFoundException will be thrown Load a generic table. Similarly, try to load an Iceberg table through Generic Table API will thrown a TableNotFoundException. Drop Table Drop an Iceberg table. Similar as load table, if the table to drop is a Generic table, a tableNotFoundException will be thrown. Drop a generic table. Drop an Iceberg table through Generic table endpoint will thrown an TableNotFound Exception List Table List all Iceberg tables List all generic tables Note that generic table shares the same namespace with Iceberg tables, the table name has to be unique under the same namespace. Furthermore, since there is currently no support for Update Generic Table, any update to the existing table requires a drop and re-create.\nWorking with Generic Table There are two ways to work with Polaris Generic Tables today:\nDirectly communicate with Polaris through REST API calls using tools such as curl. Details will be described in the later section. Use the Spark client provided if you are working with Spark. Please refer to Polaris Spark Client for detailed instructions. Create a Generic Table To create a generic table, you need to provide the corresponding fields as described in What is a Generic Table.\nThe REST API for creating a generic Table is POST /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables, and the request body looks like the following:\n{ \"name\": \"\u003ctable_name\u003e\", \"format\": \"\u003ctable_format\u003e\", \"base-location\": \"\u003ctable_base_location\u003e\", \"doc\": \"\u003ccomment or description for table\u003e\", \"properties\": { \"\u003cproperty-key\u003e\": \"\u003cproperty-value\u003e\" } } Here is an example to create a generic table with name delta_table and format as delta under a namespace delta_ns for catalog delta_catalog using curl:\ncurl -X POST http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"delta_table\", \"format\": \"delta\", \"base-location\": \"s3://\u003cmy-bucket\u003e/path/to/table\", \"doc\": \"delta table example\", \"properties\": { \"key1\": \"value1\" } }' Load a Generic Table The REST endpoint for load a generic table is GET /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/{generic-table}.\nHere is an example to load the table delta_table using curl:\ncurl -X GET http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/delta_table And the response looks like the following:\n{ \"table\": { \"name\": \"delta_table\", \"format\": \"delta\", \"base-location\": \"s3://\u003cmy-bucket\u003e/path/to/table\", \"doc\": \"delta table example\", \"properties\": { \"key1\": \"value1\" } } } List Generic Tables The REST endpoint for listing the generic tables under a given namespace is GET /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/.\nFollowing curl command lists all tables under namespace delta_namespace:\ncurl -X GET http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/ Example Response:\n{ \"identifiers\": [ { \"namespace\": [\"delta_ns\"], \"name\": \"delta_table\" } ], \"next-page-token\": null } Drop a Generic Table The drop generic table REST endpoint is DELETE /polaris/v1/{prefix}/namespaces/{namespace}/generic-tables/{generic-table}\nThe following curl call drops the table delat_table:\ncurl -X DELETE http://localhost:8181/api/catalog/polaris/v1/delta_catalog/namespaces/delta_ns/generic-tables/{generic-table} API Reference For the complete and up-to-date API specification, see the Catalog API Spec.\nLimitations Current limitations of Generic Table support:\nLimited spec information. Currently, there is no spec for information like Schema, Partition etc. No commit coordination or update capability provided at the catalog service level. Therefore, the catalog itself is unaware of anything about the underlying table except some of the loosely defined metadata. It is the responsibility of the engine (and plugins used by the engine) to determine exactly how loading or commiting data should look like based on the metadata. For example, with the delta support, th delta log serialization, deserialization and update all happens at client side.\n","categories":"","description":"","excerpt":"The Generic Table in Apache Polaris is designed to provide support for …","ref":"/releases/1.0.1/generic-table/","tags":"","title":"Generic Table (Beta)"},{"body":"Metrics Metrics are published using Micrometer; they are available from Polaris’s management interface (port 8282 by default) under the path /q/metrics. For example, if the server is running on localhost, the metrics can be accessed via http://localhost:8282/q/metrics.\nMetrics can be scraped by Prometheus or any compatible metrics scraping server. See: Prometheus for more information.\nAdditional tags can be added to the metrics by setting the polaris.metrics.tags.* property. Each tag is a key-value pair, where the key is the tag name and the value is the tag value. For example, to add a tag environment=prod to all metrics, set polaris.metrics.tags.environment=prod. Many tags can be added, such as below:\npolaris.metrics.tags.service=polaris polaris.metrics.tags.environment=prod polaris.metrics.tags.region=us-west-2 Note that by default Polaris adds one tag: application=Polaris. You can override this tag by setting the polaris.metrics.tags.application=\u003cnew-value\u003e property.\nRealm ID Tag Polaris can add the realm ID as a tag to all API and HTTP request metrics. This is disabled by default to prevent high cardinality issues, but can be enabled by setting the following properties:\npolaris.metrics.realm-id-tag.enable-in-api-metrics=true polaris.metrics.realm-id-tag.enable-in-http-metrics=true You should be particularly careful when enabling the realm ID tag in HTTP request metrics, as these metrics typically have a much higher cardinality than API request metrics.\nIn order to prevent the number of tags from growing indefinitely and causing performance issues or crashing the server, the number of unique realm IDs in HTTP request metrics is limited to 100 by default. If the number of unique realm IDs exceeds this value, a warning will be logged and no more HTTP request metrics will be recorded. This threshold can be changed by setting the polaris.metrics.realm-id-tag.http-metrics-max-cardinality property.\nTraces Traces are published using OpenTelemetry.\nBy default OpenTelemetry is disabled in Polaris, because there is no reasonable default for the collector endpoint for all cases.\nTo enable OpenTelemetry and publish traces for Polaris set quarkus.otel.sdk.disabled=false and configure a valid collector endpoint URL with http:// or https:// as the server property quarkus.otel.exporter.otlp.traces.endpoint.\nIf these properties are not set, the server will not publish traces.\nThe collector must talk the OpenTelemetry protocol (OTLP) and the port must be its gRPC port (by default 4317), e.g. “http://otlp-collector:4317”.\nBy default, Polaris adds a few attributes to the OpenTelemetry Resource to identify the server, and notably:\nservice.name: set to Apache Polaris Server (incubating); service.version: set to the Polaris version. You can override the default resource attributes or add additional ones by setting the quarkus.otel.resource.attributes property.\nThis property expects a comma-separated list of key-value pairs, where the key is the attribute name and the value is the attribute value. For example, to change the service name to Polaris and add an attribute deployment.environment=dev, set the following property:\nquarkus.otel.resource.attributes=service.name=Polaris,deployment.environment=dev The alternative syntax below can also be used:\nquarkus.otel.resource.attributes[0]=service.name=Polaris quarkus.otel.resource.attributes[1]=deployment.environment=dev Finally, two additional span attributes are added to all request parent spans:\npolaris.request.id: The unique identifier of the request, if set by the caller through the Polaris-Request-Id header. polaris.realm: The unique identifier of the realm. Always set (unless the request failed because of a realm resolution error). Troubleshooting Traces If the server is unable to publish traces, check first for a log warning message like the following:\nSEVERE [io.ope.exp.int.grp.OkHttpGrpcExporter] (OkHttp http://localhost:4317/...) Failed to export spans. The request could not be executed. Full error message: Failed to connect to localhost/0:0:0:0:0:0:0:1:4317 This means that the server is unable to connect to the collector. Check that the collector is running and that the URL is correct.\nLogging Polaris relies on Quarkus for logging.\nBy default, logs are written to the console and to a file located in the ./logs directory. The log file is rotated daily and compressed. The maximum size of the log file is 10MB, and the maximum number of backup files is 14.\nJSON logging can be enabled by setting the quarkus.log.console.json.enabled and quarkus.log.file.json.enabled properties to true. By default, JSON logging is disabled.\nThe log level can be set for the entire application or for specific packages. The default log level is INFO. To set the log level for the entire application, use the quarkus.log.level property.\nTo set the log level for a specific package, use the quarkus.log.category.\"package-name\".level, where package-name is the name of the package. For example, the package io.smallrye.config has a useful logger to help debugging configuration issues; but it needs to be set to the DEBUG level. This can be done by setting the following property:\nquarkus.log.category.\"io.smallrye.config\".level=DEBUG The log message format for both console and file output is highly configurable. The default format is:\n%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n Refer to the Logging format guide for more information on placeholders and how to customize the log message format.\nMDC Logging Polaris uses Mapped Diagnostic Context (MDC) to enrich log messages with additional context. The following MDC keys are available:\nrequestId: The unique identifier of the request, if set by the caller through the Polaris-Request-Id header. realmId: The unique identifier of the realm. Always set. traceId: The unique identifier of the trace. Present if tracing is enabled and the message is originating from a traced context. parentId: The unique identifier of the parent span. Present if tracing is enabled and the message is originating from a traced context. spanId: The unique identifier of the span. Present if tracing is enabled and the message is originating from a traced context. sampled: Whether the trace has been sampled. Present if tracing is enabled and the message is originating from a traced context. Other MDC keys can be added by setting the polaris.log.mdc.* property. Each property is a key-value pair, where the key is the MDC key name and the value is the MDC key value. For example, to add the MDC keys environment=prod and region=us-west-2 to all log messages, set the following properties:\npolaris.log.mdc.environment=prod polaris.log.mdc.region=us-west-2 MDC context is propagated across threads, including in TaskExecutor threads.\n","categories":"","description":"","excerpt":"Metrics Metrics are published using Micrometer; they are available …","ref":"/in-dev/unreleased/telemetry/","tags":"","title":"Telemetry"},{"body":"Metrics Metrics are published using Micrometer; they are available from Polaris’s management interface (port 8282 by default) under the path /q/metrics. For example, if the server is running on localhost, the metrics can be accessed via http://localhost:8282/q/metrics.\nMetrics can be scraped by Prometheus or any compatible metrics scraping server. See: Prometheus for more information.\nAdditional tags can be added to the metrics by setting the polaris.metrics.tags.* property. Each tag is a key-value pair, where the key is the tag name and the value is the tag value. For example, to add a tag environment=prod to all metrics, set polaris.metrics.tags.environment=prod. Many tags can be added, such as below:\npolaris.metrics.tags.service=polaris polaris.metrics.tags.environment=prod polaris.metrics.tags.region=us-west-2 Note that by default Polaris adds one tag: application=Polaris. You can override this tag by setting the polaris.metrics.tags.application=\u003cnew-value\u003e property.\nRealm ID Tag Polaris can add the realm ID as a tag to all API and HTTP request metrics. This is disabled by default to prevent high cardinality issues, but can be enabled by setting the following properties:\npolaris.metrics.realm-id-tag.enable-in-api-metrics=true polaris.metrics.realm-id-tag.enable-in-http-metrics=true You should be particularly careful when enabling the realm ID tag in HTTP request metrics, as these metrics typically have a much higher cardinality than API request metrics.\nIn order to prevent the number of tags from growing indefinitely and causing performance issues or crashing the server, the number of unique realm IDs in HTTP request metrics is limited to 100 by default. If the number of unique realm IDs exceeds this value, a warning will be logged and no more HTTP request metrics will be recorded. This threshold can be changed by setting the polaris.metrics.realm-id-tag.http-metrics-max-cardinality property.\nTraces Traces are published using OpenTelemetry.\nBy default OpenTelemetry is disabled in Polaris, because there is no reasonable default for the collector endpoint for all cases.\nTo enable OpenTelemetry and publish traces for Polaris set quarkus.otel.sdk.disabled=false and configure a valid collector endpoint URL with http:// or https:// as the server property quarkus.otel.exporter.otlp.traces.endpoint.\nIf these properties are not set, the server will not publish traces.\nThe collector must talk the OpenTelemetry protocol (OTLP) and the port must be its gRPC port (by default 4317), e.g. “http://otlp-collector:4317”.\nBy default, Polaris adds a few attributes to the OpenTelemetry Resource to identify the server, and notably:\nservice.name: set to Apache Polaris Server (incubating); service.version: set to the Polaris version. You can override the default resource attributes or add additional ones by setting the quarkus.otel.resource.attributes property.\nThis property expects a comma-separated list of key-value pairs, where the key is the attribute name and the value is the attribute value. For example, to change the service name to Polaris and add an attribute deployment.environment=dev, set the following property:\nquarkus.otel.resource.attributes=service.name=Polaris,deployment.environment=dev The alternative syntax below can also be used:\nquarkus.otel.resource.attributes[0]=service.name=Polaris quarkus.otel.resource.attributes[1]=deployment.environment=dev Finally, two additional span attributes are added to all request parent spans:\npolaris.request.id: The unique identifier of the request, if set by the caller through the Polaris-Request-Id header. polaris.realm: The unique identifier of the realm. Always set (unless the request failed because of a realm resolution error). Troubleshooting Traces If the server is unable to publish traces, check first for a log warning message like the following:\nSEVERE [io.ope.exp.int.grp.OkHttpGrpcExporter] (OkHttp http://localhost:4317/...) Failed to export spans. The request could not be executed. Full error message: Failed to connect to localhost/0:0:0:0:0:0:0:1:4317 This means that the server is unable to connect to the collector. Check that the collector is running and that the URL is correct.\nLogging Polaris relies on Quarkus for logging.\nBy default, logs are written to the console and to a file located in the ./logs directory. The log file is rotated daily and compressed. The maximum size of the log file is 10MB, and the maximum number of backup files is 14.\nJSON logging can be enabled by setting the quarkus.log.console.json and quarkus.log.file.json properties to true. By default, JSON logging is disabled.\nThe log level can be set for the entire application or for specific packages. The default log level is INFO. To set the log level for the entire application, use the quarkus.log.level property.\nTo set the log level for a specific package, use the quarkus.log.category.\"package-name\".level, where package-name is the name of the package. For example, the package io.smallrye.config has a useful logger to help debugging configuration issues; but it needs to be set to the DEBUG level. This can be done by setting the following property:\nquarkus.log.category.\"io.smallrye.config\".level=DEBUG The log message format for both console and file output is highly configurable. The default format is:\n%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n Refer to the Logging format guide for more information on placeholders and how to customize the log message format.\nMDC Logging Polaris uses Mapped Diagnostic Context (MDC) to enrich log messages with additional context. The following MDC keys are available:\nrequestId: The unique identifier of the request, if set by the caller through the Polaris-Request-Id header. realmId: The unique identifier of the realm. Always set. traceId: The unique identifier of the trace. Present if tracing is enabled and the message is originating from a traced context. parentId: The unique identifier of the parent span. Present if tracing is enabled and the message is originating from a traced context. spanId: The unique identifier of the span. Present if tracing is enabled and the message is originating from a traced context. sampled: Whether the trace has been sampled. Present if tracing is enabled and the message is originating from a traced context. Other MDC keys can be added by setting the polaris.log.mdc.* property. Each property is a key-value pair, where the key is the MDC key name and the value is the MDC key value. For example, to add the MDC keys environment=prod and region=us-west-2 to all log messages, set the following properties:\npolaris.log.mdc.environment=prod polaris.log.mdc.region=us-west-2 MDC context is propagated across threads, including in TaskExecutor threads.\n","categories":"","description":"","excerpt":"Metrics Metrics are published using Micrometer; they are available …","ref":"/releases/1.0.0/telemetry/","tags":"","title":"Telemetry"},{"body":"Metrics Metrics are published using Micrometer; they are available from Polaris’s management interface (port 8282 by default) under the path /q/metrics. For example, if the server is running on localhost, the metrics can be accessed via http://localhost:8282/q/metrics.\nMetrics can be scraped by Prometheus or any compatible metrics scraping server. See: Prometheus for more information.\nAdditional tags can be added to the metrics by setting the polaris.metrics.tags.* property. Each tag is a key-value pair, where the key is the tag name and the value is the tag value. For example, to add a tag environment=prod to all metrics, set polaris.metrics.tags.environment=prod. Many tags can be added, such as below:\npolaris.metrics.tags.service=polaris polaris.metrics.tags.environment=prod polaris.metrics.tags.region=us-west-2 Note that by default Polaris adds one tag: application=Polaris. You can override this tag by setting the polaris.metrics.tags.application=\u003cnew-value\u003e property.\nRealm ID Tag Polaris can add the realm ID as a tag to all API and HTTP request metrics. This is disabled by default to prevent high cardinality issues, but can be enabled by setting the following properties:\npolaris.metrics.realm-id-tag.enable-in-api-metrics=true polaris.metrics.realm-id-tag.enable-in-http-metrics=true You should be particularly careful when enabling the realm ID tag in HTTP request metrics, as these metrics typically have a much higher cardinality than API request metrics.\nIn order to prevent the number of tags from growing indefinitely and causing performance issues or crashing the server, the number of unique realm IDs in HTTP request metrics is limited to 100 by default. If the number of unique realm IDs exceeds this value, a warning will be logged and no more HTTP request metrics will be recorded. This threshold can be changed by setting the polaris.metrics.realm-id-tag.http-metrics-max-cardinality property.\nTraces Traces are published using OpenTelemetry.\nBy default OpenTelemetry is disabled in Polaris, because there is no reasonable default for the collector endpoint for all cases.\nTo enable OpenTelemetry and publish traces for Polaris set quarkus.otel.sdk.disabled=false and configure a valid collector endpoint URL with http:// or https:// as the server property quarkus.otel.exporter.otlp.traces.endpoint.\nIf these properties are not set, the server will not publish traces.\nThe collector must talk the OpenTelemetry protocol (OTLP) and the port must be its gRPC port (by default 4317), e.g. “http://otlp-collector:4317”.\nBy default, Polaris adds a few attributes to the OpenTelemetry Resource to identify the server, and notably:\nservice.name: set to Apache Polaris Server (incubating); service.version: set to the Polaris version. You can override the default resource attributes or add additional ones by setting the quarkus.otel.resource.attributes property.\nThis property expects a comma-separated list of key-value pairs, where the key is the attribute name and the value is the attribute value. For example, to change the service name to Polaris and add an attribute deployment.environment=dev, set the following property:\nquarkus.otel.resource.attributes=service.name=Polaris,deployment.environment=dev The alternative syntax below can also be used:\nquarkus.otel.resource.attributes[0]=service.name=Polaris quarkus.otel.resource.attributes[1]=deployment.environment=dev Finally, two additional span attributes are added to all request parent spans:\npolaris.request.id: The unique identifier of the request, if set by the caller through the Polaris-Request-Id header. polaris.realm: The unique identifier of the realm. Always set (unless the request failed because of a realm resolution error). Troubleshooting Traces If the server is unable to publish traces, check first for a log warning message like the following:\nSEVERE [io.ope.exp.int.grp.OkHttpGrpcExporter] (OkHttp http://localhost:4317/...) Failed to export spans. The request could not be executed. Full error message: Failed to connect to localhost/0:0:0:0:0:0:0:1:4317 This means that the server is unable to connect to the collector. Check that the collector is running and that the URL is correct.\nLogging Polaris relies on Quarkus for logging.\nBy default, logs are written to the console and to a file located in the ./logs directory. The log file is rotated daily and compressed. The maximum size of the log file is 10MB, and the maximum number of backup files is 14.\nJSON logging can be enabled by setting the quarkus.log.console.json and quarkus.log.file.json properties to true. By default, JSON logging is disabled.\nThe log level can be set for the entire application or for specific packages. The default log level is INFO. To set the log level for the entire application, use the quarkus.log.level property.\nTo set the log level for a specific package, use the quarkus.log.category.\"package-name\".level, where package-name is the name of the package. For example, the package io.smallrye.config has a useful logger to help debugging configuration issues; but it needs to be set to the DEBUG level. This can be done by setting the following property:\nquarkus.log.category.\"io.smallrye.config\".level=DEBUG The log message format for both console and file output is highly configurable. The default format is:\n%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n Refer to the Logging format guide for more information on placeholders and how to customize the log message format.\nMDC Logging Polaris uses Mapped Diagnostic Context (MDC) to enrich log messages with additional context. The following MDC keys are available:\nrequestId: The unique identifier of the request, if set by the caller through the Polaris-Request-Id header. realmId: The unique identifier of the realm. Always set. traceId: The unique identifier of the trace. Present if tracing is enabled and the message is originating from a traced context. parentId: The unique identifier of the parent span. Present if tracing is enabled and the message is originating from a traced context. spanId: The unique identifier of the span. Present if tracing is enabled and the message is originating from a traced context. sampled: Whether the trace has been sampled. Present if tracing is enabled and the message is originating from a traced context. Other MDC keys can be added by setting the polaris.log.mdc.* property. Each property is a key-value pair, where the key is the MDC key name and the value is the MDC key value. For example, to add the MDC keys environment=prod and region=us-west-2 to all log messages, set the following properties:\npolaris.log.mdc.environment=prod polaris.log.mdc.region=us-west-2 MDC context is propagated across threads, including in TaskExecutor threads.\n","categories":"","description":"","excerpt":"Metrics Metrics are published using Micrometer; they are available …","ref":"/releases/1.0.1/telemetry/","tags":"","title":"Telemetry"},{"body":"This section provides information about how access control works for Apache Polaris (Incubating).\nPolaris uses a role-based access control (RBAC) model in which the Polaris administrator assigns access privileges to catalog roles and then grants access to resources to service principals by assigning catalog roles to principal roles.\nThese are the key concepts to understanding access control in Polaris:\nSecurable object Principal role Catalog role Privilege Securable object A securable object is an object to which access can be granted. Polaris has the following securable objects:\nCatalog Namespace Iceberg table View Policy Principal role A principal role is a resource in Polaris that you can use to logically group Polaris service principals together and grant privileges on securable objects.\nPolaris supports a many-to-one relationship between service principals and principal roles. For example, to grant the same privileges to multiple service principals, you can grant a single principal role to those service principals. A service principal can be granted one principal role. When registering a service connection, the Polaris administrator specifies the principal role that is granted to the service principal.\nYou don’t grant privileges directly to a principal role. Instead, you configure object permissions at the catalog role level, and then grant catalog roles to a principal role.\nThe following table shows examples of principal roles that you might configure in Polaris:\nPrincipal role name Description Data_engineer A role that is granted to multiple service principals for running data engineering jobs. Data_scientist A role that is granted to multiple service principals for running data science or AI jobs. Catalog role A catalog role belongs to a particular catalog resource in Polaris and specifies a set of permissions for actions on the catalog or objects in the catalog, such as catalog namespaces or tables. You can create one or more catalog roles for a catalog.\nYou grant privileges to a catalog role and then grant the catalog role to a principal role to bestow the privileges to one or more service principals.\nPolaris also supports a many-to-many relationship between catalog roles and principal roles. You can grant the same catalog role to one or more principal roles. Likewise, a principal role can be granted to one or more catalog roles.\nThe following table displays examples of catalog roles that you might configure in Polaris:\nExample Catalog role Description Catalog administrators A role that has been granted multiple privileges to emulate full access to the catalog. Principal roles that have been granted this role are permitted to create, alter, read, write, and drop tables in the catalog. Catalog readers A role that has been granted read-only privileges to tables in the catalog. Principal roles that have been granted this role are allowed to read from tables in the catalog. Catalog contributor A role that has been granted read and write access privileges to all tables that belong to the catalog. Principal roles that have been granted this role are allowed to perform read and write operations on tables in the catalog. RBAC model The following diagram illustrates the RBAC model used by Polaris. For each catalog, the Polaris administrator assigns access privileges to catalog roles and then grants service principals access to resources by assigning catalog roles to principal roles. Polaris supports a many-to-one relationship between service principals and principal roles.\nAccess control privileges This section describes the privileges that are available in the Polaris access control model. Privileges are granted to catalog roles, catalog roles are granted to principal roles, and principal roles are granted to service principals to specify the operations that service principals can perform on objects in Polaris.\nTo grant the full set of privileges (drop, list, read, write, etc.) on an object, you can use the full privilege option.\nTable privileges Privilege Description TABLE_CREATE Enables registering a table with the catalog. TABLE_DROP Enables dropping a table from the catalog. TABLE_LIST Enables listing any table in the catalog. TABLE_READ_PROPERTIES Enables reading properties of the table. TABLE_WRITE_PROPERTIES Enables configuring properties for the table. TABLE_READ_DATA Enables reading data from the table by receiving short-lived read-only storage credentials from the catalog. TABLE_WRITE_DATA Enables writing data to the table by receiving short-lived read+write storage credentials from the catalog. TABLE_FULL_METADATA Grants all table privileges, except TABLE_READ_DATA and TABLE_WRITE_DATA, which need to be granted individually. TABLE_ATTACH_POLICY Enables attaching policy to a table. TABLE_DETACH_POLICY Enables detaching policy from a table. View privileges Privilege Description VIEW_CREATE Enables registering a view with the catalog. VIEW_DROP Enables dropping a view from the catalog. VIEW_LIST Enables listing any views in the catalog. VIEW_READ_PROPERTIES Enables reading all the view properties. VIEW_WRITE_PROPERTIES Enables configuring view properties. VIEW_FULL_METADATA Grants all view privileges. Namespace privileges Privilege Description NAMESPACE_CREATE Enables creating a namespace in a catalog. NAMESPACE_DROP Enables dropping the namespace from the catalog. NAMESPACE_LIST Enables listing any object in the namespace, including nested namespaces and tables. NAMESPACE_READ_PROPERTIES Enables reading all the namespace properties. NAMESPACE_WRITE_PROPERTIES Enables configuring namespace properties. NAMESPACE_FULL_METADATA Grants all namespace privileges. NAMESPACE_ATTACH_POLICY Enables attaching policy to a namespace. NAMESPACE_DETACH_POLICY Enables detaching policy from a namespace. Catalog privileges Privilege Description CATALOG_MANAGE_ACCESS Includes the ability to grant or revoke privileges on objects in a catalog to catalog roles, and the ability to grant or revoke catalog roles to or from principal roles. CATALOG_MANAGE_CONTENT Enables full management of content for the catalog. This privilege encompasses the following privileges:CATALOG_MANAGE_METADATATABLE_FULL_METADATANAMESPACE_FULL_METADATAVIEW_FULL_METADATATABLE_WRITE_DATATABLE_READ_DATACATALOG_READ_PROPERTIESCATALOG_WRITE_PROPERTIES CATALOG_MANAGE_METADATA Enables full management of the catalog, catalog roles, namespaces, and tables. CATALOG_READ_PROPERTIES Enables listing catalogs and reading properties of the catalog. CATALOG_WRITE_PROPERTIES Enables configuring catalog properties. CATALOG_ATTACH_POLICY Enables attaching policy to a catalog. CATALOG_DETACH_POLICY Enables detaching policy from a catalog. Policy privileges Privilege Description POLICY_CREATE Enables creating a policy under specified namespace. POLICY_READ Enables reading policy content and metadata. POLICY_WRITE Enables updating the policy details such as its content or description. POLICY_LIST Enables listing any policy from the catalog. POLICY_DROP Enables dropping a policy if it is not attached to any resource entity. POLICY_FULL_METADATA Grants all policy privileges. POLICY_ATTACH Enables policy to be attached to entities. POLICY_DETACH Enables policy to be detached from entities. RBAC example The following diagram illustrates how RBAC works in Polaris and includes the following users:\nAlice: A service admin who signs up for Polaris. Alice can create service principals. She can also create catalogs and namespaces and configure access control for Polaris resources.\nBob: A data engineer who uses Apache Spark™ to interact with Polaris.\nAlice has created a service principal for Bob. It has been granted the Data_engineer principal role, which in turn has been granted the following catalog roles: Catalog contributor and Data administrator (for both the Silver and Gold zone catalogs in the following diagram).\nThe Catalog contributor role grants permission to create namespaces and tables in the Bronze zone catalog.\nThe Data administrator roles grant full administrative rights to the Silver zone catalog and Gold zone catalog.\nMark: A data scientist who uses trains models with data managed by Polaris.\nAlice has created a service principal for Mark. It has been granted the Data_scientist principal role, which in turn has been granted the catalog role named Catalog reader.\nThe Catalog reader role grants read-only access for a catalog named Gold zone catalog.\n","categories":"","description":"","excerpt":"This section provides information about how access control works for …","ref":"/in-dev/unreleased/access-control/","tags":"","title":"Access Control"},{"body":"This section provides information about how access control works for Apache Polaris (Incubating).\nPolaris uses a role-based access control (RBAC) model in which the Polaris administrator assigns access privileges to catalog roles and then grants access to resources to service principals by assigning catalog roles to principal roles.\nThese are the key concepts to understanding access control in Polaris:\nSecurable object Principal role Catalog role Privilege Securable object A securable object is an object to which access can be granted. Polaris has the following securable objects:\nCatalog Namespace Iceberg table View Principal role A principal role is a resource in Polaris that you can use to logically group Polaris service principals together and grant privileges on securable objects.\nPolaris supports a many-to-one relationship between service principals and principal roles. For example, to grant the same privileges to multiple service principals, you can grant a single principal role to those service principals. A service principal can be granted one principal role. When registering a service connection, the Polaris administrator specifies the principal role that is granted to the service principal.\nYou don’t grant privileges directly to a principal role. Instead, you configure object permissions at the catalog role level, and then grant catalog roles to a principal role.\nThe following table shows examples of principal roles that you might configure in Polaris:\nPrincipal role name Description Data_engineer A role that is granted to multiple service principals for running data engineering jobs. Data_scientist A role that is granted to multiple service principals for running data science or AI jobs. Catalog role A catalog role belongs to a particular catalog resource in Polaris and specifies a set of permissions for actions on the catalog or objects in the catalog, such as catalog namespaces or tables. You can create one or more catalog roles for a catalog.\nYou grant privileges to a catalog role and then grant the catalog role to a principal role to bestow the privileges to one or more service principals.\nNote\nIf you update the privileges bestowed to a service principal, the updates won’t take effect for up to one hour. This means that if you revoke or grant some privileges for a catalog, the updated privileges won’t take effect on any service principal with access to that catalog for up to one hour.\nPolaris also supports a many-to-many relationship between catalog roles and principal roles. You can grant the same catalog role to one or more principal roles. Likewise, a principal role can be granted to one or more catalog roles.\nThe following table displays examples of catalog roles that you might configure in Polaris:\nExample Catalog role Description Catalog administrators A role that has been granted multiple privileges to emulate full access to the catalog.\nPrincipal roles that have been granted this role are permitted to create, alter, read, write, and drop tables in the catalog. Catalog readers A role that has been granted read-only privileges to tables in the catalog.\nPrincipal roles that have been granted this role are allowed to read from tables in the catalog. Catalog contributor A role that has been granted read and write access privileges to all tables that belong to the catalog.\nPrincipal roles that have been granted this role are allowed to perform read and write operations on tables in the catalog. RBAC model The following diagram illustrates the RBAC model used by Polaris. For each catalog, the Polaris administrator assigns access privileges to catalog roles and then grants service principals access to resources by assigning catalog roles to principal roles. Polaris supports a many-to-one relationship between service principals and principal roles.\nAccess control privileges This section describes the privileges that are available in the Polaris access control model. Privileges are granted to catalog roles, catalog roles are granted to principal roles, and principal roles are granted to service principals to specify the operations that service principals can perform on objects in Polaris.\nImportant\nYou can only grant privileges at the catalog level. Fine-grained access controls are not available. For example, you can grant read privileges to all tables in a catalog but not to an individual table in the catalog.\nTo grant the full set of privileges (drop, list, read, write, etc.) on an object, you can use the full privilege option.\nTable privileges Privilege Description TABLE_CREATE Enables registering a table with the catalog. TABLE_DROP Enables dropping a table from the catalog. TABLE_LIST Enables listing any tables in the catalog. TABLE_READ_PROPERTIES Enables reading properties of the table. TABLE_WRITE_PROPERTIES Enables configuring properties for the table. TABLE_READ_DATA Enables reading data from the table by receiving short-lived read-only storage credentials from the catalog. TABLE_WRITE_DATA Enables writing data to the table by receiving short-lived read+write storage credentials from the catalog. TABLE_FULL_METADATA Grants all table privileges, except TABLE_READ_DATA and TABLE_WRITE_DATA, which need to be granted individually. View privileges Privilege Description VIEW_CREATE Enables registering a view with the catalog. VIEW_DROP Enables dropping a view from the catalog. VIEW_LIST Enables listing any views in the catalog. VIEW_READ_PROPERTIES Enables reading all the view properties. VIEW_WRITE_PROPERTIES Enables configuring view properties. VIEW_FULL_METADATA Grants all view privileges. Namespace privileges Privilege Description NAMESPACE_CREATE Enables creating a namespace in a catalog. NAMESPACE_DROP Enables dropping the namespace from the catalog. NAMESPACE_LIST Enables listing any object in the namespace, including nested namespaces and tables. NAMESPACE_READ_PROPERTIES Enables reading all the namespace properties. NAMESPACE_WRITE_PROPERTIES Enables configuring namespace properties. NAMESPACE_FULL_METADATA Grants all namespace privileges. Catalog privileges Privilege Description CATALOG_MANAGE_ACCESS Includes the ability to grant or revoke privileges on objects in a catalog to catalog roles, and the ability to grant or revoke catalog roles to or from principal roles. CATALOG_MANAGE_CONTENT Enables full management of content for the catalog. This privilege encompasses the following privileges:CATALOG_MANAGE_METADATATABLE_FULL_METADATANAMESPACE_FULL_METADATAVIEW_FULL_METADATATABLE_WRITE_DATATABLE_READ_DATACATALOG_READ_PROPERTIESCATALOG_WRITE_PROPERTIES CATALOG_MANAGE_METADATA Enables full management of the catalog, catalog roles, namespaces, and tables. CATALOG_READ_PROPERTIES Enables listing catalogs and reading properties of the catalog. CATALOG_WRITE_PROPERTIES Enables configuring catalog properties. RBAC example The following diagram illustrates how RBAC works in Polaris and includes the following users:\nAlice: A service admin who signs up for Polaris. Alice can create service principals. She can also create catalogs and namespaces and configure access control for Polaris resources.\nBob: A data engineer who uses Apache Spark™ to interact with Polaris.\nAlice has created a service principal for Bob. It has been granted the Data_engineer principal role, which in turn has been granted the following catalog roles: Catalog contributor and Data administrator (for both the Silver and Gold zone catalogs in the following diagram).\nThe Catalog contributor role grants permission to create namespaces and tables in the Bronze zone catalog.\nThe Data administrator roles grant full administrative rights to the Silver zone catalog and Gold zone catalog.\nMark: A data scientist who uses trains models with data managed by Polaris.\nAlice has created a service principal for Mark. It has been granted the Data_scientist principal role, which in turn has been granted the catalog role named Catalog reader.\nThe Catalog reader role grants read-only access for a catalog named Gold zone catalog.\n","categories":"","description":"","excerpt":"This section provides information about how access control works for …","ref":"/releases/0.9.0/access-control/","tags":"","title":"Access Control"},{"body":"This section provides information about how access control works for Apache Polaris (Incubating).\nPolaris uses a role-based access control (RBAC) model in which the Polaris administrator assigns access privileges to catalog roles and then grants access to resources to service principals by assigning catalog roles to principal roles.\nThese are the key concepts to understanding access control in Polaris:\nSecurable object Principal role Catalog role Privilege Securable object A securable object is an object to which access can be granted. Polaris has the following securable objects:\nCatalog Namespace Iceberg table View Principal role A principal role is a resource in Polaris that you can use to logically group Polaris service principals together and grant privileges on securable objects.\nPolaris supports a many-to-one relationship between service principals and principal roles. For example, to grant the same privileges to multiple service principals, you can grant a single principal role to those service principals. A service principal can be granted one principal role. When registering a service connection, the Polaris administrator specifies the principal role that is granted to the service principal.\nYou don’t grant privileges directly to a principal role. Instead, you configure object permissions at the catalog role level, and then grant catalog roles to a principal role.\nThe following table shows examples of principal roles that you might configure in Polaris:\nPrincipal role name Description Data_engineer A role that is granted to multiple service principals for running data engineering jobs. Data_scientist A role that is granted to multiple service principals for running data science or AI jobs. Catalog role A catalog role belongs to a particular catalog resource in Polaris and specifies a set of permissions for actions on the catalog or objects in the catalog, such as catalog namespaces or tables. You can create one or more catalog roles for a catalog.\nYou grant privileges to a catalog role and then grant the catalog role to a principal role to bestow the privileges to one or more service principals.\nNote\nIf you update the privileges bestowed to a service principal, the updates won’t take effect for up to one hour. This means that if you revoke or grant some privileges for a catalog, the updated privileges won’t take effect on any service principal with access to that catalog for up to one hour.\nPolaris also supports a many-to-many relationship between catalog roles and principal roles. You can grant the same catalog role to one or more principal roles. Likewise, a principal role can be granted to one or more catalog roles.\nThe following table displays examples of catalog roles that you might configure in Polaris:\nExample Catalog role Description Catalog administrators A role that has been granted multiple privileges to emulate full access to the catalog. Principal roles that have been granted this role are permitted to create, alter, read, write, and drop tables in the catalog. Catalog readers A role that has been granted read-only privileges to tables in the catalog. Principal roles that have been granted this role are allowed to read from tables in the catalog. Catalog contributor A role that has been granted read and write access privileges to all tables that belong to the catalog. Principal roles that have been granted this role are allowed to perform read and write operations on tables in the catalog. RBAC model The following diagram illustrates the RBAC model used by Polaris. For each catalog, the Polaris administrator assigns access privileges to catalog roles and then grants service principals access to resources by assigning catalog roles to principal roles. Polaris supports a many-to-one relationship between service principals and principal roles.\nAccess control privileges This section describes the privileges that are available in the Polaris access control model. Privileges are granted to catalog roles, catalog roles are granted to principal roles, and principal roles are granted to service principals to specify the operations that service principals can perform on objects in Polaris.\nImportant\nYou can only grant privileges at the catalog level. Fine-grained access controls are not available. For example, you can grant read privileges to all tables in a catalog but not to an individual table in the catalog.\nTo grant the full set of privileges (drop, list, read, write, etc.) on an object, you can use the full privilege option.\nTable privileges Privilege Description TABLE_CREATE Enables registering a table with the catalog. TABLE_DROP Enables dropping a table from the catalog. TABLE_LIST Enables listing any table in the catalog. TABLE_READ_PROPERTIES Enables reading properties of the table. TABLE_WRITE_PROPERTIES Enables configuring properties for the table. TABLE_READ_DATA Enables reading data from the table by receiving short-lived read-only storage credentials from the catalog. TABLE_WRITE_DATA Enables writing data to the table by receiving short-lived read+write storage credentials from the catalog. TABLE_FULL_METADATA Grants all table privileges, except TABLE_READ_DATA and TABLE_WRITE_DATA, which need to be granted individually. TABLE_ATTACH_POLICY Enables attaching policy to a table. TABLE_DETACH_POLICY Enables detaching policy from a table. View privileges Privilege Description VIEW_CREATE Enables registering a view with the catalog. VIEW_DROP Enables dropping a view from the catalog. VIEW_LIST Enables listing any views in the catalog. VIEW_READ_PROPERTIES Enables reading all the view properties. VIEW_WRITE_PROPERTIES Enables configuring view properties. VIEW_FULL_METADATA Grants all view privileges. Namespace privileges Privilege Description NAMESPACE_CREATE Enables creating a namespace in a catalog. NAMESPACE_DROP Enables dropping the namespace from the catalog. NAMESPACE_LIST Enables listing any object in the namespace, including nested namespaces and tables. NAMESPACE_READ_PROPERTIES Enables reading all the namespace properties. NAMESPACE_WRITE_PROPERTIES Enables configuring namespace properties. NAMESPACE_FULL_METADATA Grants all namespace privileges. NAMESPACE_ATTACH_POLICY Enables attaching policy to a namespace. NAMESPACE_DETACH_POLICY Enables detaching policy from a namespace. Catalog privileges Privilege Description CATALOG_MANAGE_ACCESS Includes the ability to grant or revoke privileges on objects in a catalog to catalog roles, and the ability to grant or revoke catalog roles to or from principal roles. CATALOG_MANAGE_CONTENT Enables full management of content for the catalog. This privilege encompasses the following privileges:CATALOG_MANAGE_METADATATABLE_FULL_METADATANAMESPACE_FULL_METADATAVIEW_FULL_METADATATABLE_WRITE_DATATABLE_READ_DATACATALOG_READ_PROPERTIESCATALOG_WRITE_PROPERTIES CATALOG_MANAGE_METADATA Enables full management of the catalog, catalog roles, namespaces, and tables. CATALOG_READ_PROPERTIES Enables listing catalogs and reading properties of the catalog. CATALOG_WRITE_PROPERTIES Enables configuring catalog properties. CATALOG_ATTACH_POLICY Enables attaching policy to a catalog. CATALOG_DETACH_POLICY Enables detaching policy from a catalog. Policy privileges Privilege Description POLICY_CREATE Enables creating a policy under specified namespace. POLICY_READ Enables reading policy content and metadata. POLICY_WRITE Enables updating the policy details such as its content or description. POLICY_LIST Enables listing any policy from the catalog. POLICY_DROP Enables dropping a policy if it is not attached to any resource entity. POLICY_FULL_METADATA Grants all policy privileges. POLICY_ATTACH Enables policy to be attached to entities. POLICY_DETACH Enables policy to be detached from entities. RBAC example The following diagram illustrates how RBAC works in Polaris and includes the following users:\nAlice: A service admin who signs up for Polaris. Alice can create service principals. She can also create catalogs and namespaces and configure access control for Polaris resources.\nBob: A data engineer who uses Apache Spark™ to interact with Polaris.\nAlice has created a service principal for Bob. It has been granted the Data_engineer principal role, which in turn has been granted the following catalog roles: Catalog contributor and Data administrator (for both the Silver and Gold zone catalogs in the following diagram).\nThe Catalog contributor role grants permission to create namespaces and tables in the Bronze zone catalog.\nThe Data administrator roles grant full administrative rights to the Silver zone catalog and Gold zone catalog.\nMark: A data scientist who uses trains models with data managed by Polaris.\nAlice has created a service principal for Mark. It has been granted the Data_scientist principal role, which in turn has been granted the catalog role named Catalog reader.\nThe Catalog reader role grants read-only access for a catalog named Gold zone catalog.\n","categories":"","description":"","excerpt":"This section provides information about how access control works for …","ref":"/releases/1.0.0/access-control/","tags":"","title":"Access Control"},{"body":"This section provides information about how access control works for Apache Polaris (Incubating).\nPolaris uses a role-based access control (RBAC) model in which the Polaris administrator assigns access privileges to catalog roles and then grants access to resources to service principals by assigning catalog roles to principal roles.\nThese are the key concepts to understanding access control in Polaris:\nSecurable object Principal role Catalog role Privilege Securable object A securable object is an object to which access can be granted. Polaris has the following securable objects:\nCatalog Namespace Iceberg table View Principal role A principal role is a resource in Polaris that you can use to logically group Polaris service principals together and grant privileges on securable objects.\nPolaris supports a many-to-one relationship between service principals and principal roles. For example, to grant the same privileges to multiple service principals, you can grant a single principal role to those service principals. A service principal can be granted one principal role. When registering a service connection, the Polaris administrator specifies the principal role that is granted to the service principal.\nYou don’t grant privileges directly to a principal role. Instead, you configure object permissions at the catalog role level, and then grant catalog roles to a principal role.\nThe following table shows examples of principal roles that you might configure in Polaris:\nPrincipal role name Description Data_engineer A role that is granted to multiple service principals for running data engineering jobs. Data_scientist A role that is granted to multiple service principals for running data science or AI jobs. Catalog role A catalog role belongs to a particular catalog resource in Polaris and specifies a set of permissions for actions on the catalog or objects in the catalog, such as catalog namespaces or tables. You can create one or more catalog roles for a catalog.\nYou grant privileges to a catalog role and then grant the catalog role to a principal role to bestow the privileges to one or more service principals.\nNote\nIf you update the privileges bestowed to a service principal, the updates won’t take effect for up to one hour. This means that if you revoke or grant some privileges for a catalog, the updated privileges won’t take effect on any service principal with access to that catalog for up to one hour.\nPolaris also supports a many-to-many relationship between catalog roles and principal roles. You can grant the same catalog role to one or more principal roles. Likewise, a principal role can be granted to one or more catalog roles.\nThe following table displays examples of catalog roles that you might configure in Polaris:\nExample Catalog role Description Catalog administrators A role that has been granted multiple privileges to emulate full access to the catalog. Principal roles that have been granted this role are permitted to create, alter, read, write, and drop tables in the catalog. Catalog readers A role that has been granted read-only privileges to tables in the catalog. Principal roles that have been granted this role are allowed to read from tables in the catalog. Catalog contributor A role that has been granted read and write access privileges to all tables that belong to the catalog. Principal roles that have been granted this role are allowed to perform read and write operations on tables in the catalog. RBAC model The following diagram illustrates the RBAC model used by Polaris. For each catalog, the Polaris administrator assigns access privileges to catalog roles and then grants service principals access to resources by assigning catalog roles to principal roles. Polaris supports a many-to-one relationship between service principals and principal roles.\nAccess control privileges This section describes the privileges that are available in the Polaris access control model. Privileges are granted to catalog roles, catalog roles are granted to principal roles, and principal roles are granted to service principals to specify the operations that service principals can perform on objects in Polaris.\nImportant\nYou can only grant privileges at the catalog level. Fine-grained access controls are not available. For example, you can grant read privileges to all tables in a catalog but not to an individual table in the catalog.\nTo grant the full set of privileges (drop, list, read, write, etc.) on an object, you can use the full privilege option.\nTable privileges Privilege Description TABLE_CREATE Enables registering a table with the catalog. TABLE_DROP Enables dropping a table from the catalog. TABLE_LIST Enables listing any table in the catalog. TABLE_READ_PROPERTIES Enables reading properties of the table. TABLE_WRITE_PROPERTIES Enables configuring properties for the table. TABLE_READ_DATA Enables reading data from the table by receiving short-lived read-only storage credentials from the catalog. TABLE_WRITE_DATA Enables writing data to the table by receiving short-lived read+write storage credentials from the catalog. TABLE_FULL_METADATA Grants all table privileges, except TABLE_READ_DATA and TABLE_WRITE_DATA, which need to be granted individually. TABLE_ATTACH_POLICY Enables attaching policy to a table. TABLE_DETACH_POLICY Enables detaching policy from a table. View privileges Privilege Description VIEW_CREATE Enables registering a view with the catalog. VIEW_DROP Enables dropping a view from the catalog. VIEW_LIST Enables listing any views in the catalog. VIEW_READ_PROPERTIES Enables reading all the view properties. VIEW_WRITE_PROPERTIES Enables configuring view properties. VIEW_FULL_METADATA Grants all view privileges. Namespace privileges Privilege Description NAMESPACE_CREATE Enables creating a namespace in a catalog. NAMESPACE_DROP Enables dropping the namespace from the catalog. NAMESPACE_LIST Enables listing any object in the namespace, including nested namespaces and tables. NAMESPACE_READ_PROPERTIES Enables reading all the namespace properties. NAMESPACE_WRITE_PROPERTIES Enables configuring namespace properties. NAMESPACE_FULL_METADATA Grants all namespace privileges. NAMESPACE_ATTACH_POLICY Enables attaching policy to a namespace. NAMESPACE_DETACH_POLICY Enables detaching policy from a namespace. Catalog privileges Privilege Description CATALOG_MANAGE_ACCESS Includes the ability to grant or revoke privileges on objects in a catalog to catalog roles, and the ability to grant or revoke catalog roles to or from principal roles. CATALOG_MANAGE_CONTENT Enables full management of content for the catalog. This privilege encompasses the following privileges:CATALOG_MANAGE_METADATATABLE_FULL_METADATANAMESPACE_FULL_METADATAVIEW_FULL_METADATATABLE_WRITE_DATATABLE_READ_DATACATALOG_READ_PROPERTIESCATALOG_WRITE_PROPERTIES CATALOG_MANAGE_METADATA Enables full management of the catalog, catalog roles, namespaces, and tables. CATALOG_READ_PROPERTIES Enables listing catalogs and reading properties of the catalog. CATALOG_WRITE_PROPERTIES Enables configuring catalog properties. CATALOG_ATTACH_POLICY Enables attaching policy to a catalog. CATALOG_DETACH_POLICY Enables detaching policy from a catalog. Policy privileges Privilege Description POLICY_CREATE Enables creating a policy under specified namespace. POLICY_READ Enables reading policy content and metadata. POLICY_WRITE Enables updating the policy details such as its content or description. POLICY_LIST Enables listing any policy from the catalog. POLICY_DROP Enables dropping a policy if it is not attached to any resource entity. POLICY_FULL_METADATA Grants all policy privileges. POLICY_ATTACH Enables policy to be attached to entities. POLICY_DETACH Enables policy to be detached from entities. RBAC example The following diagram illustrates how RBAC works in Polaris and includes the following users:\nAlice: A service admin who signs up for Polaris. Alice can create service principals. She can also create catalogs and namespaces and configure access control for Polaris resources.\nBob: A data engineer who uses Apache Spark™ to interact with Polaris.\nAlice has created a service principal for Bob. It has been granted the Data_engineer principal role, which in turn has been granted the following catalog roles: Catalog contributor and Data administrator (for both the Silver and Gold zone catalogs in the following diagram).\nThe Catalog contributor role grants permission to create namespaces and tables in the Bronze zone catalog.\nThe Data administrator roles grant full administrative rights to the Silver zone catalog and Gold zone catalog.\nMark: A data scientist who uses trains models with data managed by Polaris.\nAlice has created a service principal for Mark. It has been granted the Data_scientist principal role, which in turn has been granted the catalog role named Catalog reader.\nThe Catalog reader role grants read-only access for a catalog named Gold zone catalog.\n","categories":"","description":"","excerpt":"This section provides information about how access control works for …","ref":"/releases/1.0.1/access-control/","tags":"","title":"Access Control"},{"body":" Contributing to Apache Polaris Thank you for considering contributing to Apache Polaris. Any contribution (code, test cases, documentation, use cases, …) is valuable!\nThis documentation will help you get started.\nContribute bug reports and feature requests You can report an issue in the Polaris Catalog issue tracker.\nHow to report a bug Note: If you find a security vulnerability, do NOT open an issue. Please email security@apache.org instead.\nWhen filing an issue, make sure to answer these five questions:\nWhat version of Apache Polaris are you using? What operating system and processor architecture are you using? What did you do? What did you expect to see? What did you see instead? Troubleshooting questions should be posted on:\nSlack dev mailing list (you can subscribe) instead of the issue tracker. Maintainers and community members will answer your questions there or ask you to file an issue if you’ve encountered a bug.\nHow to suggest a feature or enhancement Apache Polaris aims to provide the Apache Iceberg community with new levels of choice, flexibility and control over their data, with full enterprise security and Apache Iceberg interoperability with Amazon Web Services (AWS), Confluent, Dremio, Google Cloud, Microsoft Azure, Salesforce and more.\nIf you’re looking for a feature that doesn’t exist in Apache Polaris, you’re probably not alone. Others likely have similar needs. Please open a GitHub Issue describing the feature you’d like to see, why you need it, and how it should work.\nWhen creating your feature request, document your requirements first. Please, try to not directly describe the solution.\nBefore you begin contributing code Review open issues and discuss your approach If you want to dive into development yourself then you can check out existing open issues or requests for features that need to be implemented. Take ownership of an issue and try fix it.\nBefore starting on a large code change, please describe the concept/design of what you plan to do on the issue/feature request you intend to address. If unsure if the design is good or will be accepted, discuss it with the community in the respective issue first, before you do too much active development.\nProvide your changes in a Pull Request The best way to provide changes is to fork Apache Polaris repository on GitHub and provide a Pull Request with your changes. To make it easy to apply your changes please use the following conventions:\nBefore opening a pull request Pull Requests should be based on the main branch. Create a branch that will house your change: git clone https://github.com/apache/polaris cd polaris git checkout main git pull git checkout -b my-branch Work on the changes of your pull requests locally. Recommended checks: # Ensure the code is properly formatted and compiles: ./gradlew format compileAll # Ensure the code is passing the checks (including formatting checks \u0026 tests): ./gradlew check You may want to push your changes to your personal Polaris fork. Git will emit a URL that you can use to create the Pull Request. Do not create the Pull Request yet. git push --set-upstream your-github-accout Opening a Pull Request The Pull Request summary should provide a concise summary of the change, get inspired by Conventional Commits. The Pull Request description should provide the background (rationale) of the change and describe the changes in way that someone who has no prior knowledge can understand the rationale of the change and the change itself. If there is a matching GitHub Issue, add a separate line at the end of the commit message of the issue that the PR fixes. Do not add the issue number into the subject. Fixes #123456 If the PR does not fully fix the issue, use Related to #123456 instead of Fixes #123456. Tips:\nIf the branch for your Pull Request contains only one (squashed) commit, GitHub will populate the PR summary and description from that single commit. When opening a PR consider whether the PR is “draft” or already “ready for review”. “Draft” means work in progress, things will change, but comments are welcome. “Ready for review” means that the PR is requested to be merged as is (pending review feedback). Working on a Pull Request Don’t forget to periodically rebase your branch: git pull --rebase git push your-github-accout my-branch --force Test that your changes work by adapting or adding tests. Verify the build passes (see README.md for build instructions). If your Pull Request has conflicts with the main branch, please rebase and fix the conflicts. If your PR requires more work or time or bigger changes, please put the PR to “draft” state to indicate that it is not meant to be “thoroughly” reviewed at this point. Merging a Pull Request When a PR is about to be merged, cross-check the commit summary and message for the merged Git commit. Keep in mind that the Git commit subject and message is going to be read by other people, potentially even after years. The Git commit subject and message will appear “as is” in release notes. Make sure the subject and message are properly formatted and contains a concise description of the changes in way that someone who has no prior knowledge can understand the rationale of the change and the change itself. Remove information that’s of no use for someone reading the Git commit log, for example single intermediate commit messages like formatting or fix test. Java version requirements The Apache Polaris build currently requires Java 21 or later. There are a few tools that help you running the right Java version:\nSDKMAN! follow the installation instructions, then run sdk list java to see the available distributions and versions, then run sdk install java \u003cidentifer from list\u003e using the identifier for the distribution and version (\u003e= 21) of your choice. jenv If on a Mac you can use jenv to set the appropriate SDK. Good Practices Change of public interface (or more generally speaking Polaris extension point) should be discussed and approved on the dev mailing list. The discussion on the dev mailing list should happen before having a “ready-for-review” Pull Request. git log can help you find the original/relevant authors of the code you are modifying. If you need, feel free to tag the author in your Pull Request comment if you need assistance or review. Do not re-create a pull-request for the same change. Use one Pull Request related to the same change(s). The purpose here is to keep the history and all comments in the Pull Request. Consider open questions and concerns in all comments of your Pull Request, provide replies and resolve addressed comments, if those don’t serve reference purposes. If a comment doesn’t contain nit, minor, or not a blocker mention, please provide feedback to the comment before merging. Give time for review. For instance two working days is a good base to get first reviews and comments. If you have the feeling that the discussions in a Pull Request are not going to a consensus, feel free to bring the discussion on the dev mailing list. ","categories":"","description":"","excerpt":" Contributing to Apache Polaris Thank you for considering contributing …","ref":"/community/contributing-guidelines/","tags":"","title":""},{"body":" Release Guide This guide walks you through the release process of the Apache Polaris podling.\nSetup To create a release candidate, you will need:\nyour Apache credentials (for repository.apache.org and dist.apache.org repositories) a GPG key for signing artifacts, published in KEYS file If you haven’t published your GPG key yet, you must publish it before starting the release process:\nsvn co https://dist.apache.org/repos/dist/release/incubator/polaris polaris-dist-release cd polaris-dist-release echo \"\" \u003e\u003e KEYS # append a new line gpg --list-sigs \u003cYOUR KEY ID HERE\u003e \u003e\u003e KEYS # append signatures gpg --armor --export \u003cYOUR KEY ID HERE\u003e \u003e\u003e KEYS # append public key block svn commit -m \"add key for \u003cYOUR NAME HERE\u003e\" To send the key to the Ubuntu key-server, Apache Nexus needs it to validate published artifacts:\ngpg --keyserver hkps://keyserver.ubuntu.com --send-keys \u003cYOUR KEY ID HERE\u003e Dist repository The Apache dist repository (dist.apache.org) is used to populate download.apache.org and archives.apache.org. There are two spaces on dist:\ndev is where you stage the source distribution and other user convenient artifacts (distributions, helm charts, …) release is where the artifacts will be copied when the release vote passed Apache dist is a svn repository, you need your Apache credentials to commit there.\nMaven repository Apache uses Nexus as Maven repository (repository.apache.org) where releases are staged (during vote) and copied to Maven Central when the release vote passed.\nYou have to use Apache credentials on Nexus, configured in ~/.gradle/gradle.properties file using mavenUser and mavenPassword:\napacheUsername=yourApacheId apachePassword=yourPassword Note: an alternative is to use ORG_GRADLE_PROJECT_apacheUsername and ORG_GRADLE_PROJECT_apachePassword environment variables:\nexport ORG_GRADLE_PROJECT_apacheUsername=yourApacheId export ORG_GRADLE_PROJECT_apachePassword=bar PGP signing During release process, the artifacts will be signed with your key, eventually using gpg-agent.\nTo configure gradle to sign the artifacts, you can add the following settings in your ~/.gradle/gradle.properties file:\nsigning.gnupg.keyName=Your Key Name To use gpg instead of gpg2, also set signing.gnupg.executable=gpg.\nFor more information, see the Gradle signing documentation.\nGitHub Repository The release should be executed against https://github.com/apache/polaris.git repository (not a fork). Set it as remote with name apache for release if it’s not already set up.\nCreating a release candidate Initiate a discussion about the release with the community This step can be useful to gather ongoing patches that the community thinks should be in the upcoming release.\nThe communication can be started via a [DISCUSS] mail on the dev@polaris.apache.org mailing list and the desired tickets can be added to the github milestone of the next release.\nNote, creating a milestone in github requires a committer. However, a non-committer can assign tasks to a milestone if added to the list of collaborators in .asf.yaml.\nCreate release branch If it’s the first RC for the release, you have to create a release branch:\ngit branch release/x.y.z git push apache release/x.y.z Go in the branch, and set the target release version:\ngit checkoout release/x.y.z echo \"x.y.z\" \u003e version.txt git commit -a git push Update CHANGELOG.md:\n./gradlew patchChangelog git commit -a git push Note: You should submit a PR to propagate (automated) CHANGELOG updates from the release branch to main.\nIf more changes are cherry-picked for the next RC, and those change introduce CHANGELOG entries, follow this update process:\nManually add an -rcN suffix to the previously generated versioned CHANGELOG section. Rerun the patchChangelog command Manually remove RC sections from the CHANGELOG Submit a PR to propagate CHANGELOG updates from the release branch to main. Note: the CHANGELOG patch commit should probably be the last commit on the release branch when an RC is cut. If more changes are cherry-picked for the next RC, it is best to drop the CHANGELOG patch commit, apply cherry-picks, and re-run patchChangelog.\nNote: You should also submit a PR on main branch to bump the version in the version.txt file.\nCreate release tag On the release branch, you create a tag for the RC:\ngit tag apache-polaris-x.y.z-rci git push apache apache-polaris-x.y.z-rci Switch to the tag:\ngit checkout apache-polaris-x.y.z.rci Verify the build pass This is an optional step, but good to do. The purpose here is to verify the build works fine:\n./gradlew clean build It’s also welcome to verify the regression tests (see regtests/README.md for details).\nBuild and stage the distributions You can now build the source distribution:\n./gradlew build sourceTarball -Prelease -PuseGpgAgent -x test -x intTest The source distribution archives are available in build/distribution folder.\nThe binary distributions (for convenience) are available in:\nruntime/distribution/build/distributions Now, we can stage the artifacts to dist dev repository:\nsvn co https://dist.apache.org/repos/dist/dev/incubator/polaris polaris-dist-dev cd polaris-dist-dev mkdir x.y.z cp /path/to/polaris/github/clone/repo/build/distribution/* x.y.z cp /path/to/polaris/github/clone/repo/runtime/distribution/build/distributions/* x.y.z cp -r /path/to/polaris/github/clone/repo/helm/polaris helm-chart/x.y.z svn add x.y.z svn add helm-chart/x.y.z svn commit -m\"Stage Apache Polaris x.y.z RCx\" Build and stage Maven artifacts You can now build and publish the Maven artifacts on a Nexus staging repository:\n./gradlew publishToApache -Prelease -PuseGpgAgent Next, you have to close the staging repository:\nGo to Nexus and log in In the left menu, click on “Staging Repositories” Select the Polaris repository At the top, select “Close” and follow the instructions In the comment field, use “Apache Polaris x.y.z RCi” Build and staging Docker images You can now publish Docker images on DockerHub:\n./gradlew :polaris-server:assemble :polaris-server:quarkusAppPartsBuild --rerun -Dquarkus.container-image.build=true -Dquarkus.container-image.push=true -Dquarkus.docker.buildx.platform=\"linux/amd64,linux/arm64\" -Dquarkus.container-image.tag=x.y.z-rci ./gradlew :polaris-admin:assemble :polaris-admin:quarkusAppPartsBuild --rerun -Dquarkus.container-image.build=true -Dquarkus.container-image.push=true -Dquarkus.docker.buildx.platform=\"linux/amd64,linux/arm64\" -Dquarkus.container-image.tag=x.y.z-rci Start the vote thread The last step for a release candidate is to create a VOTE thread on the dev mailing list.\nA generated email template is available in the build/distribution folder.\nExample title subject:\n[VOTE] Release Apache Polaris x.y.z (rci) Example content:\nHi everyone, I propose that we release the following RC as the official Apache Polaris x.y.z release. * This corresponds to the tag: apache-polaris-x.y.z-rci * https://github.com/apache/polaris/commits/apache-polaris-x.y.z-rci * https://github.com/apache/polaris/tree/\u003cSHA1\u003e The release tarball, signature, and checksums are here: * https://dist.apache.org/repos/dist/dev/incubator/polaris/x.y.z Helm charts are available on: * https://dist.apache.org/repos/dist/dev/incubator/polaris/helm-chart Docker images: * https://hub.docker.com/r/apache/polaris/tags (x.y.z-rci) * https://hub.docker.com/r/apache/polaris-admin-tool/tags (x.y.z-rci) You can find the KEYS file here: * https://downloads.apache.org/incubator/polaris/KEYS Convenience binary artifacts are staged on Nexus. The Maven repositories URLs are: * https://repository.apache.org/content/repositories/orgapachepolaris-\u003cID\u003e/ Please download, verify, and test. Please vote in the next 72 hours. [ ] +1 Release this as Apache polaris x.y.z [ ] +0 [ ] -1 Do not release this because... Only PPMC members and mentors have binding votes, but other community members are encouraged to cast non-binding votes. This vote will pass if there are 3 binding +1 votes and more binding +1 votes than -1 votes. NB: if this vote passes, a new vote has to be started on the Incubator general mailing list. When a candidate is passed or rejected, reply with the vote result:\n[RESULT][VOTE] Release Apache Polaris x.y.z (rci) Thanks everyone who participated in the vote for Release Apache Polaris x.y.z (rci). The vote result is: +1: a (binding), b (non-binding) +0: c (binding), d (non-binding) -1: e (binding), f (non-binding) A new vote is starting in the Apache Incubator general mailing list. Start a new vote on the Incubator general mailing list As Polaris is an Apache Incubator project, you now have to start a new vote on the Apache Incubator general mailing list.\nYou have to send this email to general@incubator.apache.org:\n[VOTE] Release Apache Polaris x.y.z (rci) Hello everyone, The Apache Polaris community has voted and approved the release of Apache Polaris x.y.z (rci). We now kindly request the IPMC members review and vote for this release. Polaris community vote thread: * https://lists.apache.org/thread/\u003cVOTE THREAD\u003e Vote result thread: * https://lists.apache.org/thread/\u003cVOTE RESULT\u003e The release candidate: * https://dist.apache.org/repos/dist/dev/incubator/polaris/x.y.z Git tag for the release: * https://github.com/apache/polaris/releases/tag/apache-polaris-x.y.z-rci Git commit for the release: * https://github.com/apache/polaris/commit/\u003cCOMMIT\u003e Maven staging repository: * https://repository.apache.org/content/repositories/orgapachepolaris-\u003cID\u003e/ Please download, verify and test. Please vote in the next 72 hours. [ ] +1 approve [ ] +0 no opinion [ ] -1 disapprove with the reason To learn more about apache Polaris, please see https://polaris.apache.org/ Checklist for reference: [ ] Download links are valid. [ ] Checksums and signatures. [ ] LICENSE/NOTICE files exist [ ] No unexpected binary files [ ] All source files have ASF headers [ ] Can compile from source Binding votes are the votes from the IPMC members. Similar to the previous vote, send the result on the Incubator general mailing list:\n[RESULT][VOTE] Release Apache Polaris x.y.z (rci) Hi everyone, This vote passed with the following result: Finishing the release After the release votes passed, you need to release the last candidate’s artifacts.\nPublishing the release First, copy the distribution from the dist dev space to the dist release space:\nsvn mv https://dist.apache.org/repos/dist/dev/incubator/polaris/x.y.z https://dist.apache.org/repos/dist/release/incubator/polaris svn mv https://dist.apache.org/repos/dist/dev/incubator/polaris/helm-chart/x.y.z https://dist.apache.org/repos/dist/release/incubator/polaris/helm-chart Next, add a release tag to the git repository based on the candidate tag:\ngit tag -a apache-polaris-x.y.z apache-polaris-x.y.z-rci Update GitHub with the release: https://github.com/apache/polaris/releases/tag/apache-polaris-x.y.z\nThen release the candidate repository on Nexus.\nPublishing docs Open a PR against branch versioned-docs to publish the documentation Open a PR against the main branch to update website Add download links and release notes in Download page Add the release in the website menu Announcing the release To announce the release, wait until Maven Central has mirrored the artifacts.\nSend a mail to dev@iceberg.apache.org and announce@apache.org:\n[ANNOUNCE] Apache Polaris x.y.z The Apache Polaris team is pleased to announce Apache Polaris x.y.z. \u003cAdd Quick Description of the Release\u003e This release can be downloaded https://www.apache.org/dyn/closer.cgi/incubator/polaris/apache-polaris-x.y.z. Release notes: https://polaris.apache.org/blog/apache-polaris-x.y.z Artifacts are available on Maven Central. Apache Polaris is an open-source, fully-featured catalog for Apache Iceberg™. It implements Iceberg's REST API, enabling seamless multi-engine interoperability across a wide range of platforms, including Apache Doris™, Apache Flink®, Apache Spark™, Dremio®, StarRocks, and Trino. Enjoy ! How to verify a release Validating distributions Release vote email includes links to:\nDistribution archives (source, admin, server) on dist.apache.org Signature files (.asc) Checksum files (.sha512) KEYS file After downloading the distributions archives, signatures, checksums, and KEYS file, here are the instructions on how to verify signatures, checksums.\nVerifying signatures First, import the keys in your local keyring:\ncurl https://downloads.apache.org/incubator/polaris/KEYS -o KEYS gpg --import KEYS Next, verify all .asc files:\ngpg --verify apache-polaris-[...].asc Verifying checksums shasum -a 512 --check apache-polaris-[...].sha512 Verifying build and test In the source distribution:\n./gradlew build Voting Votes are cast by replying on the vote email on the dev mailing list, with either +1, 0, -1.\nIn addition to your vote, it’s customary to specify if your vote is binding or non-binding. Only members of the PPMC and mentors have formally binding votes, and IPMC on the vote on the Incubator general mailing list. If you’re unsure, you can specify that your vote is non-binding. You can find more details on https://www.apache.org/foundation/voting.html.\n","categories":"","description":"","excerpt":" Release Guide This guide walks you through the release process of the …","ref":"/community/release-guide/","tags":"","title":""},{"body":"Overview This page provides information on how to configure Apache Polaris (Incubating). Unless stated otherwise, this information is valid both for Polaris Docker images (and Kubernetes deployments) as well as for Polaris binary distributions.\n[!NOTE] For Production tips and best practices, refer to Configuring Polaris for Production.\nFirst off, Polaris server runs on Quarkus, and uses its configuration mechanisms. Read Quarkus configuration guide to get familiar with the basics.\nQuarkus aggregates configuration properties from multiple sources, applying them in a specific order of precedence. When a property is defined in multiple sources, the value from the source with the higher priority overrides those from lower-priority sources.\nThe sources are listed below, from highest to lowest priority:\nSystem properties: properties set via the Java command line using -Dproperty.name=value. Environment variables (see below for important details). Settings in $PWD/config/application.properties file. The application.properties files packaged in Polaris. Default values: hardcoded defaults within the application. When using environment variables, there are two naming conventions:\nIf possible, just use the property name as the environment variable name. This works fine in most cases, e.g. in Kubernetes deployments. For example, polaris.realm-context.realms can be included as is in a container YAML definition:\nenv: - name: \"polaris.realm-context.realms\" value: \"realm1,realm2\" If running from a script or shell prompt, however, stricter naming rules apply: variable names can consist solely of uppercase letters, digits, and the _ (underscore) sign. In such situations, the environment variable name must be derived from the property name, by using uppercase letters, and replacing all dots, dashes and quotes by underscores. For example, polaris.realm-context.realms becomes POLARIS_REALM_CONTEXT_REALMS. See here for more details.\n[!IMPORTANT] While convenient, uppercase-only environment variables can be problematic for complex property names. In these situations, it’s preferable to use system properties or a configuration file.\nAs stated above, a configuration file can also be provided at runtime; it should be available (mounted) at $PWD/config/application.properties for Polaris server to recognize it. In Polaris official Docker images, this location is /deployment/config/application.properties.\nFor Kubernetes deployments, the configuration file is typically defined as a ConfigMap, then mounted in the container at /deployment/config/application.properties. It can be mounted in read-only mode, as Polaris only reads the configuration file once, at startup.\nPolaris Configuration Options Reference Configuration Property Default Value Description polaris.persistence.type relational-jdbc Define the persistence backend used by Polaris (in-memory, relational-jdbc, eclipse-link (deprecated)). See [Configuring Apache Polaris for Production)[https://polaris.apache.org/in-dev/unreleased/configuring-polaris-for-production/) polaris.persistence.relational.jdbc.max-retries 1 Total number of retries JDBC persistence will attempt on connection resets or serialization failures before giving up. polaris.persistence.relational.jdbc.max_duaration_in_ms 5000 ms Max time interval (ms) since the start of a transaction when retries can be attempted. polaris.persistence.relational.jdbc.initial_delay_in_ms 100 ms Initial delay before retrying. The delay is doubled after each retry. polaris.persistence.eclipselink.configurationFile Define the location of the persistence.xml. By default, it’s the built-in persistence.xml in use. polaris.persistence.eclipselink.persistenceUnit polaris Define the name of the persistence unit to use, as defined in the persistence.xml. polaris.realm-context.type default Define the type of the Polaris realm to use. polaris.realm-context.realms POLARIS Define the list of realms to use. polaris.realm-context.header-name Polaris-Realm Define the header name defining the realm context. polaris.features.\"ENFORCE_PRINCIPAL_CREDENTIAL_ROTATION_REQUIRED_CHECKING\" false Flag to enforce check if credential rotation. polaris.features.\"SUPPORTED_CATALOG_STORAGE_TYPES\" FILE Define the catalog supported storage. Supported values are S3, GCS, AZURE, FILE. polaris.features.realm-overrides.\"my-realm\".\"SKIP_CREDENTIAL_SUBSCOPING_INDIRECTION\" true “Override” realm features, here the skip credential subscoping indirection flag. polaris.authentication.authenticator.type default Define the Polaris authenticator type. polaris.authentication.token-service.type default Define the Polaris token service type. polaris.authentication.token-broker.type rsa-key-pair Define the Polaris token broker type. Also configure the location of the key files. For RSA: if the locations of the key files are not configured, an ephemeral key-pair will be created on each Polaris server instance startup, which breaks existing tokens after server restarts and is also incompatible with running multiple Polaris server instances. polaris.authentication.token-broker.max-token-generation PT1H Define the max token generation policy on the token broker. polaris.authentication.token-broker.rsa-key-pair.private-key-file Define the location of the RSA-256 private key file, if present the public-key file must be specified, too. polaris.authentication.token-broker.rsa-key-pair.public-key-file Define the location of the RSA-256 public key file, if present the private-key file must be specified, too. polaris.authentication.token-broker.symmetric-key.secret secret Define the secret of the symmetric key. polaris.authentication.token-broker.symmetric-key.file /tmp/symmetric.key Define the location of the symmetric key file. polaris.storage.aws.access-key accessKey Define the AWS S3 access key. If unset, the default credential provider chain will be used. polaris.storage.aws.secret-key secretKey Define the AWS S3 secret key. If unset, the default credential provider chain will be used. polaris.storage.gcp.token token Define the Google Cloud Storage token. If unset, the default credential provider chain will be used. polaris.storage.gcp.lifespan PT1H Define the Google Cloud Storage lifespan type. If unset, the default credential provider chain will be used. polaris.log.request-id-header-name Polaris-Request-Id Define the header name to match request ID in the log. polaris.log.mdc.aid polaris Define the log context (e.g. MDC) AID. polaris.log.mdc.sid polaris-service Define the log context (e.g. MDC) SID. polaris.rate-limiter.filter.type no-op Define the Polaris rate limiter. Supported values are no-op, token-bucket. polaris.rate-limiter.token-bucket.type default Define the token bucket rate limiter. polaris.rate-limiter.token-bucket.requests-per-second 9999 Define the number of requests per second for the token bucket rate limiter. polaris.rate-limiter.token-bucket.window PT10S Define the window type for the token bucket rate limiter. polaris.metrics.tags.\u003ctag-name\u003e=\u003ctag-value\u003e application=Polaris Define arbitrary metric tags to include in every request. polaris.metrics.realm-id-tag.api-metrics-enabled false Whether to enable the realm_id metric tag in API metrics. polaris.metrics.realm-id-tag.http-metrics-enabled false Whether to enable the realm_id metric tag in HTTP request metrics. polaris.metrics.realm-id-tag.http-metrics-max-cardinality 100 The maximum cardinality for the realm_id tag in HTTP request metrics. polaris.tasks.max-concurrent-tasks 100 Define the max number of concurrent tasks. polaris.tasks.max-queued-tasks 1000 Define the max number of tasks in queue. polaris.config.rollback.compaction.on-conflicts.enabled false When set to true Polaris will apply the deconfliction by rollbacking those REPLACE operations snapshots which have the property of polaris.internal.rollback.compaction.on-conflict in their snapshot summary set to rollback, to resolve conflicts at the server end. There are non Polaris configuration properties that can be useful:\nConfiguration Property Default Value Description quarkus.log.level INFO Define the root log level. quarkus.log.category.\"org.apache.polaris\".level Define the log level for a specific category. quarkus.default-locale System locale Force the use of a specific locale, for instance en_US. quarkus.http.port 8181 Define the HTTP port number. quarkus.http.auth.basic false Enable the HTTP basic authentication. quarkus.http.limits.max-body-size 10240K Define the HTTP max body size limit. quarkus.http.cors.origins Define the HTTP CORS origins. quarkus.http.cors.methods PATCH, POST, DELETE, GET, PUT Define the HTTP CORS covered methods. quarkus.http.cors.headers * Define the HTTP CORS covered headers. quarkus.http.cors.exposed-headers * Define the HTTP CORS covered exposed headers. quarkus.http.cors.access-control-max-age PT10M Define the HTTP CORS access control max age. quarkus.http.cors.access-control-allow-credentials true Define the HTTP CORS access control allow credentials flag. quarkus.management.enabled true Enable the management server. quarkus.management.port 8182 Define the port number of the Polaris management server. quarkus.management.root-path Define the root path where /metrics and /health endpoints are based on. quarkus.otel.sdk.disabled true Enable the OpenTelemetry layer. [!NOTE] This section is only relevant for Polaris Docker images and Kubernetes deployments.\nThere are many other actionable environment variables available in the official Polaris Docker image; they come from the base image used by Polaris, ubi9/openjdk-21-runtime. They should be used to fine-tune the Java runtime directly, e.g. to enable debugging or to set the heap size. These variables are not specific to Polaris, but are inherited from the base image. If in doubt, leave everything at its default!\nEnvironment variable Description JAVA_OPTS or JAVA_OPTIONS NOT RECOMMENDED. JVM options passed to the java command (example: “-verbose:class”). Setting this variable will override all options set by any of the other variables in this table. To pass extra settings, use JAVA_OPTS_APPEND instead. JAVA_OPTS_APPEND User specified Java options to be appended to generated options in JAVA_OPTS (example: “-Dsome.property=foo”). JAVA_TOOL_OPTIONS This variable is defined and honored by all OpenJDK distros, see here. Options defined here take precedence over all else; using this variable is generally not necessary, but can be useful e.g. to enforce JVM startup parameters, to set up remote debug, or to define JVM agents. JAVA_MAX_MEM_RATIO Is used to calculate a default maximal heap memory based on a containers restriction. If used in a container without any memory constraints for the container then this option has no effect. If there is a memory constraint then -XX:MaxRAMPercentage is set to a ratio of the container available memory as set here. The default is 80 which means 80% of the available memory is used as an upper boundary. You can skip this mechanism by setting this value to 0 in which case no -XX:MaxRAMPercentage option is added. JAVA_DEBUG If set remote debugging will be switched on. Disabled by default (example: true\"). JAVA_DEBUG_PORT Port used for remote debugging. Defaults to “5005” (tip: use “*:5005” to enable debugging on all network interfaces). GC_MIN_HEAP_FREE_RATIO Minimum percentage of heap free after GC to avoid expansion. Default is 10. GC_MAX_HEAP_FREE_RATIO Maximum percentage of heap free after GC to avoid shrinking. Default is 20. GC_TIME_RATIO Specifies the ratio of the time spent outside the garbage collection. Default is 4. GC_ADAPTIVE_SIZE_POLICY_WEIGHT The weighting given to the current GC time versus previous GC times. Default is 90. GC_METASPACE_SIZE The initial metaspace size. There is no default (example: “20”). GC_MAX_METASPACE_SIZE The maximum metaspace size. There is no default (example: “100”). GC_CONTAINER_OPTIONS Specify Java GC to use. The value of this variable should contain the necessary JRE command-line options to specify the required GC, which will override the default of -XX:+UseParallelGC (example: -XX:+UseG1GC). Here are some examples: Example docker run option Using another GC -e GC_CONTAINER_OPTIONS=\"-XX:+UseShenandoahGC\" lets Polaris use Shenandoah GC instead of the default parallel GC. Set the Java heap size to a fixed amount -e JAVA_OPTS_APPEND=\"-Xms8g -Xmx8g\" lets Polaris use a Java heap of 8g. Set the maximum heap percentage -e JAVA_MAX_MEM_RATIO=\"70\" lets Polaris use 70% percent of the available memory. Troubleshooting Configuration Issues If you encounter issues with the configuration, you can ask Polaris to print out the configuration it is using. To do this, set the log level for the io.smallrye.config category to DEBUG, and also set the console appender level to DEBUG:\nquarkus.log.console.level=DEBUG quarkus.log.category.\"io.smallrye.config\".level=DEBUG [!IMPORTANT] This will print out all configuration values, including sensitive ones like passwords. Don’t do this in production, and don’t share this output with anyone you don’t trust!\n","categories":"","description":"","excerpt":"Overview This page provides information on how to configure Apache …","ref":"/in-dev/unreleased/configuration/","tags":"","title":"Configuring Polaris"},{"body":"Overview This page provides information on how to configure Apache Polaris (Incubating). Unless stated otherwise, this information is valid both for Polaris Docker images (and Kubernetes deployments) as well as for Polaris binary distributions.\nNote: for Production tips and best practices, refer to Configuring Polaris for Production.\nFirst off, Polaris server runs on Quarkus, and uses its configuration mechanisms. Read Quarkus configuration guide to get familiar with the basics.\nQuarkus aggregates configuration properties from multiple sources, applying them in a specific order of precedence. When a property is defined in multiple sources, the value from the source with the higher priority overrides those from lower-priority sources.\nThe sources are listed below, from highest to lowest priority:\nSystem properties: properties set via the Java command line using -Dproperty.name=value. Environment variables (see below for important details). Settings in $PWD/config/application.properties file. The application.properties files packaged in Polaris. Default values: hardcoded defaults within the application. When using environment variables, there are two naming conventions:\nIf possible, just use the property name as the environment variable name. This works fine in most cases, e.g. in Kubernetes deployments. For example, polaris.realm-context.realms can be included as is in a container YAML definition:\nenv: - name: \"polaris.realm-context.realms\" value: \"realm1,realm2\" If running from a script or shell prompt, however, stricter naming rules apply: variable names can consist solely of uppercase letters, digits, and the _ (underscore) sign. In such situations, the environment variable name must be derived from the property name, by using uppercase letters, and replacing all dots, dashes and quotes by underscores. For example, polaris.realm-context.realms becomes POLARIS_REALM_CONTEXT_REALMS. See here for more details.\n[!IMPORTANT] While convenient, uppercase-only environment variables can be problematic for complex property names. In these situations, it’s preferable to use system properties or a configuration file.\nAs stated above, a configuration file can also be provided at runtime; it should be available (mounted) at $PWD/config/application.properties for Polaris server to recognize it. In Polaris official Docker images, this location is /deployment/config/application.properties.\nFor Kubernetes deployments, the configuration file is typically defined as a ConfigMap, then mounted in the container at /deployment/config/application.properties. It can be mounted in read-only mode, as Polaris only reads the configuration file once, at startup.\nPolaris Configuration Options Reference Configuration Property Default Value Description polaris.persistence.type relational-jdbc Define the persistence backend used by Polaris (in-memory, relational-jdbc, eclipse-link (deprecated)). See [Configuring Apache Polaris for Production)[https://polaris.apache.org/releases/1.0.0/configuring-polaris-for-production/) polaris.persistence.relational.jdbc.max-retries 1 Total number of retries JDBC persistence will attempt on connection resets or serialization failures before giving up. polaris.persistence.relational.jdbc.max_duaration_in_ms 5000 ms Max time interval (ms) since the start of a transaction when retries can be attempted. polaris.persistence.relational.jdbc.initial_delay_in_ms 100 ms Initial delay before retrying. The delay is doubled after each retry. polaris.persistence.eclipselink.configurationFile Define the location of the persistence.xml. By default, it’s the built-in persistence.xml in use. polaris.persistence.eclipselink.persistenceUnit polaris Define the name of the persistence unit to use, as defined in the persistence.xml. polaris.realm-context.type default Define the type of the Polaris realm to use. polaris.realm-context.realms POLARIS Define the list of realms to use. polaris.realm-context.header-name Polaris-Realm Define the header name defining the realm context. polaris.features.\"ENFORCE_PRINCIPAL_CREDENTIAL_ROTATION_REQUIRED_CHECKING\" false Flag to enforce check if credential rotation. polaris.features.\"SUPPORTED_CATALOG_STORAGE_TYPES\" FILE Define the catalog supported storage. Supported values are S3, GCS, AZURE, FILE. polaris.features.realm-overrides.\"my-realm\".\"SKIP_CREDENTIAL_SUBSCOPING_INDIRECTION\" true “Override” realm features, here the skip credential subscoping indirection flag. polaris.authentication.authenticator.type default Define the Polaris authenticator type. polaris.authentication.token-service.type default Define the Polaris token service type. polaris.authentication.token-broker.type rsa-key-pair Define the Polaris token broker type. Also configure the location of the key files. For RSA: if the locations of the key files are not configured, an ephemeral key-pair will be created on each Polaris server instance startup, which breaks existing tokens after server restarts and is also incompatible with running multiple Polaris server instances. polaris.authentication.token-broker.max-token-generation PT1H Define the max token generation policy on the token broker. polaris.authentication.token-broker.rsa-key-pair.private-key-file Define the location of the RSA-256 private key file, if present the public-key file must be specified, too. polaris.authentication.token-broker.rsa-key-pair.public-key-file Define the location of the RSA-256 public key file, if present the private-key file must be specified, too. polaris.authentication.token-broker.symmetric-key.secret secret Define the secret of the symmetric key. polaris.authentication.token-broker.symmetric-key.file /tmp/symmetric.key Define the location of the symmetric key file. polaris.storage.aws.access-key accessKey Define the AWS S3 access key. If unset, the default credential provider chain will be used. polaris.storage.aws.secret-key secretKey Define the AWS S3 secret key. If unset, the default credential provider chain will be used. polaris.storage.gcp.token token Define the Google Cloud Storage token. If unset, the default credential provider chain will be used. polaris.storage.gcp.lifespan PT1H Define the Google Cloud Storage lifespan type. If unset, the default credential provider chain will be used. polaris.log.request-id-header-name Polaris-Request-Id Define the header name to match request ID in the log. polaris.log.mdc.aid polaris Define the log context (e.g. MDC) AID. polaris.log.mdc.sid polaris-service Define the log context (e.g. MDC) SID. polaris.rate-limiter.filter.type no-op Define the Polaris rate limiter. Supported values are no-op, token-bucket. polaris.rate-limiter.token-bucket.type default Define the token bucket rate limiter. polaris.rate-limiter.token-bucket.requests-per-second 9999 Define the number of requests per second for the token bucket rate limiter. polaris.rate-limiter.token-bucket.window PT10S Define the window type for the token bucket rate limiter. polaris.metrics.tags.\u003ctag-name\u003e=\u003ctag-value\u003e application=Polaris Define arbitrary metric tags to include in every request. polaris.metrics.realm-id-tag.api-metrics-enabled false Whether to enable the realm_id metric tag in API metrics. polaris.metrics.realm-id-tag.http-metrics-enabled false Whether to enable the realm_id metric tag in HTTP request metrics. polaris.metrics.realm-id-tag.http-metrics-max-cardinality 100 The maximum cardinality for the realm_id tag in HTTP request metrics. polaris.tasks.max-concurrent-tasks 100 Define the max number of concurrent tasks. polaris.tasks.max-queued-tasks 1000 Define the max number of tasks in queue. There are non Polaris configuration properties that can be useful:\nConfiguration Property Default Value Description quarkus.log.level INFO Define the root log level. quarkus.log.category.\"org.apache.polaris\".level Define the log level for a specific category. quarkus.default-locale System locale Force the use of a specific locale, for instance en_US. quarkus.http.port 8181 Define the HTTP port number. quarkus.http.auth.basic false Enable the HTTP basic authentication. quarkus.http.limits.max-body-size 10240K Define the HTTP max body size limit. quarkus.http.cors.origins Define the HTTP CORS origins. quarkus.http.cors.methods PATCH, POST, DELETE, GET, PUT Define the HTTP CORS covered methods. quarkus.http.cors.headers * Define the HTTP CORS covered headers. quarkus.http.cors.exposed-headers * Define the HTTP CORS covered exposed headers. quarkus.http.cors.access-control-max-age PT10M Define the HTTP CORS access control max age. quarkus.http.cors.access-control-allow-credentials true Define the HTTP CORS access control allow credentials flag. quarkus.management.enabled true Enable the management server. quarkus.management.port 8182 Define the port number of the Polaris management server. quarkus.management.root-path Define the root path where /metrics and /health endpoints are based on. quarkus.otel.sdk.disabled true Enable the OpenTelemetry layer. Note: This section is only relevant for Polaris Docker images and Kubernetes deployments.\nThere are many other actionable environment variables available in the official Polaris Docker image; they come from the base image used by Polaris, ubi9/openjdk-21-runtime. They should be used to fine-tune the Java runtime directly, e.g. to enable debugging or to set the heap size. These variables are not specific to Polaris, but are inherited from the base image. If in doubt, leave everything at its default!\nEnvironment variable Description JAVA_OPTS or JAVA_OPTIONS NOT RECOMMENDED. JVM options passed to the java command (example: “-verbose:class”). Setting this variable will override all options set by any of the other variables in this table. To pass extra settings, use JAVA_OPTS_APPEND instead. JAVA_OPTS_APPEND User specified Java options to be appended to generated options in JAVA_OPTS (example: “-Dsome.property=foo”). JAVA_TOOL_OPTIONS This variable is defined and honored by all OpenJDK distros, see here. Options defined here take precedence over all else; using this variable is generally not necessary, but can be useful e.g. to enforce JVM startup parameters, to set up remote debug, or to define JVM agents. JAVA_MAX_MEM_RATIO Is used to calculate a default maximal heap memory based on a containers restriction. If used in a container without any memory constraints for the container then this option has no effect. If there is a memory constraint then -XX:MaxRAMPercentage is set to a ratio of the container available memory as set here. The default is 80 which means 80% of the available memory is used as an upper boundary. You can skip this mechanism by setting this value to 0 in which case no -XX:MaxRAMPercentage option is added. JAVA_DEBUG If set remote debugging will be switched on. Disabled by default (example: true\"). JAVA_DEBUG_PORT Port used for remote debugging. Defaults to “5005” (tip: use “*:5005” to enable debugging on all network interfaces). GC_MIN_HEAP_FREE_RATIO Minimum percentage of heap free after GC to avoid expansion. Default is 10. GC_MAX_HEAP_FREE_RATIO Maximum percentage of heap free after GC to avoid shrinking. Default is 20. GC_TIME_RATIO Specifies the ratio of the time spent outside the garbage collection. Default is 4. GC_ADAPTIVE_SIZE_POLICY_WEIGHT The weighting given to the current GC time versus previous GC times. Default is 90. GC_METASPACE_SIZE The initial metaspace size. There is no default (example: “20”). GC_MAX_METASPACE_SIZE The maximum metaspace size. There is no default (example: “100”). GC_CONTAINER_OPTIONS Specify Java GC to use. The value of this variable should contain the necessary JRE command-line options to specify the required GC, which will override the default of -XX:+UseParallelGC (example: -XX:+UseG1GC). Here are some examples: Example docker run option Using another GC -e GC_CONTAINER_OPTIONS=\"-XX:+UseShenandoahGC\" lets Polaris use Shenandoah GC instead of the default parallel GC. Set the Java heap size to a fixed amount -e JAVA_OPTS_APPEND=\"-Xms8g -Xmx8g\" lets Polaris use a Java heap of 8g. Set the maximum heap percentage -e JAVA_MAX_MEM_RATIO=\"70\" lets Polaris use 70% percent of the available memory. Troubleshooting Configuration Issues If you encounter issues with the configuration, you can ask Polaris to print out the configuration it is using. To do this, set the log level for the io.smallrye.config category to DEBUG, and also set the console appender level to DEBUG:\nquarkus.log.console.level=DEBUG quarkus.log.category.\"io.smallrye.config\".level=DEBUG [!IMPORTANT] This will print out all configuration values, including sensitive ones like passwords. Don’t do this in production, and don’t share this output with anyone you don’t trust!\n","categories":"","description":"","excerpt":"Overview This page provides information on how to configure Apache …","ref":"/releases/1.0.0/configuration/","tags":"","title":"Configuring Polaris"},{"body":"Overview This page provides information on how to configure Apache Polaris (Incubating). Unless stated otherwise, this information is valid both for Polaris Docker images (and Kubernetes deployments) as well as for Polaris binary distributions.\nNote: for Production tips and best practices, refer to Configuring Polaris for Production.\nFirst off, Polaris server runs on Quarkus, and uses its configuration mechanisms. Read Quarkus configuration guide to get familiar with the basics.\nQuarkus aggregates configuration properties from multiple sources, applying them in a specific order of precedence. When a property is defined in multiple sources, the value from the source with the higher priority overrides those from lower-priority sources.\nThe sources are listed below, from highest to lowest priority:\nSystem properties: properties set via the Java command line using -Dproperty.name=value. Environment variables (see below for important details). Settings in $PWD/config/application.properties file. The application.properties files packaged in Polaris. Default values: hardcoded defaults within the application. When using environment variables, there are two naming conventions:\nIf possible, just use the property name as the environment variable name. This works fine in most cases, e.g. in Kubernetes deployments. For example, polaris.realm-context.realms can be included as is in a container YAML definition:\nenv: - name: \"polaris.realm-context.realms\" value: \"realm1,realm2\" If running from a script or shell prompt, however, stricter naming rules apply: variable names can consist solely of uppercase letters, digits, and the _ (underscore) sign. In such situations, the environment variable name must be derived from the property name, by using uppercase letters, and replacing all dots, dashes and quotes by underscores. For example, polaris.realm-context.realms becomes POLARIS_REALM_CONTEXT_REALMS. See here for more details.\n[!IMPORTANT] While convenient, uppercase-only environment variables can be problematic for complex property names. In these situations, it’s preferable to use system properties or a configuration file.\nAs stated above, a configuration file can also be provided at runtime; it should be available (mounted) at $PWD/config/application.properties for Polaris server to recognize it. In Polaris official Docker images, this location is /deployment/config/application.properties.\nFor Kubernetes deployments, the configuration file is typically defined as a ConfigMap, then mounted in the container at /deployment/config/application.properties. It can be mounted in read-only mode, as Polaris only reads the configuration file once, at startup.\nPolaris Configuration Options Reference Configuration Property Default Value Description polaris.persistence.type relational-jdbc Define the persistence backend used by Polaris (in-memory, relational-jdbc, eclipse-link (deprecated)). See [Configuring Apache Polaris for Production)[https://polaris.apache.org/releases/1.0.1/configuring-polaris-for-production/) polaris.persistence.relational.jdbc.max-retries 1 Total number of retries JDBC persistence will attempt on connection resets or serialization failures before giving up. polaris.persistence.relational.jdbc.max_duaration_in_ms 5000 ms Max time interval (ms) since the start of a transaction when retries can be attempted. polaris.persistence.relational.jdbc.initial_delay_in_ms 100 ms Initial delay before retrying. The delay is doubled after each retry. polaris.persistence.eclipselink.configurationFile Define the location of the persistence.xml. By default, it’s the built-in persistence.xml in use. polaris.persistence.eclipselink.persistenceUnit polaris Define the name of the persistence unit to use, as defined in the persistence.xml. polaris.realm-context.type default Define the type of the Polaris realm to use. polaris.realm-context.realms POLARIS Define the list of realms to use. polaris.realm-context.header-name Polaris-Realm Define the header name defining the realm context. polaris.features.\"ENFORCE_PRINCIPAL_CREDENTIAL_ROTATION_REQUIRED_CHECKING\" false Flag to enforce check if credential rotation. polaris.features.\"SUPPORTED_CATALOG_STORAGE_TYPES\" FILE Define the catalog supported storage. Supported values are S3, GCS, AZURE, FILE. polaris.features.realm-overrides.\"my-realm\".\"SKIP_CREDENTIAL_SUBSCOPING_INDIRECTION\" true “Override” realm features, here the skip credential subscoping indirection flag. polaris.authentication.authenticator.type default Define the Polaris authenticator type. polaris.authentication.token-service.type default Define the Polaris token service type. polaris.authentication.token-broker.type rsa-key-pair Define the Polaris token broker type. Also configure the location of the key files. For RSA: if the locations of the key files are not configured, an ephemeral key-pair will be created on each Polaris server instance startup, which breaks existing tokens after server restarts and is also incompatible with running multiple Polaris server instances. polaris.authentication.token-broker.max-token-generation PT1H Define the max token generation policy on the token broker. polaris.authentication.token-broker.rsa-key-pair.private-key-file Define the location of the RSA-256 private key file, if present the public-key file must be specified, too. polaris.authentication.token-broker.rsa-key-pair.public-key-file Define the location of the RSA-256 public key file, if present the private-key file must be specified, too. polaris.authentication.token-broker.symmetric-key.secret secret Define the secret of the symmetric key. polaris.authentication.token-broker.symmetric-key.file /tmp/symmetric.key Define the location of the symmetric key file. polaris.storage.aws.access-key accessKey Define the AWS S3 access key. If unset, the default credential provider chain will be used. polaris.storage.aws.secret-key secretKey Define the AWS S3 secret key. If unset, the default credential provider chain will be used. polaris.storage.gcp.token token Define the Google Cloud Storage token. If unset, the default credential provider chain will be used. polaris.storage.gcp.lifespan PT1H Define the Google Cloud Storage lifespan type. If unset, the default credential provider chain will be used. polaris.log.request-id-header-name Polaris-Request-Id Define the header name to match request ID in the log. polaris.log.mdc.aid polaris Define the log context (e.g. MDC) AID. polaris.log.mdc.sid polaris-service Define the log context (e.g. MDC) SID. polaris.rate-limiter.filter.type no-op Define the Polaris rate limiter. Supported values are no-op, token-bucket. polaris.rate-limiter.token-bucket.type default Define the token bucket rate limiter. polaris.rate-limiter.token-bucket.requests-per-second 9999 Define the number of requests per second for the token bucket rate limiter. polaris.rate-limiter.token-bucket.window PT10S Define the window type for the token bucket rate limiter. polaris.metrics.tags.\u003ctag-name\u003e=\u003ctag-value\u003e application=Polaris Define arbitrary metric tags to include in every request. polaris.metrics.realm-id-tag.api-metrics-enabled false Whether to enable the realm_id metric tag in API metrics. polaris.metrics.realm-id-tag.http-metrics-enabled false Whether to enable the realm_id metric tag in HTTP request metrics. polaris.metrics.realm-id-tag.http-metrics-max-cardinality 100 The maximum cardinality for the realm_id tag in HTTP request metrics. polaris.tasks.max-concurrent-tasks 100 Define the max number of concurrent tasks. polaris.tasks.max-queued-tasks 1000 Define the max number of tasks in queue. polaris.config.rollback.compaction.on-conflicts.enabled false When set to true Polaris will apply the deconfliction by rollbacking those REPLACE operations snapshots which have the property of polaris.internal.rollback.compaction.on-conflict in their snapshot summary set to rollback, to resolve conflicts at the server end. There are non Polaris configuration properties that can be useful:\nConfiguration Property Default Value Description quarkus.log.level INFO Define the root log level. quarkus.log.category.\"org.apache.polaris\".level Define the log level for a specific category. quarkus.default-locale System locale Force the use of a specific locale, for instance en_US. quarkus.http.port 8181 Define the HTTP port number. quarkus.http.auth.basic false Enable the HTTP basic authentication. quarkus.http.limits.max-body-size 10240K Define the HTTP max body size limit. quarkus.http.cors.origins Define the HTTP CORS origins. quarkus.http.cors.methods PATCH, POST, DELETE, GET, PUT Define the HTTP CORS covered methods. quarkus.http.cors.headers * Define the HTTP CORS covered headers. quarkus.http.cors.exposed-headers * Define the HTTP CORS covered exposed headers. quarkus.http.cors.access-control-max-age PT10M Define the HTTP CORS access control max age. quarkus.http.cors.access-control-allow-credentials true Define the HTTP CORS access control allow credentials flag. quarkus.management.enabled true Enable the management server. quarkus.management.port 8182 Define the port number of the Polaris management server. quarkus.management.root-path Define the root path where /metrics and /health endpoints are based on. quarkus.otel.sdk.disabled true Enable the OpenTelemetry layer. Note: This section is only relevant for Polaris Docker images and Kubernetes deployments.\nThere are many other actionable environment variables available in the official Polaris Docker image; they come from the base image used by Polaris, ubi9/openjdk-21-runtime. They should be used to fine-tune the Java runtime directly, e.g. to enable debugging or to set the heap size. These variables are not specific to Polaris, but are inherited from the base image. If in doubt, leave everything at its default!\nEnvironment variable Description JAVA_OPTS or JAVA_OPTIONS NOT RECOMMENDED. JVM options passed to the java command (example: “-verbose:class”). Setting this variable will override all options set by any of the other variables in this table. To pass extra settings, use JAVA_OPTS_APPEND instead. JAVA_OPTS_APPEND User specified Java options to be appended to generated options in JAVA_OPTS (example: “-Dsome.property=foo”). JAVA_TOOL_OPTIONS This variable is defined and honored by all OpenJDK distros, see here. Options defined here take precedence over all else; using this variable is generally not necessary, but can be useful e.g. to enforce JVM startup parameters, to set up remote debug, or to define JVM agents. JAVA_MAX_MEM_RATIO Is used to calculate a default maximal heap memory based on a containers restriction. If used in a container without any memory constraints for the container then this option has no effect. If there is a memory constraint then -XX:MaxRAMPercentage is set to a ratio of the container available memory as set here. The default is 80 which means 80% of the available memory is used as an upper boundary. You can skip this mechanism by setting this value to 0 in which case no -XX:MaxRAMPercentage option is added. JAVA_DEBUG If set remote debugging will be switched on. Disabled by default (example: true\"). JAVA_DEBUG_PORT Port used for remote debugging. Defaults to “5005” (tip: use “*:5005” to enable debugging on all network interfaces). GC_MIN_HEAP_FREE_RATIO Minimum percentage of heap free after GC to avoid expansion. Default is 10. GC_MAX_HEAP_FREE_RATIO Maximum percentage of heap free after GC to avoid shrinking. Default is 20. GC_TIME_RATIO Specifies the ratio of the time spent outside the garbage collection. Default is 4. GC_ADAPTIVE_SIZE_POLICY_WEIGHT The weighting given to the current GC time versus previous GC times. Default is 90. GC_METASPACE_SIZE The initial metaspace size. There is no default (example: “20”). GC_MAX_METASPACE_SIZE The maximum metaspace size. There is no default (example: “100”). GC_CONTAINER_OPTIONS Specify Java GC to use. The value of this variable should contain the necessary JRE command-line options to specify the required GC, which will override the default of -XX:+UseParallelGC (example: -XX:+UseG1GC). Here are some examples: Example docker run option Using another GC -e GC_CONTAINER_OPTIONS=\"-XX:+UseShenandoahGC\" lets Polaris use Shenandoah GC instead of the default parallel GC. Set the Java heap size to a fixed amount -e JAVA_OPTS_APPEND=\"-Xms8g -Xmx8g\" lets Polaris use a Java heap of 8g. Set the maximum heap percentage -e JAVA_MAX_MEM_RATIO=\"70\" lets Polaris use 70% percent of the available memory. Troubleshooting Configuration Issues If you encounter issues with the configuration, you can ask Polaris to print out the configuration it is using. To do this, set the log level for the io.smallrye.config category to DEBUG, and also set the console appender level to DEBUG:\nquarkus.log.console.level=DEBUG quarkus.log.category.\"io.smallrye.config\".level=DEBUG [!IMPORTANT] This will print out all configuration values, including sensitive ones like passwords. Don’t do this in production, and don’t share this output with anyone you don’t trust!\n","categories":"","description":"","excerpt":"Overview This page provides information on how to configure Apache …","ref":"/releases/1.0.1/configuration/","tags":"","title":"Configuring Polaris"},{"body":"Apache Polaris supports authentication via external identity providers (IdPs) using OpenID Connect (OIDC) in addition to the internal authentication system. This feature enables flexible identity federation with enterprise IdPs and allows gradual migration or hybrid authentication strategies across realms in Polaris.\nAuthentication Types Polaris supports three authentication modes:\ninternal (Default) Only Polaris internal authentication is used. external Authenticates using external OIDC providers (via Quarkus OIDC). Disables the internal token endpoint (returns HTTP 501). mixed Tries internal authentication first; if this fails, it falls back to OIDC. Authentication can be configured globally or per realm by setting the following properties:\n# Global default polaris.authentication.type=internal # Per-realm override polaris.authentication.realm1.type=external polaris.authentication.realm2.type=mixed Key Components Authenticator The Authenticator is a component responsible for resolving the principal and the principal roles, and for creating a PolarisPrincipal from the credentials provided by the authentication process. It is a central component and is invoked for all types of authentication.\nThe type property is used to define the Authenticator implementation. It is overridable per realm:\npolaris.authentication.authenticator.type=default polaris.authentication.realm1.authenticator.type=custom Active Roles Provider The ActiveRolesProvider is a component responsible for determining which roles the principal is requesting and should be activated. It is common to all authentication types.\nOnly the type property is defined; it is used to define the provider implementation. It is overridable per realm:\npolaris.authentication.active-roles-provider.type=default Internal Authentication Configuration Token Broker The TokenBroker signs and verifies tokens to ensure that they can be validated and remain unaltered.\npolaris.authentication.token-broker.type=rsa-key-pair polaris.authentication.token-broker.max-token-generation=PT1H Two types are available:\nrsa-key-pair (recommended for production): Uses an RSA key pair for token signing and validation. symmetric-key: Uses a shared secret for both operations; suitable for single-node deployments or testing. The property polaris.authentication.token-broker.max-token-generation specifies the maximum validity duration of tokens issued by the internal TokenBroker.\nFormat: ISO-8601 duration (e.g., PT1H for 1 hour, PT30M for 30 minutes). Default: PT1H. Token Service The Token Service and TokenServiceConfiguration (Quarkus) is responsible for issuing and validating tokens (e.g., bearer tokens) for authenticated principals when internal authentication is used. It works in coordination with the Authenticator and TokenBroker. The default implementation is default, and this must be configured when using internal authentication.\npolaris.authentication.token-service.type=default Role Mapping When using internal authentication, token requests should include a scope parameter that specifies the roles to be activated for the principal. The scope parameter is a space-separated list of role names.\nThe default ActiveRolesProvider expects role names to be in the following format: PRINCIPAL_ROLE:\u003crole name\u003e.\nFor example, if the principal has the roles service_admin and catalog_admin and wants both activated, the scope parameter should look like this:\nscope=PRINCIPAL_ROLE:service_admin PRINCIPAL_ROLE:catalog_admin Here is an example of a full request to the Polaris token endpoint using internal authentication:\nPOST /api/catalog/v1/oauth/tokens HTTP/1.1 Host: polaris.example.com:8181 Content-Type: application/x-www-form-urlencoded grant_type=client_credentials\u0026client_id=root\u0026client_secret=s3cr3t\u0026scope=PRINCIPAL_ROLE%3Aservice_admin%20PRINCIPAL_ROLE%3Acatalog_admin External Authentication Configuration External authentication is configured via Quarkus OIDC and Polaris-specific OIDC extensions. The following settings are used to integrate with an identity provider and extract identity and role information from tokens.\nOIDC Tenant Configuration At least one OIDC tenant must be explicitly enabled. In Polaris, realms and OIDC tenants are distinct concepts. An OIDC tenant represents a specific identity provider configuration (e.g., quarkus.oidc.idp1). A realm is a logical partition within Polaris.\nMultiple realms can share a single OIDC tenant. Each realm can be associated with only one OIDC tenant. Therefore, multi-realm deployments can share a common identity provider while still enforcing realm-level scoping. To configure the default tenant:\nquarkus.oidc.tenant-enabled=true quarkus.oidc.auth-server-url=https://auth.example.com/realms/polaris quarkus.oidc.client-id=polaris Alternatively, it is possible to use multiple named tenants. Each OIDC-named tenant is then configured with standard Quarkus settings:\nquarkus.oidc.oidc-tenant1.auth-server-url=http://localhost:8080/realms/polaris quarkus.oidc.oidc-tenant1.client-id=client1 quarkus.oidc.oidc-tenant1.application-type=service When using multiple OIDC tenants, it’s your responsibility to configure tenant resolution appropriately. See the Quarkus OpenID Connect Multitenany Guide.\nPrincipal Mapping While OIDC tenant resolution is entirely delegated to Quarkus, Polaris requires additional configuration to extract the Polaris principal and its roles from the credentials generated and validated by Quarkus. This part of the authentication process is configured with Polaris-specific properties that map JWT claims to Polaris principal fields:\npolaris.oidc.principal-mapper.type=default polaris.oidc.principal-mapper.id-claim-path=polaris/principal_id polaris.oidc.principal-mapper.name-claim-path=polaris/principal_name These properties are overridable per OIDC tenant:\npolaris.oidc.oidc-tenant1.principal-mapper.id-claim-path=polaris/principal_id polaris.oidc.oidc-tenant1.principal-mapper.name-claim-path=polaris/principal_name [!IMPORTANT]: The default implementation of PrincipalMapper can only work with JWT tokens. If your IDP issues opaque tokens instead, you will need to provide a custom implementation.\nRole Mapping Similarly, Polaris requires additional configuration to map roles provided by Quarkus to roles defined in Polaris. The process happens in two phases: first, Quarkus maps the JWT claims to security roles, using the quarkus.oidc.roles.* properties; then, Polaris-specific properties are used to map the Quarkus-provided security roles to Polaris roles:\nquarkus.oidc.roles.role-claim-path=polaris/roles polaris.oidc.principal-roles-mapper.type=default polaris.oidc.principal-roles-mapper.filter=^(?!profile$|email$).* polaris.oidc.principal-roles-mapper.mappings[0].regex=^.*$ polaris.oidc.principal-roles-mapper.mappings[0].replacement=PRINCIPAL_ROLE:$0 These mappings can be overridden per OIDC tenant and used across different realms that rely on external identity providers. For example:\npolaris.oidc.oidc-tenant1.principal-roles-mapper.type=custom polaris.oidc.oidc-tenant1.principal-roles-mapper.filter=PRINCIPAL_ROLE:.* polaris.oidc.oidc-tenant1.principal-roles-mapper.mappings[0].regex=PRINCIPAL_ROLE:(.*) polaris.oidc.oidc-tenant1.principal-roles-mapper.mappings[0].replacement=PRINCIPAL_ROLE:$1 The default ActiveRolesProvider expects the security identity to expose role names in the following format: PRINCIPAL_ROLE:\u003crole name\u003e. You can use the filter and mappings properties to adjust the role names as they appear in the JWT claims.\nFor example, assume that the security identity produced by Quarkus exposes the following roles: role_service_admin and role_catalog_admin. Polaris expects PRINCIPAL_ROLE:service_admin and PRINCIPAL_ROLE:catalog_admin respectively. The following configuration can be used to achieve the desired mapping:\n# Exclude role names that don't start with \"role_\" polaris.oidc.principal-roles-mapper.filter=role_.* # Extract the text after \"role_\" polaris.oidc.principal-roles-mapper.mappings[0].regex=role_(.*) # Replace the extracted text with \"PRINCIPAL_ROLE:\" polaris.oidc.principal-roles-mapper.mappings[0].replacement=PRINCIPAL_ROLE:$1 See more examples below.\nDeveloper Architecture Notes The following sections describe internal implementation details for developers who want to understand or extend Polaris authentication.\nAuthentication Architecture Polaris separates authentication into two logical phases using Quarkus Security:\nCredential extraction – parsing headers and tokens Credential authentication – validating identity and assigning roles Key Interfaces Authenticator: A core interface used to authenticate credentials. DecodedToken: Used in internal auth and inherits from PrincipalCredential. ActiveRolesProvider: Resolves the set of roles associated with the authenticated user for the current request. Roles may be derived from OIDC claims or internal mappings. The DefaultAuthenticator is used to implement realm-specific logic based on these abstractions.\nToken Broker Configuration When internal authentication is enabled, Polaris uses token brokers to handle the decoding and validation of authentication tokens. These brokers are request-scoped and can be configured per realm. Each realm may use its own strategy, such as RSA key pairs or shared secrets, depending on security requirements.\nDeveloper Authentication Workflows Internal Authentication InternalAuthenticationMechanism parses the auth header. Uses TokenBroker to decode the token. Builds PrincipalAuthInfo and generates SecurityIdentity (Quarkus). Authenticator.authenticate() validates the credential. ActiveRolesProvider assigns roles. External Authentication OidcAuthenticationMechanism (Quarkus) processes the auth header. OidcTenantResolvingAugmentor selects the OIDC tenant. PrincipalAuthInfoAugmentor extracts JWT claims. Authenticator.authenticate() validates the claims. ActiveRolesProvider assigns roles. Mixed Authentication InternalAuthenticationMechanism tries decoding. If successful, proceed with internal authentication. Otherwise, fall back to external (OIDC) authentication. OIDC Configuration Reference Principal Mapping Interface: PrincipalMapper\nThe PrincipalMapper is responsible for extracting the Polaris principal ID and display name from OIDC tokens.\nImplementation selector:\nThis property selects the implementation of the PrincipalMapper interface. The default implementation extracts fields from specific claim paths.\npolaris.oidc.principal-mapper.type=default Configuration properties for the default implementation:\npolaris.oidc.principal-mapper.id-claim-path=polaris/principal_id polaris.oidc.principal-mapper.name-claim-path=polaris/principal_name It can be overridden per OIDC tenant.\nRoles Mapping Interface: PrincipalRolesMapper\nPolaris uses this component to transform role claims from OIDC tokens into Polaris roles.\nQuarkus OIDC configuration:\nThis setting instructs Quarkus on where to locate roles within the OIDC token.\nquarkus.oidc.roles.role-claim-path=polaris/roles Implementation selector:\nThis property selects the implementation of PrincipalRolesMapper. The default implementation applies regular expression (regex) transformations to OIDC roles.\npolaris.oidc.principal-roles-mapper.type=default Configuration properties for the default implementation:\npolaris.oidc.principal-roles-mapper.filter=^(?!profile$|email$).* polaris.oidc.principal-roles-mapper.mappings[0].regex=^.*$ polaris.oidc.principal-roles-mapper.mappings[0].replacement=PRINCIPAL_ROLE:$0 Example JWT Mappings Example 1: Custom Claim Paths JWT\n{ \"polaris\": { \"roles\": [\"PRINCIPAL_ROLE:ALL\"], \"principal_name\": \"root\", \"principal_id\": 1 } } Configuration\nquarkus.oidc.roles.role-claim-path=polaris/roles polaris.oidc.principal-mapper.id-claim-path=polaris/principal_id polaris.oidc.principal-mapper.name-claim-path=polaris/principal_name Example 2: Generic OIDC Claims JWT\n{ \"sub\": \"1\", \"scope\": \"service_admin catalog_admin profile email\", \"preferred_username\": \"root\" } Configuration\nquarkus.oidc.roles.role-claim-path=scope polaris.oidc.principal-mapper.id-claim-path=sub polaris.oidc.principal-mapper.name-claim-path=preferred_username polaris.oidc.principal-roles-mapper.filter=^(?!profile$|email$).* polaris.oidc.principal-roles-mapper.mappings[0].regex=^.*$ polaris.oidc.principal-roles-mapper.mappings[0].replacement=PRINCIPAL_ROLE:$0 Result\nPolaris roles: PRINCIPAL_ROLE:service_admin and PRINCIPAL_ROLE:catalog_admin\n","categories":"","description":"","excerpt":"Apache Polaris supports authentication via external identity providers …","ref":"/in-dev/unreleased/external-idp/","tags":"","title":"External Identity Providers"},{"body":"The default polaris-server.yml configuration is intended for development and testing. When deploying Polaris in production, there are several best practices to keep in mind.\nSecurity Configurations Notable configuration used to secure a Polaris deployment are outlined below.\noauth2 [!WARNING]\nEnsure that the tokenBroker setting reflects the token broker specified in authenticator below.\nConfigure OAuth with this setting. Remove the TestInlineBearerTokenPolarisAuthenticator option and uncomment the DefaultPolarisAuthenticator authenticator option beneath it. Then, configure the token broker. You can configure the token broker to use either asymmetric or symmetric keys. authenticator.tokenBroker [!WARNING]\nEnsure that the tokenBroker setting reflects the token broker specified in oauth2 above.\ncallContextResolver \u0026 realmContextResolver Use these configurations to specify a service that can resolve a realm from bearer tokens. The service(s) used here must implement the relevant interfaces (i.e. CallContextResolver and RealmContextResolver). Metastore Management [!IMPORTANT]\nThe default in-memory implementation for metastoreManager is meant for testing and not suitable for production usage. Instead, consider an implementation such as eclipse-link which allows you to store metadata in a remote database.\nA Metastore Manger should be configured with an implementation that durably persists Polaris entities. Use the configuration metaStoreManager to configure a MetastoreManager implementation where Polaris entities will be persisted.\nBe sure to secure your metastore backend since it will be storing credentials and catalog metadata.\nConfiguring EclipseLink To use EclipseLink for metastore management, specify the configuration metaStoreManager.conf-file to point to an EclipseLink persistence.xml file. This file, local to the Polaris service, contains details of the database used for metastore management and the connection settings. For more information, refer to the metastore documentation.\n[!IMPORTANT] EclipseLink requires\nBuilding the JAR for the EclipseLink extension Setting the eclipseLink gradle property to true. This can be achieved by setting eclipseLink=true in the gradle.properties file, or by passing the property explicitly while building all JARs, e.g.: ./gradlew -PeclipseLink=true clean assemble\nBootstrapping Before using Polaris when using a metastore manager other than in-memory, you must bootstrap the metastore manager. This is a manual operation that must be performed only once in order to prepare the metastore manager to integrate with Polaris. When the metastore manager is bootstrapped, any existing Polaris entities in the metastore manager may be purged.\nBy default, Polaris will create randomised CLIENT_ID and CLIENT_SECRET for the root principal and store their hashes in the metastore backend. In order to provide your own credentials for root principal (so you can request tokens via api/catalog/v1/oauth/tokens), set the following envrionment variables for realm name my_realm:\nexport POLARIS_BOOTSTRAP_MY_REALM_ROOT_CLIENT_ID=my-client-id export POLARIS_BOOTSTRAP_MY_REALM_ROOT_CLIENT_SECRET=my-client-secret IMPORTANT: In case you use default-realm for metastore backend database, you won’t be able to use export command. Use this instead:\nenv POLARIS_BOOTSTRAP_DEFAULT-REALM_ROOT_CLIENT_ID=my-client-id POLARIS_BOOTSTRAP_DEFAULT-REALM_ROOT_CLIENT_SECRET=my-client-secret \u003cbootstrap command\u003e Now, to bootstrap Polaris, run:\njava -jar /path/to/jar/polaris-service-all.jar bootstrap polaris-server.yml or in a container:\nbin/polaris-service bootstrap config/polaris-server.yml Afterward, Polaris can be launched normally:\njava -jar /path/to/jar/polaris-service-all.jar server polaris-server.yml You can verify the setup by attempting a token issue for the root principal:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens -d \"grant_type=client_credentials\u0026client_id=my-client-id\u0026client_secret=my-client-secret\u0026scope=PRINCIPAL_ROLE:ALL\" which should return:\n{\"access_token\":\"...\",\"token_type\":\"bearer\",\"issued_token_type\":\"urn:ietf:params:oauth:token-type:access_token\",\"expires_in\":3600} Note that if you used non-default realm name, for example, iceberg instead of default-realm in your polaris-server.yml, then you should add an appropriate request header:\ncurl -X POST -H 'realm: iceberg' http://localhost:8181/api/catalog/v1/oauth/tokens -d \"grant_type=client_credentials\u0026client_id=my-client-id\u0026client_secret=my-client-secret\u0026scope=PRINCIPAL_ROLE:ALL\" Other Configurations When deploying Polaris in production, consider adjusting the following configurations:\nfeatureConfiguration.SUPPORTED_CATALOG_STORAGE_TYPES By default Polaris catalogs are allowed to be located in local filesystem with the FILE storage type. This should be disabled for production systems. Use this configuration to additionally disable any other storage types that will not be in use. ","categories":"","description":"","excerpt":"The default polaris-server.yml configuration is intended for …","ref":"/releases/0.9.0/configuring-polaris-for-production/","tags":"","title":"Configuring Apache Polaris (Incubating) for Production"},{"body":"The default server configuration is intended for development and testing. When you deploy Polaris in production, review and apply the following checklist:\nConfigure OAuth2 keys Enforce realm header validation (require-header=true) Use a durable metastore (JDBC + PostgreSQL) Bootstrap valid realms in the metastore Disable local FILE storage Configure OAuth2 Polaris authentication requires specifying a token broker factory type. Two implementations are supported out of the box:\nrsa-key-pair uses a pair of public and private keys; symmetric-key uses a shared secret. By default, Polaris uses rsa-key-pair, with randomly generated keys.\n[!IMPORTANT] The default rsa-key-pair configuration is not suitable when deploying many replicas of Polaris, as each replica will have its own set of keys. This will cause token validation to fail when a request is routed to a different replica than the one that issued the token.\nIt is highly recommended to configure Polaris with previously-generated RSA keys. This can be done by setting the following properties:\npolaris.authentication.token-broker.type=rsa-key-pair polaris.authentication.token-broker.rsa-key-pair.public-key-file=/tmp/public.key polaris.authentication.token-broker.rsa-key-pair.private-key-file=/tmp/private.key To generate an RSA key pair, you can use the following commands:\nopenssl genrsa -out private.key 2048 openssl rsa -in private.key -pubout -out public.key Alternatively, you can use a symmetric key by setting the following properties:\npolaris.authentication.token-broker.type=symmetric-key polaris.authentication.token-broker.symmetric-key.file=/tmp/symmetric.key Note: it is also possible to set the symmetric key secret directly in the configuration file. If possible, pass the secret as an environment variable to avoid storing sensitive information in the configuration file:\npolaris.authentication.token-broker.symmetric-key.secret=${POLARIS_SYMMETRIC_KEY_SECRET} Finally, you can also configure the token broker to use a maximum lifespan by setting the following property:\npolaris.authentication.token-broker.max-token-generation=PT1H Typically, in Kubernetes, you would define the keys as a Secret and mount them as files in the container.\nRealm Context Resolver By default, Polaris resolves realms based on incoming request headers. You can configure the realm context resolver by setting the following properties in application.properties:\npolaris.realm-context.realms=POLARIS,MY-REALM polaris.realm-context.header-name=Polaris-Realm Where:\nrealms is a comma-separated list of allowed realms. This setting must be correctly configured. At least one realm must be specified. header-name is the name of the header used to resolve the realm; by default, it is Polaris-Realm. If a request contains the specified header, Polaris will use the realm specified in the header. If the realm is not in the list of allowed realms, Polaris will return a 404 Not Found response.\nIf a request does not contain the specified header, however, by default Polaris will use the first realm in the list as the default realm. In the above example, POLARIS is the default realm and would be used if the Polaris-Realm header is not present in the request.\nThis is not recommended for production use, as it may lead to security vulnerabilities. To avoid this, set the following property to true:\npolaris.realm-context.require-header=true This will cause Polaris to also return a 404 Not Found response if the realm header is not present in the request.\nMetastore Configuration A metastore should be configured with an implementation that durably persists Polaris entities. By default, Polaris uses an in-memory metastore.\n[!IMPORTANT] The default in-memory metastore is not suitable for production use, as it will lose all data when the server is restarted; it is also unusable when multiple Polaris replicas are used.\nTo enable a durable metastore, configure your system to use the Relational JDBC-backed metastore. This implementation leverages Quarkus for datasource management and supports configuration through environment variables or JVM -D flags at startup. For more information, refer to the Quarkus configuration reference.\nConfigure the metastore by setting the following ENV variables:\nPOLARIS_PERSISTENCE_TYPE=relational-jdbc QUARKUS_DATASOURCE_USERNAME=\u003cyour-username\u003e QUARKUS_DATASOURCE_PASSWORD=\u003cyour-password\u003e QUARKUS_DATASOURCE_JDBC_URL=\u003cjdbc-url-of-postgres\u003e The relational JDBC metastore is a Quarkus-managed datasource and only supports Postgres and H2 as of now. Please refer to the documentation here: Configure data sources in Quarkus\n[!IMPORTANT] Be sure to secure your metastore backend since it will be storing sensitive data and catalog metadata.\nNote: Polaris will always create schema ‘polaris_schema’ during bootstrap under the configured database.\nBootstrapping Before using Polaris, you must bootstrap the metastore. This is a manual operation that must be performed only once for each realm in order to prepare the metastore to integrate with Polaris.\nBy default, when bootstrapping a new realm, Polaris will create randomised CLIENT_ID and CLIENT_SECRET for the root principal and store their hashes in the metastore backend.\nDepending on your database, this may not be convenient as the generated credentials are not stored in clear text in the database.\nIn order to provide your own credentials for root principal (so you can request tokens via api/catalog/v1/oauth/tokens), use the Polaris Admin Tool\nYou can verify the setup by attempting a token issue for the root principal:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=my-client-id\" \\ -d \"client_secret=my-client-secret\" \\ -d \"scope=PRINCIPAL_ROLE:ALL\" Which should return an access token:\n{ \"access_token\": \"...\", \"token_type\": \"bearer\", \"issued_token_type\": \"urn:ietf:params:oauth:token-type:access_token\", \"expires_in\": 3600 } If you used a non-default realm name, add the appropriate request header to the curl command, otherwise Polaris will resolve the realm to the first one in the configuration polaris.realm-context.realms. Here is an example to set realm header:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \\ -H \"Polaris-Realm: my-realm\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=my-client-id\" \\ -d \"client_secret=my-client-secret\" \\ -d \"scope=PRINCIPAL_ROLE:ALL\" Disable FILE Storage Type By default, Polaris allows using the local file system (FILE) for catalog storage. This is fine for testing, but not recommended for production. To disable it, set the supported storage types like this:\npolaris.features.\"SUPPORTED_CATALOG_STORAGE_TYPES\" = [ \"S3\", \"Azure\" ] Leave out FILE to prevent its use. Only include the storage types your setup needs.\nUpgrade Considerations The Polaris Evolution page discusses backward compatibility and upgrade concerns.\n","categories":"","description":"","excerpt":"The default server configuration is intended for development and …","ref":"/in-dev/unreleased/configuring-polaris-for-production/","tags":"","title":"Configuring Polaris for Production"},{"body":"The default server configuration is intended for development and testing. When you deploy Polaris in production, review and apply the following checklist:\nConfigure OAuth2 keys Enforce realm header validation (require-header=true) Use a durable metastore (JDBC + PostgreSQL) Bootstrap valid realms in the metastore Disable local FILE storage Configure OAuth2 Polaris authentication requires specifying a token broker factory type. Two implementations are supported out of the box:\nrsa-key-pair uses a pair of public and private keys; symmetric-key uses a shared secret. By default, Polaris uses rsa-key-pair, with randomly generated keys.\n[!IMPORTANT] The default rsa-key-pair configuration is not suitable when deploying many replicas of Polaris, as each replica will have its own set of keys. This will cause token validation to fail when a request is routed to a different replica than the one that issued the token.\nIt is highly recommended to configure Polaris with previously-generated RSA keys. This can be done by setting the following properties:\npolaris.authentication.token-broker.type=rsa-key-pair polaris.authentication.token-broker.rsa-key-pair.public-key-file=/tmp/public.key polaris.authentication.token-broker.rsa-key-pair.private-key-file=/tmp/private.key To generate an RSA key pair, you can use the following commands:\nopenssl genrsa -out private.key 2048 openssl rsa -in private.key -pubout -out public.key Alternatively, you can use a symmetric key by setting the following properties:\npolaris.authentication.token-broker.type=symmetric-key polaris.authentication.token-broker.symmetric-key.file=/tmp/symmetric.key Note: it is also possible to set the symmetric key secret directly in the configuration file. If possible, pass the secret as an environment variable to avoid storing sensitive information in the configuration file:\npolaris.authentication.token-broker.symmetric-key.secret=${POLARIS_SYMMETRIC_KEY_SECRET} Finally, you can also configure the token broker to use a maximum lifespan by setting the following property:\npolaris.authentication.token-broker.max-token-generation=PT1H Typically, in Kubernetes, you would define the keys as a Secret and mount them as files in the container.\nRealm Context Resolver By default, Polaris resolves realms based on incoming request headers. You can configure the realm context resolver by setting the following properties in application.properties:\npolaris.realm-context.realms=POLARIS,MY-REALM polaris.realm-context.header-name=Polaris-Realm Where:\nrealms is a comma-separated list of allowed realms. This setting must be correctly configured. At least one realm must be specified. header-name is the name of the header used to resolve the realm; by default, it is Polaris-Realm. If a request contains the specified header, Polaris will use the realm specified in the header. If the realm is not in the list of allowed realms, Polaris will return a 404 Not Found response.\nIf a request does not contain the specified header, however, by default Polaris will use the first realm in the list as the default realm. In the above example, POLARIS is the default realm and would be used if the Polaris-Realm header is not present in the request.\nThis is not recommended for production use, as it may lead to security vulnerabilities. To avoid this, set the following property to true:\npolaris.realm-context.require-header=true This will cause Polaris to also return a 404 Not Found response if the realm header is not present in the request.\nMetastore Configuration A metastore should be configured with an implementation that durably persists Polaris entities. By default, Polaris uses an in-memory metastore.\n[!IMPORTANT] The default in-memory metastore is not suitable for production use, as it will lose all data when the server is restarted; it is also unusable when multiple Polaris replicas are used.\nTo enable a durable metastore, configure your system to use the Relational JDBC-backed metastore. This implementation leverages Quarkus for datasource management and supports configuration through environment variables or JVM -D flags at startup. For more information, refer to the Quarkus configuration reference.\nConfigure the metastore by setting the following ENV variables:\nPOLARIS_PERSISTENCE_TYPE=relational-jdbc QUARKUS_DATASOURCE_DB_KIND=postgresql QUARKUS_DATASOURCE_USERNAME=\u003cyour-username\u003e QUARKUS_DATASOURCE_PASSWORD=\u003cyour-password\u003e QUARKUS_DATASOURCE_JDBC_URL=\u003cjdbc-url-of-postgres\u003e The relational JDBC metastore is a Quarkus-managed datasource and only supports Postgres and H2 as of now. Please refer to the documentation here: Configure data sources in Quarkus\n[!IMPORTANT] Be sure to secure your metastore backend since it will be storing sensitive data and catalog metadata.\nNote: Polaris will always create schema ‘polaris_schema’ during bootstrap under the configured database.\nBootstrapping Before using Polaris, you must bootstrap the metastore. This is a manual operation that must be performed only once for each realm in order to prepare the metastore to integrate with Polaris.\nBy default, when bootstrapping a new realm, Polaris will create randomised CLIENT_ID and CLIENT_SECRET for the root principal and store their hashes in the metastore backend.\nDepending on your database, this may not be convenient as the generated credentials are not stored in clear text in the database.\nIn order to provide your own credentials for root principal (so you can request tokens via api/catalog/v1/oauth/tokens), use the Polaris Admin Tool\nYou can verify the setup by attempting a token issue for the root principal:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=my-client-id\" \\ -d \"client_secret=my-client-secret\" \\ -d \"scope=PRINCIPAL_ROLE:ALL\" Which should return an access token:\n{ \"access_token\": \"...\", \"token_type\": \"bearer\", \"issued_token_type\": \"urn:ietf:params:oauth:token-type:access_token\", \"expires_in\": 3600 } If you used a non-default realm name, add the appropriate request header to the curl command, otherwise Polaris will resolve the realm to the first one in the configuration polaris.realm-context.realms. Here is an example to set realm header:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \\ -H \"Polaris-Realm: my-realm\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=my-client-id\" \\ -d \"client_secret=my-client-secret\" \\ -d \"scope=PRINCIPAL_ROLE:ALL\" Disable FILE Storage Type By default, Polaris allows using the local file system (FILE) for catalog storage. This is fine for testing, but not recommended for production. To disable it, set the supported storage types like this:\npolaris.features.\"SUPPORTED_CATALOG_STORAGE_TYPES\" = [ \"S3\", \"Azure\" ] Leave out FILE to prevent its use. Only include the storage types your setup needs.\nUpgrade Considerations The Polaris Evolution page discusses backward compatibility and upgrade concerns.\n","categories":"","description":"","excerpt":"The default server configuration is intended for development and …","ref":"/releases/1.0.0/configuring-polaris-for-production/","tags":"","title":"Configuring Polaris for Production"},{"body":"The default server configuration is intended for development and testing. When you deploy Polaris in production, review and apply the following checklist:\nConfigure OAuth2 keys Enforce realm header validation (require-header=true) Use a durable metastore (JDBC + PostgreSQL) Bootstrap valid realms in the metastore Disable local FILE storage Configure OAuth2 Polaris authentication requires specifying a token broker factory type. Two implementations are supported out of the box:\nrsa-key-pair uses a pair of public and private keys; symmetric-key uses a shared secret. By default, Polaris uses rsa-key-pair, with randomly generated keys.\n[!IMPORTANT] The default rsa-key-pair configuration is not suitable when deploying many replicas of Polaris, as each replica will have its own set of keys. This will cause token validation to fail when a request is routed to a different replica than the one that issued the token.\nIt is highly recommended to configure Polaris with previously-generated RSA keys. This can be done by setting the following properties:\npolaris.authentication.token-broker.type=rsa-key-pair polaris.authentication.token-broker.rsa-key-pair.public-key-file=/tmp/public.key polaris.authentication.token-broker.rsa-key-pair.private-key-file=/tmp/private.key To generate an RSA key pair, you can use the following commands:\nopenssl genrsa -out private.key 2048 openssl rsa -in private.key -pubout -out public.key Alternatively, you can use a symmetric key by setting the following properties:\npolaris.authentication.token-broker.type=symmetric-key polaris.authentication.token-broker.symmetric-key.file=/tmp/symmetric.key Note: it is also possible to set the symmetric key secret directly in the configuration file. If possible, pass the secret as an environment variable to avoid storing sensitive information in the configuration file:\npolaris.authentication.token-broker.symmetric-key.secret=${POLARIS_SYMMETRIC_KEY_SECRET} Finally, you can also configure the token broker to use a maximum lifespan by setting the following property:\npolaris.authentication.token-broker.max-token-generation=PT1H Typically, in Kubernetes, you would define the keys as a Secret and mount them as files in the container.\nRealm Context Resolver By default, Polaris resolves realms based on incoming request headers. You can configure the realm context resolver by setting the following properties in application.properties:\npolaris.realm-context.realms=POLARIS,MY-REALM polaris.realm-context.header-name=Polaris-Realm Where:\nrealms is a comma-separated list of allowed realms. This setting must be correctly configured. At least one realm must be specified. header-name is the name of the header used to resolve the realm; by default, it is Polaris-Realm. If a request contains the specified header, Polaris will use the realm specified in the header. If the realm is not in the list of allowed realms, Polaris will return a 404 Not Found response.\nIf a request does not contain the specified header, however, by default Polaris will use the first realm in the list as the default realm. In the above example, POLARIS is the default realm and would be used if the Polaris-Realm header is not present in the request.\nThis is not recommended for production use, as it may lead to security vulnerabilities. To avoid this, set the following property to true:\npolaris.realm-context.require-header=true This will cause Polaris to also return a 404 Not Found response if the realm header is not present in the request.\nMetastore Configuration A metastore should be configured with an implementation that durably persists Polaris entities. By default, Polaris uses an in-memory metastore.\n[!IMPORTANT] The default in-memory metastore is not suitable for production use, as it will lose all data when the server is restarted; it is also unusable when multiple Polaris replicas are used.\nTo enable a durable metastore, configure your system to use the Relational JDBC-backed metastore. This implementation leverages Quarkus for datasource management and supports configuration through environment variables or JVM -D flags at startup. For more information, refer to the Quarkus configuration reference.\nConfigure the metastore by setting the following ENV variables:\nPOLARIS_PERSISTENCE_TYPE=relational-jdbc QUARKUS_DATASOURCE_DB_KIND=postgresql QUARKUS_DATASOURCE_USERNAME=\u003cyour-username\u003e QUARKUS_DATASOURCE_PASSWORD=\u003cyour-password\u003e QUARKUS_DATASOURCE_JDBC_URL=\u003cjdbc-url-of-postgres\u003e The relational JDBC metastore is a Quarkus-managed datasource and only supports Postgres and H2 as of now. Please refer to the documentation here: Configure data sources in Quarkus\n[!IMPORTANT] Be sure to secure your metastore backend since it will be storing sensitive data and catalog metadata.\nNote: Polaris will always create schema ‘polaris_schema’ during bootstrap under the configured database.\nBootstrapping Before using Polaris, you must bootstrap the metastore. This is a manual operation that must be performed only once for each realm in order to prepare the metastore to integrate with Polaris.\nBy default, when bootstrapping a new realm, Polaris will create randomised CLIENT_ID and CLIENT_SECRET for the root principal and store their hashes in the metastore backend.\nDepending on your database, this may not be convenient as the generated credentials are not stored in clear text in the database.\nIn order to provide your own credentials for root principal (so you can request tokens via api/catalog/v1/oauth/tokens), use the Polaris Admin Tool\nYou can verify the setup by attempting a token issue for the root principal:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=my-client-id\" \\ -d \"client_secret=my-client-secret\" \\ -d \"scope=PRINCIPAL_ROLE:ALL\" Which should return an access token:\n{ \"access_token\": \"...\", \"token_type\": \"bearer\", \"issued_token_type\": \"urn:ietf:params:oauth:token-type:access_token\", \"expires_in\": 3600 } If you used a non-default realm name, add the appropriate request header to the curl command, otherwise Polaris will resolve the realm to the first one in the configuration polaris.realm-context.realms. Here is an example to set realm header:\ncurl -X POST http://localhost:8181/api/catalog/v1/oauth/tokens \\ -H \"Polaris-Realm: my-realm\" \\ -d \"grant_type=client_credentials\" \\ -d \"client_id=my-client-id\" \\ -d \"client_secret=my-client-secret\" \\ -d \"scope=PRINCIPAL_ROLE:ALL\" Disable FILE Storage Type By default, Polaris allows using the local file system (FILE) for catalog storage. This is fine for testing, but not recommended for production. To disable it, set the supported storage types like this:\npolaris.features.\"SUPPORTED_CATALOG_STORAGE_TYPES\" = [ \"S3\", \"Azure\" ] Leave out FILE to prevent its use. Only include the storage types your setup needs.\nUpgrade Considerations The Polaris Evolution page discusses backward compatibility and upgrade concerns.\n","categories":"","description":"","excerpt":"The default server configuration is intended for development and …","ref":"/releases/1.0.1/configuring-polaris-for-production/","tags":"","title":"Configuring Polaris for Production"},{"body":"Apache Polaris now provides Catalog support for Generic Tables (non-Iceberg tables), please check out the Catalog API Spec for Generic Table API specs.\nAlong with the Generic Table Catalog support, Polaris is also releasing a Spark client, which helps to provide an end-to-end solution for Apache Spark to manage Delta tables using Polaris.\nNote the Polaris Spark client is able to handle both Iceberg and Delta tables, not just Delta.\nThis page documents how to connect Spark with Polaris Service using the Polaris Spark client.\nQuick Start with Local Polaris service If you want to quickly try out the functionality with a local Polaris service, simply check out the Polaris repo and follow the instructions in the Spark plugin getting-started README.\nCheck out the Polaris repo:\ngit clone https://github.com/apache/polaris.git ~/polaris Start Spark against a deployed Polaris service Before starting, ensure that the deployed Polaris service supports Generic Tables, and that Spark 3.5(version 3.5.3 or later is installed). Spark 3.5.6 is recommended, and you can follow the instructions below to get a Spark 3.5.6 distribution.\ncd ~ wget https://www.apache.org/dyn/closer.lua/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz?action=download mkdir spark-3.5 tar xzvf spark-3.5.6-bin-hadoop3.tgz -C spark-3.5 --strip-components=1 cd spark-3.5 Connecting with Spark using the Polaris Spark client The following CLI command can be used to start the Spark with connection to the deployed Polaris service using a released Polaris Spark client.\nbin/spark-shell \\ --packages \u003cpolaris-spark-client-package\u003e,org.apache.iceberg:iceberg-aws-bundle:1.9.1,io.delta:delta-spark_2.12:3.3.1 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.warehouse=\u003cpolaris-catalog-name\u003e \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e=org.apache.polaris.spark.SparkCatalog \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.uri=\u003cpolaris-service-uri\u003e \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.credential='\u003cclient-id\u003e:\u003cclient-secret\u003e' \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.token-refresh-enabled=true Assume the released Polaris Spark client you want to use is org.apache.polaris:polaris-spark-3.5_2.12:1.0.0, replace the polaris-spark-client-package field with the release.\nThe spark-catalog-name is the catalog name you will use with Spark, and polaris-catalog-name is the catalog name used by Polaris service, for simplicity, you can use the same name.\nReplace the polaris-service-uri with the uri of the deployed Polaris service. For example, with a locally deployed Polaris service, the uri would be http://localhost:8181/api/catalog.\nFor client-id and client-secret values, you can refer to Using Polaris for more details.\nYou can also start the connection by programmatically initialize a SparkSession, following is an example with PySpark:\nfrom pyspark.sql import SparkSession spark = SparkSession.builder .config(\"spark.jars.packages\", \"\u003cpolaris-spark-client-package\u003e,org.apache.iceberg:iceberg-aws-bundle:1.9.1,io.delta:delta-spark_2.12:3.3.1\") .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e\", \"org.apache.polaris.spark.SparkCatalog\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.uri\", \u003cpolaris-service-uri\u003e) .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.token-refresh-enabled\", \"true\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.credential\", \"\u003cclient-id\u003e:\u003cclient_secret\u003e\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.warehouse\", \u003cpolaris_catalog_name\u003e) .config(\"spark.sql.catalog.polaris.scope\", 'PRINCIPAL_ROLE:ALL') .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", 'vended-credentials') .getOrCreate() Similar as the CLI command, make sure the corresponding fields are replaced correctly.\nCreate tables with Spark After Spark is started, you can use it to create and access Iceberg and Delta tables, for example:\nspark.sql(\"USE polaris\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS.PUBLIC\") spark.sql(\"USE NAMESPACE DELTA_NS.PUBLIC\") spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS PEOPLE ( id int, name string) USING delta LOCATION 'file:///tmp/var/delta_tables/people'; \"\"\") Connecting with Spark using local Polaris Spark client jar If you would like to use a version of the Spark client that is currently not yet released, you can build a Spark client jar locally from source. Please check out the Polaris repo and refer to the Spark plugin README for detailed instructions.\nLimitations The Polaris Spark client has the following functionality limitations:\nCreate table as select (CTAS) is not supported for Delta tables. As a result, the saveAsTable method of Dataframe is also not supported, since it relies on the CTAS support. Create a Delta table without explicit location is not supported. Rename a Delta table is not supported. ALTER TABLE … SET LOCATION is not supported for DELTA table. For other non-Iceberg tables like csv, it is not supported. ","categories":"","description":"","excerpt":"Apache Polaris now provides Catalog support for Generic Tables …","ref":"/in-dev/unreleased/polaris-spark-client/","tags":"","title":"Polaris Spark Client"},{"body":"Apache Polaris now provides Catalog support for Generic Tables (non-Iceberg tables), please check out the Catalog API Spec for Generic Table API specs.\nAlong with the Generic Table Catalog support, Polaris is also releasing a Spark client, which helps to provide an end-to-end solution for Apache Spark to manage Delta tables using Polaris.\nNote the Polaris Spark client is able to handle both Iceberg and Delta tables, not just Delta.\nThis page documents how to connect Spark with Polaris Service using the Polaris Spark client.\nQuick Start with Local Polaris service If you want to quickly try out the functionality with a local Polaris service, simply check out the Polaris repo and follow the instructions in the Spark plugin getting-started README.\nCheck out the Polaris repo:\ncd ~ git clone https://github.com/apache/polaris.git Start Spark against a deployed Polaris service Before starting, ensure that the deployed Polaris service supports Generic Tables, and that Spark 3.5(version 3.5.3 or later is installed). Spark 3.5.5 is recommended, and you can follow the instructions below to get a Spark 3.5.5 distribution.\ncd ~ wget https://archive.apache.org/dist/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz mkdir spark-3.5 tar xzvf spark-3.5.5-bin-hadoop3.tgz -C spark-3.5 --strip-components=1 cd spark-3.5 Connecting with Spark using the Polaris Spark client The following CLI command can be used to start the Spark with connection to the deployed Polaris service using a released Polaris Spark client.\nbin/spark-shell \\ --packages \u003cpolaris-spark-client-package\u003e,org.apache.iceberg:iceberg-aws-bundle:1.9.0,io.delta:delta-spark_2.12:3.3.1 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.warehouse=\u003cpolaris-catalog-name\u003e \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e=org.apache.polaris.spark.SparkCatalog \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.uri=\u003cpolaris-service-uri\u003e \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.credential='\u003cclient-id\u003e:\u003cclient-secret\u003e' \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.token-refresh-enabled=true Assume the released Polaris Spark client you want to use is org.apache.polaris:polaris-spark-3.5_2.12:1.0.0, replace the polaris-spark-client-package field with the release.\nThe spark-catalog-name is the catalog name you will use with Spark, and polaris-catalog-name is the catalog name used by Polaris service, for simplicity, you can use the same name.\nReplace the polaris-service-uri with the uri of the deployed Polaris service. For example, with a locally deployed Polaris service, the uri would be http://localhost:8181/api/catalog.\nFor client-id and client-secret values, you can refer to Using Polaris for more details.\nYou can also start the connection by programmatically initialize a SparkSession, following is an example with PySpark:\nfrom pyspark.sql import SparkSession spark = SparkSession.builder .config(\"spark.jars.packages\", \"\u003cpolaris-spark-client-package\u003e,org.apache.iceberg:iceberg-aws-bundle:1.9.0,io.delta:delta-spark_2.12:3.3.1\") .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e\", \"org.apache.polaris.spark.SparkCatalog\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.uri\", \u003cpolaris-service-uri\u003e) .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.token-refresh-enabled\", \"true\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.credential\", \"\u003cclient-id\u003e:\u003cclient_secret\u003e\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.warehouse\", \u003cpolaris_catalog_name\u003e) .config(\"spark.sql.catalog.polaris.scope\", 'PRINCIPAL_ROLE:ALL') .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", 'vended-credentials') .getOrCreate() Similar as the CLI command, make sure the corresponding fields are replaced correctly.\nCreate tables with Spark After Spark is started, you can use it to create and access Iceberg and Delta tables, for example:\nspark.sql(\"USE polaris\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS.PUBLIC\") spark.sql(\"USE NAMESPACE DELTA_NS.PUBLIC\") spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS PEOPLE ( id int, name string) USING delta LOCATION 'file:///tmp/var/delta_tables/people'; \"\"\") Connecting with Spark using local Polaris Spark client jar If you would like to use a version of the Spark client that is currently not yet released, you can build a Spark client jar locally from source. Please check out the Polaris repo and refer to the Spark plugin README for detailed instructions.\nLimitations The Polaris Spark client has the following functionality limitations:\nCreate table as select (CTAS) is not supported for Delta tables. As a result, the saveAsTable method of Dataframe is also not supported, since it relies on the CTAS support. Create a Delta table without explicit location is not supported. Rename a Delta table is not supported. ALTER TABLE … SET LOCATION is not supported for DELTA table. For other non-Iceberg tables like csv, it is not supported. Iceberg Spark Client compatibility with Polaris Spark Client The Polaris Spark client today depends on a specific Iceberg client version, and the version dependency is described in the following table:\nSpark Client Version Iceberg Spark Client Version 1.0.0 1.9.0 The Iceberg dependency is automatically downloaded when the Polaris package is downloaded, so there is no need to add the Iceberg Spark client in the packages configuration.\n","categories":"","description":"","excerpt":"Apache Polaris now provides Catalog support for Generic Tables …","ref":"/releases/1.0.0/polaris-spark-client/","tags":"","title":"Polaris Spark Client"},{"body":"Apache Polaris now provides Catalog support for Generic Tables (non-Iceberg tables), please check out the Catalog API Spec for Generic Table API specs.\nAlong with the Generic Table Catalog support, Polaris is also releasing a Spark client, which helps to provide an end-to-end solution for Apache Spark to manage Delta tables using Polaris.\nNote the Polaris Spark client is able to handle both Iceberg and Delta tables, not just Delta.\nThis page documents how to connect Spark with Polaris Service using the Polaris Spark client.\nQuick Start with Local Polaris service If you want to quickly try out the functionality with a local Polaris service, simply check out the Polaris repo and follow the instructions in the Spark plugin getting-started README.\nCheck out the Polaris repo:\ncd ~ git clone https://github.com/apache/polaris.git Start Spark against a deployed Polaris service Before starting, ensure that the deployed Polaris service supports Generic Tables, and that Spark 3.5(version 3.5.3 or later is installed). Spark 3.5.5 is recommended, and you can follow the instructions below to get a Spark 3.5.5 distribution.\ncd ~ wget https://archive.apache.org/dist/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz mkdir spark-3.5 tar xzvf spark-3.5.5-bin-hadoop3.tgz -C spark-3.5 --strip-components=1 cd spark-3.5 Connecting with Spark using the Polaris Spark client The following CLI command can be used to start the Spark with connection to the deployed Polaris service using a released Polaris Spark client.\nbin/spark-shell \\ --packages \u003cpolaris-spark-client-package\u003e,org.apache.iceberg:iceberg-aws-bundle:1.9.0,io.delta:delta-spark_2.12:3.3.1 \\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.warehouse=\u003cpolaris-catalog-name\u003e \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.header.X-Iceberg-Access-Delegation=vended-credentials \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e=org.apache.polaris.spark.SparkCatalog \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.uri=\u003cpolaris-service-uri\u003e \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.credential='\u003cclient-id\u003e:\u003cclient-secret\u003e' \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.scope='PRINCIPAL_ROLE:ALL' \\ --conf spark.sql.catalog.\u003cspark-catalog-name\u003e.token-refresh-enabled=true Assume the released Polaris Spark client you want to use is org.apache.polaris:polaris-spark-3.5_2.12:1.0.1, replace the polaris-spark-client-package field with the release.\nThe spark-catalog-name is the catalog name you will use with Spark, and polaris-catalog-name is the catalog name used by Polaris service, for simplicity, you can use the same name.\nReplace the polaris-service-uri with the uri of the deployed Polaris service. For example, with a locally deployed Polaris service, the uri would be http://localhost:8181/api/catalog.\nFor client-id and client-secret values, you can refer to Using Polaris for more details.\nYou can also start the connection by programmatically initialize a SparkSession, following is an example with PySpark:\nfrom pyspark.sql import SparkSession spark = SparkSession.builder .config(\"spark.jars.packages\", \"\u003cpolaris-spark-client-package\u003e,org.apache.iceberg:iceberg-aws-bundle:1.9.0,io.delta:delta-spark_2.12:3.3.1\") .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e\", \"org.apache.polaris.spark.SparkCatalog\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.uri\", \u003cpolaris-service-uri\u003e) .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.token-refresh-enabled\", \"true\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.credential\", \"\u003cclient-id\u003e:\u003cclient_secret\u003e\") .config(\"spark.sql.catalog.\u003cspark-catalog-name\u003e.warehouse\", \u003cpolaris_catalog_name\u003e) .config(\"spark.sql.catalog.polaris.scope\", 'PRINCIPAL_ROLE:ALL') .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", 'vended-credentials') .getOrCreate() Similar as the CLI command, make sure the corresponding fields are replaced correctly.\nCreate tables with Spark After Spark is started, you can use it to create and access Iceberg and Delta tables, for example:\nspark.sql(\"USE polaris\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS\") spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS.PUBLIC\") spark.sql(\"USE NAMESPACE DELTA_NS.PUBLIC\") spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS PEOPLE ( id int, name string) USING delta LOCATION 'file:///tmp/var/delta_tables/people'; \"\"\") Connecting with Spark using local Polaris Spark client jar If you would like to use a version of the Spark client that is currently not yet released, you can build a Spark client jar locally from source. Please check out the Polaris repo and refer to the Spark plugin README for detailed instructions.\nLimitations The Polaris Spark client has the following functionality limitations:\nCreate table as select (CTAS) is not supported for Delta tables. As a result, the saveAsTable method of Dataframe is also not supported, since it relies on the CTAS support. Create a Delta table without explicit location is not supported. Rename a Delta table is not supported. ALTER TABLE … SET LOCATION is not supported for DELTA table. For other non-Iceberg tables like csv, it is not supported. ","categories":"","description":"","excerpt":"Apache Polaris now provides Catalog support for Generic Tables …","ref":"/releases/1.0.1/polaris-spark-client/","tags":"","title":"Polaris Spark Client"},{"body":" A Helm chart for Apache Polaris (incubating).\nHomepage: https://polaris.apache.org/\nSource Code https://github.com/apache/polaris Installation Running locally with a Minikube cluster The below instructions assume Minikube and Helm are installed.\nStart the Minikube cluster, build and load image into the Minikube cluster:\nminikube start eval $(minikube docker-env) ./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild --rerun \\ :polaris-admin:assemble \\ :polaris-admin:quarkusAppPartsBuild --rerun \\ -Dquarkus.container-image.build=true Installing the chart locally The below instructions assume a local Kubernetes cluster is running and Helm is installed.\nCommon setup Create the target namespace:\nkubectl create namespace polaris Create all the required resources in the polaris namespace. This usually includes a Postgres database, Kubernetes secrets, and service accounts. The Polaris chart does not create these resources automatically, as they are not required for all Polaris deployments. The chart will fail if these resources are not created beforehand. You can find some examples in the helm/polaris/ci/fixtures directory, but beware that these are primarily intended for tests.\nBelow are two sample deployment models for installing the chart: one with a non-persistent backend and another with a persistent backend.\n[!WARNING] The examples below use values files located in the helm/polaris/ci directory. These files are intended for testing purposes primarily, and may not be suitable for production use. For production deployments, create your own values files based on the provided examples.\nNon-persistent backend Install the chart with a non-persistent backend. From Polaris repo root:\nhelm upgrade --install --namespace polaris \\ polaris helm/polaris Persistent backend [!WARNING] The Postgres deployment set up in the fixtures directory is intended for testing purposes only and is not suitable for production use. For production deployments, use a managed Postgres service or a properly configured and secured Postgres instance.\nInstall the chart with a persistent backend. From Polaris repo root:\nhelm upgrade --install --namespace polaris \\ --values helm/polaris/ci/persistence-values.yaml \\ polaris helm/polaris kubectl wait --namespace polaris --for=condition=ready pod --selector=app.kubernetes.io/name=polaris --timeout=120s To access Polaris and Postgres locally, set up port forwarding for both services (This is needed for bootstrap processes):\nkubectl port-forward -n polaris $(kubectl get pod -n polaris -l app.kubernetes.io/name=polaris -o jsonpath='{.items[0].metadata.name}') 8181:8181 kubectl port-forward -n polaris $(kubectl get pod -n polaris -l app.kubernetes.io/name=postgres -o jsonpath='{.items[0].metadata.name}') 5432:5432 Run the catalog bootstrap using the Polaris admin tool. This step initializes the catalog with the required configuration:\ncontainer_envs=$(kubectl exec -it -n polaris $(kubectl get pod -n polaris -l app.kubernetes.io/name=polaris -o jsonpath='{.items[0].metadata.name}') -- env) export QUARKUS_DATASOURCE_USERNAME=$(echo \"$container_envs\" | grep quarkus.datasource.username | awk -F '=' '{print $2}' | tr -d '\\n\\r') export QUARKUS_DATASOURCE_PASSWORD=$(echo \"$container_envs\" | grep quarkus.datasource.password | awk -F '=' '{print $2}' | tr -d '\\n\\r') export QUARKUS_DATASOURCE_JDBC_URL=$(echo \"$container_envs\" | grep quarkus.datasource.jdbc.url | sed 's/postgres/localhost/2' | awk -F '=' '{print $2}' | tr -d '\\n\\r') java -jar runtime/admin/build/quarkus-app/quarkus-run.jar bootstrap -c POLARIS,root,pass -r POLARIS Uninstalling helm uninstall --namespace polaris polaris kubectl delete --namespace polaris -f helm/polaris/ci/fixtures/ kubectl delete namespace polaris Development \u0026 Testing This section is intended for developers who want to run the Polaris Helm chart tests.\nPrerequisites The following tools are required to run the tests:\nHelm Unit Test Chart Testing Quick installation instructions for these tools:\nhelm plugin install https://github.com/helm-unittest/helm-unittest.git brew install chart-testing The integration tests also require some fixtures to be deployed. The ci/fixtures directory contains the required resources. To deploy them, run the following command:\nkubectl apply --namespace polaris -f helm/polaris/ci/fixtures/ kubectl wait --namespace polaris --for=condition=ready pod --selector=app.kubernetes.io/name=postgres --timeout=120s The helm/polaris/ci contains a number of values files that will be used to install the chart with different configurations.\nRunning the unit tests Helm unit tests do not require a Kubernetes cluster. To run the unit tests, execute Helm Unit from the Polaris repo root:\nhelm unittest helm/polaris You can also lint the chart using the Chart Testing tool, with the following command:\nct lint --charts helm/polaris Running the integration tests Integration tests require a Kubernetes cluster. See installation instructions above for setting up a local cluster.\nIntegration tests are run with the Chart Testing tool:\nct install --namespace polaris --charts ./helm/polaris Values Key Type Default Description advancedConfig object {} Advanced configuration. You can pass here any valid Polaris or Quarkus configuration property. Any property that is defined here takes precedence over all the other configuration values generated by this chart. Properties can be passed “flattened” or as nested YAML objects (see examples below). Note: values should be strings; avoid using numbers, booleans, or other types. affinity object {} Affinity and anti-affinity for polaris pods. See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity. authentication object {\"authenticator\":{\"type\":\"default\"},\"realmOverrides\":{},\"tokenBroker\":{\"maxTokenGeneration\":\"PT1H\",\"secret\":{\"name\":null,\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\",\"rsaKeyPair\":{\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\"},\"secretKey\":\"symmetric.pem\",\"symmetricKey\":{\"secretKey\":\"symmetric.key\"}},\"type\":\"rsa-key-pair\"},\"tokenService\":{\"type\":\"default\"},\"type\":\"internal\"} Polaris authentication configuration. authentication.authenticator object {\"type\":\"default\"} The Authenticator implementation to use. Only one built-in type is supported: default. authentication.realmOverrides object {} Authentication configuration overrides per realm. authentication.tokenBroker object {\"maxTokenGeneration\":\"PT1H\",\"secret\":{\"name\":null,\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\",\"rsaKeyPair\":{\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\"},\"secretKey\":\"symmetric.pem\",\"symmetricKey\":{\"secretKey\":\"symmetric.key\"}},\"type\":\"rsa-key-pair\"} The TokenBroker implementation to use. Two built-in types are supported: rsa-key-pair and symmetric-key. Only relevant when using internal (or mixed) authentication. When using external authentication, the token broker is not used. authentication.tokenBroker.maxTokenGeneration string \"PT1H\" Maximum token generation duration (e.g., PT1H for 1 hour). authentication.tokenBroker.secret object {\"name\":null,\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\",\"rsaKeyPair\":{\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\"},\"secretKey\":\"symmetric.pem\",\"symmetricKey\":{\"secretKey\":\"symmetric.key\"}} The secret name to pull the public and private keys, or the symmetric key secret from. authentication.tokenBroker.secret.name string nil The name of the secret to pull the keys from. If not provided, a key pair will be generated. This is not recommended for production. authentication.tokenBroker.secret.privateKey string \"private.pem\" DEPRECATED: Use authentication.tokenBroker.secret.rsaKeyPair.privateKey instead. Key name inside the secret for the private key authentication.tokenBroker.secret.publicKey string \"public.pem\" DEPRECATED: Use authentication.tokenBroker.secret.rsaKeyPair.publicKey instead. Key name inside the secret for the public key authentication.tokenBroker.secret.rsaKeyPair object {\"privateKey\":\"private.pem\",\"publicKey\":\"public.pem\"} Optional: configuration specific to RSA key pair secret. authentication.tokenBroker.secret.rsaKeyPair.privateKey string \"private.pem\" Key name inside the secret for the private key authentication.tokenBroker.secret.rsaKeyPair.publicKey string \"public.pem\" Key name inside the secret for the public key authentication.tokenBroker.secret.secretKey string \"symmetric.pem\" DEPRECATED: Use authentication.tokenBroker.secret.symmetricKey.secretKey instead. Key name inside the secret for the symmetric key authentication.tokenBroker.secret.symmetricKey object {\"secretKey\":\"symmetric.key\"} Optional: configuration specific to symmetric key secret. authentication.tokenBroker.secret.symmetricKey.secretKey string \"symmetric.key\" Key name inside the secret for the symmetric key authentication.tokenService object {\"type\":\"default\"} The token service (IcebergRestOAuth2ApiService) implementation to use. Two built-in types are supported: default and disabled. Only relevant when using internal (or mixed) authentication. When using external authentication, the token service is always disabled. authentication.type string \"internal\" The type of authentication to use. Three built-in types are supported: internal, external, and mixed. autoscaling.enabled bool false Specifies whether automatic horizontal scaling should be enabled. Do not enable this when using in-memory version store type. autoscaling.maxReplicas int 3 The maximum number of replicas to maintain. autoscaling.minReplicas int 1 The minimum number of replicas to maintain. autoscaling.targetCPUUtilizationPercentage int 80 Optional; set to zero or empty to disable. autoscaling.targetMemoryUtilizationPercentage string nil Optional; set to zero or empty to disable. configMapLabels object {} Additional Labels to apply to polaris configmap. containerSecurityContext object {\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"runAsNonRoot\":true,\"runAsUser\":10000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}} Security context for the polaris container. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/. containerSecurityContext.runAsUser int 10000 UID 10000 is compatible with Polaris OSS default images; change this if you are using a different image. cors object {\"accessControlAllowCredentials\":null,\"accessControlMaxAge\":null,\"allowedHeaders\":[],\"allowedMethods\":[],\"allowedOrigins\":[],\"exposedHeaders\":[]} Polaris CORS configuration. cors.accessControlAllowCredentials string nil The Access-Control-Allow-Credentials response header. The value of this header will default to true if allowedOrigins property is set and there is a match with the precise Origin header. cors.accessControlMaxAge string nil The Access-Control-Max-Age response header value indicating how long the results of a pre-flight request can be cached. Must be a valid duration. cors.allowedHeaders list [] HTTP headers allowed for CORS, ex: X-Custom, Content-Disposition. If this is not set or empty, all requested headers are considered allowed. cors.allowedMethods list [] HTTP methods allowed for CORS, ex: GET, PUT, POST. If this is not set or empty, all requested methods are considered allowed. cors.allowedOrigins list [] Origins allowed for CORS, e.g. http://polaris.apache.org, http://localhost:8181. In case an entry of the list is surrounded by forward slashes, it is interpreted as a regular expression. cors.exposedHeaders list [] HTTP headers exposed to the client, ex: X-Custom, Content-Disposition. The default is an empty list. extraEnv list [] Advanced configuration via Environment Variables. Extra environment variables to add to the Polaris server container. You can pass here any valid EnvVar object: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#envvar-v1-core This can be useful to get configuration values from Kubernetes secrets or config maps. extraInitContainers list [] Add additional init containers to the polaris pod(s) See https://kubernetes.io/docs/concepts/workloads/pods/init-containers/. extraServices list [] Additional service definitions. All service definitions always select all Polaris pods. Use this if you need to expose specific ports with different configurations, e.g. expose polaris-http with an alternate LoadBalancer service instead of ClusterIP. extraVolumeMounts list [] Extra volume mounts to add to the polaris container. See https://kubernetes.io/docs/concepts/storage/volumes/. extraVolumes list [] Extra volumes to add to the polaris pod. See https://kubernetes.io/docs/concepts/storage/volumes/. features object {\"realmOverrides\":{}} Polaris features configuration. features.realmOverrides object {} Features to enable or disable per realm. This field is a map of maps. The realm name is the key, and the value is a map of feature names to values. If a feature is not present in the map, the default value from the ‘defaults’ field is used. fileIo object {\"type\":\"default\"} Polaris FileIO configuration. fileIo.type string \"default\" The type of file IO to use. Two built-in types are supported: default and wasb. The wasb one translates WASB paths to ABFS ones. image.configDir string \"/deployments/config\" The path to the directory where the application.properties file, and other configuration files, if any, should be mounted. Note: if you are using EclipseLink, then this value must be at least two folders down to the root folder, e.g. /deployments/config is OK, whereas /deployments is not. image.pullPolicy string \"IfNotPresent\" The image pull policy. image.repository string \"apache/polaris\" The image repository to pull from. image.tag string \"latest\" The image tag. imagePullSecrets list [] References to secrets in the same namespace to use for pulling any of the images used by this chart. Each entry is a LocalObjectReference to an existing secret in the namespace. The secret must contain a .dockerconfigjson key with a base64-encoded Docker configuration file. See https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ for more information. ingress.annotations object {} Annotations to add to the ingress. ingress.className string \"\" Specifies the ingressClassName; leave empty if you don’t want to customize it ingress.enabled bool false Specifies whether an ingress should be created. ingress.hosts list [{\"host\":\"chart-example.local\",\"paths\":[]}] A list of host paths used to configure the ingress. ingress.tls list [] A list of TLS certificates; each entry has a list of hosts in the certificate, along with the secret name used to terminate TLS traffic on port 443. livenessProbe object {\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"terminationGracePeriodSeconds\":30,\"timeoutSeconds\":10} Configures the liveness probe for polaris pods. livenessProbe.failureThreshold int 3 Minimum consecutive failures for the probe to be considered failed after having succeeded. Minimum value is 1. livenessProbe.initialDelaySeconds int 5 Number of seconds after the container has started before liveness probes are initiated. Minimum value is 0. livenessProbe.periodSeconds int 10 How often (in seconds) to perform the probe. Minimum value is 1. livenessProbe.successThreshold int 1 Minimum consecutive successes for the probe to be considered successful after having failed. Minimum value is 1. livenessProbe.terminationGracePeriodSeconds int 30 Optional duration in seconds the pod needs to terminate gracefully upon probe failure. Minimum value is 1. livenessProbe.timeoutSeconds int 10 Number of seconds after which the probe times out. Minimum value is 1. logging object {\"categories\":{\"org.apache.iceberg.rest\":\"INFO\",\"org.apache.polaris\":\"INFO\"},\"console\":{\"enabled\":true,\"format\":\"%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n\",\"json\":false,\"threshold\":\"ALL\"},\"file\":{\"enabled\":false,\"fileName\":\"polaris.log\",\"format\":\"%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n\",\"json\":false,\"logsDir\":\"/deployments/logs\",\"rotation\":{\"fileSuffix\":null,\"maxBackupIndex\":5,\"maxFileSize\":\"100Mi\"},\"storage\":{\"className\":\"standard\",\"selectorLabels\":{},\"size\":\"512Gi\"},\"threshold\":\"ALL\"},\"level\":\"INFO\",\"mdc\":{},\"requestIdHeaderName\":\"Polaris-Request-Id\"} Logging configuration. logging.categories object {\"org.apache.iceberg.rest\":\"INFO\",\"org.apache.polaris\":\"INFO\"} Configuration for specific log categories. logging.console object {\"enabled\":true,\"format\":\"%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n\",\"json\":false,\"threshold\":\"ALL\"} Configuration for the console appender. logging.console.enabled bool true Whether to enable the console appender. logging.console.format string \"%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n\" The log format to use. Ignored if JSON format is enabled. See https://quarkus.io/guides/logging#logging-format for details. logging.console.json bool false Whether to log in JSON format. logging.console.threshold string \"ALL\" The log level of the console appender. logging.file object {\"enabled\":false,\"fileName\":\"polaris.log\",\"format\":\"%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n\",\"json\":false,\"logsDir\":\"/deployments/logs\",\"rotation\":{\"fileSuffix\":null,\"maxBackupIndex\":5,\"maxFileSize\":\"100Mi\"},\"storage\":{\"className\":\"standard\",\"selectorLabels\":{},\"size\":\"512Gi\"},\"threshold\":\"ALL\"} Configuration for the file appender. logging.file.enabled bool false Whether to enable the file appender. logging.file.fileName string \"polaris.log\" The log file name. logging.file.format string \"%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] [%X{requestId},%X{realmId}] [%X{traceId},%X{parentId},%X{spanId},%X{sampled}] (%t) %s%e%n\" The log format to use. Ignored if JSON format is enabled. See https://quarkus.io/guides/logging#logging-format for details. logging.file.json bool false Whether to log in JSON format. logging.file.logsDir string \"/deployments/logs\" The local directory where log files are stored. The persistent volume claim will be mounted here. logging.file.rotation object {\"fileSuffix\":null,\"maxBackupIndex\":5,\"maxFileSize\":\"100Mi\"} Log rotation configuration. logging.file.rotation.fileSuffix string nil An optional suffix to append to the rotated log files. If present, the rotated log files will be grouped in time buckets, and each bucket will contain at most maxBackupIndex files. The suffix must be in a date-time format that is understood by DateTimeFormatter. If the suffix ends with .gz or .zip, the rotated files will also be compressed using the corresponding algorithm. logging.file.rotation.maxBackupIndex int 5 The maximum number of backup files to keep. logging.file.rotation.maxFileSize string \"100Mi\" The maximum size of the log file before it is rotated. Should be expressed as a Kubernetes quantity. logging.file.storage object {\"className\":\"standard\",\"selectorLabels\":{},\"size\":\"512Gi\"} The log storage configuration. A persistent volume claim will be created using these settings. logging.file.storage.className string \"standard\" The storage class name of the persistent volume claim to create. logging.file.storage.selectorLabels object {} Labels to add to the persistent volume claim spec selector; a persistent volume with matching labels must exist. Leave empty if using dynamic provisioning. logging.file.storage.size string \"512Gi\" The size of the persistent volume claim to create. logging.file.threshold string \"ALL\" The log level of the file appender. logging.level string \"INFO\" The log level of the root category, which is used as the default log level for all categories. logging.mdc object {} Configuration for MDC (Mapped Diagnostic Context). Values specified here will be added to the log context of all incoming requests and can be used in log patterns. logging.requestIdHeaderName string \"Polaris-Request-Id\" The header name to use for the request ID. managementService object {\"annotations\":{},\"clusterIP\":\"None\",\"externalTrafficPolicy\":null,\"internalTrafficPolicy\":null,\"ports\":[{\"name\":\"polaris-mgmt\",\"nodePort\":null,\"port\":8182,\"protocol\":null,\"targetPort\":null}],\"sessionAffinity\":null,\"trafficDistribution\":null,\"type\":\"ClusterIP\"} Management service settings. These settings are used to configure liveness and readiness probes, and to configure the dedicated headless service that will expose health checks and metrics, e.g. for metrics scraping and service monitoring. managementService.annotations object {} Annotations to add to the service. managementService.clusterIP string \"None\" By default, the management service is headless, i.e. it does not have a cluster IP. This is generally the right option for exposing health checks and metrics, e.g. for metrics scraping and service monitoring. managementService.ports list [{\"name\":\"polaris-mgmt\",\"nodePort\":null,\"port\":8182,\"protocol\":null,\"targetPort\":null}] The ports the management service will listen on. At least one port is required; the first port implicitly becomes the HTTP port that the application will use for serving management requests. By default, it’s 8182. Note: port names must be unique and no more than 15 characters long. managementService.ports[0] object {\"name\":\"polaris-mgmt\",\"nodePort\":null,\"port\":8182,\"protocol\":null,\"targetPort\":null} The name of the management port. Required. managementService.ports[0].nodePort string nil The port on each node on which this service is exposed when type is NodePort or LoadBalancer. Usually assigned by the system. If not specified, a port will be allocated if this Service requires one. If this field is specified when creating a Service which does not need it, creation will fail. managementService.ports[0].port int 8182 The port the management service listens on. By default, the management interface is exposed on HTTP port 8182. managementService.ports[0].protocol string nil The IP protocol for this port. Supports “TCP”, “UDP”, and “SCTP”. Default is TCP. managementService.ports[0].targetPort string nil Number or name of the port to access on the pods targeted by the service. If this is a string, it will be looked up as a named port in the target Pod’s container ports. If this is not specified, the value of the ‘port’ field is used. managementService.type string \"ClusterIP\" The type of service to create. Valid values are: ExternalName, ClusterIP, NodePort, and LoadBalancer. The default value is ClusterIP. metrics.enabled bool true Specifies whether metrics for the polaris server should be enabled. metrics.tags object {} Additional tags (dimensional labels) to add to the metrics. nodeSelector object {} Node labels which must match for the polaris pod to be scheduled on that node. See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector. oidc object {\"authServeUrl\":null,\"client\":{\"id\":\"polaris\",\"secret\":{\"key\":\"clientSecret\",\"name\":null}},\"principalMapper\":{\"idClaimPath\":null,\"nameClaimPath\":null,\"type\":\"default\"},\"principalRolesMapper\":{\"filter\":null,\"mappings\":[],\"rolesClaimPath\":null,\"type\":\"default\"}} Polaris OIDC configuration. Only relevant when at least one realm is configured for external (or mixed) authentication. The currently supported configuration is for a single, default OIDC tenant. For more complex scenarios, including OIDC multi-tenancy, you will need to provide the relevant configuration using the advancedConfig section. oidc.authServeUrl string nil The authentication server URL. Must be provided if at least one realm is configured for external authentication. oidc.client object {\"id\":\"polaris\",\"secret\":{\"key\":\"clientSecret\",\"name\":null}} The client to use when authenticating with the authentication server. oidc.client.id string \"polaris\" The client ID to use when contacting the authentication server’s introspection endpoint in order to validate tokens. oidc.client.secret object {\"key\":\"clientSecret\",\"name\":null} The secret to pull the client secret from. If no client secret is required, leave the secret name unset. oidc.client.secret.key string \"clientSecret\" The key name inside the secret to pull the client secret from. oidc.client.secret.name string nil The name of the secret to pull the client secret from. If not provided, the client is assumed to not require a client secret when contacting the introspection endpoint. oidc.principalMapper object {\"idClaimPath\":null,\"nameClaimPath\":null,\"type\":\"default\"} Principal mapping configuration. oidc.principalMapper.idClaimPath string nil The path to the claim that contains the principal ID. Nested paths can be expressed using “/” as a separator, e.g. “polaris/principal_id” would look for the “principal_id” field inside the “polaris” object in the token claims. Optional. Either this option or nameClaimPath (or both) must be provided. oidc.principalMapper.nameClaimPath string nil The claim that contains the principal name. Nested paths can be expressed using “/” as a separator, e.g. “polaris/principal_name” would look for the “principal_name” field inside the “polaris” object in the token claims. Optional. Either this option or idClaimPath (or both) must be provided. oidc.principalMapper.type string \"default\" The PrincipalMapper implementation to use. Only one built-in type is supported: default. oidc.principalRolesMapper object {\"filter\":null,\"mappings\":[],\"rolesClaimPath\":null,\"type\":\"default\"} Principal roles mapping configuration. oidc.principalRolesMapper.filter string nil A regular expression that matches the role names in the identity. Only roles that match this regex will be included in the Polaris-specific roles. oidc.principalRolesMapper.mappings list [] A list of regex mappings that will be applied to each role name in the identity. This can be used to transform the role names in the identity into role names as expected by Polaris. The default Authenticator expects the security identity to expose role names in the format POLARIS_ROLE:\u003crole name\u003e. oidc.principalRolesMapper.rolesClaimPath string nil The path to the claim that contains the principal roles. Nested paths can be expressed using “/” as a separator, e.g. “polaris/principal_roles” would look for the “principal_roles” field inside the “polaris” object in the token claims. If not set, Quarkus looks for roles in standard locations. See https://quarkus.io/guides/security-oidc-bearer-token-authentication#token-claims-and-security-identity-roles. oidc.principalRolesMapper.type string \"default\" The PrincipalRolesMapper implementation to use. Only one built-in type is supported: default. persistence object {\"relationalJdbc\":{\"secret\":{\"jdbcUrl\":\"jdbcUrl\",\"name\":null,\"password\":\"password\",\"username\":\"username\"}},\"type\":\"in-memory\"} Polaris persistence configuration. persistence.relationalJdbc object {\"secret\":{\"jdbcUrl\":\"jdbcUrl\",\"name\":null,\"password\":\"password\",\"username\":\"username\"}} The configuration for the relational-jdbc persistence manager. persistence.relationalJdbc.secret object {\"jdbcUrl\":\"jdbcUrl\",\"name\":null,\"password\":\"password\",\"username\":\"username\"} The secret name to pull the database connection properties from. persistence.relationalJdbc.secret.jdbcUrl string \"jdbcUrl\" The secret key holding the database JDBC connection URL persistence.relationalJdbc.secret.name string nil The secret name to pull database connection properties from persistence.relationalJdbc.secret.password string \"password\" The secret key holding the database password for authentication persistence.relationalJdbc.secret.username string \"username\" The secret key holding the database username for authentication persistence.type string \"in-memory\" The type of persistence to use. Two built-in types are supported: in-memory and relational-jdbc. The eclipse-link type is also supported but is deprecated. podAnnotations object {} Annotations to apply to polaris pods. podLabels object {} Additional Labels to apply to polaris pods. podSecurityContext object {\"fsGroup\":10001,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}} Security context for the polaris pod. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/. podSecurityContext.fsGroup int 10001 GID 10001 is compatible with Polaris OSS default images; change this if you are using a different image. rateLimiter object {\"tokenBucket\":{\"requestsPerSecond\":9999,\"type\":\"default\",\"window\":\"PT10S\"},\"type\":\"no-op\"} Polaris rate limiter configuration. rateLimiter.tokenBucket object {\"requestsPerSecond\":9999,\"type\":\"default\",\"window\":\"PT10S\"} The configuration for the default rate limiter, which uses the token bucket algorithm with one bucket per realm. rateLimiter.tokenBucket.requestsPerSecond int 9999 The maximum number of requests per second allowed for each realm. rateLimiter.tokenBucket.type string \"default\" The type of the token bucket rate limiter. Only the default type is supported out of the box. rateLimiter.tokenBucket.window string \"PT10S\" The time window. rateLimiter.type string \"no-op\" The type of rate limiter filter to use. Two built-in types are supported: default and no-op. readinessProbe object {\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":10} Configures the readiness probe for polaris pods. readinessProbe.failureThreshold int 3 Minimum consecutive failures for the probe to be considered failed after having succeeded. Minimum value is 1. readinessProbe.initialDelaySeconds int 5 Number of seconds after the container has started before readiness probes are initiated. Minimum value is 0. readinessProbe.periodSeconds int 10 How often (in seconds) to perform the probe. Minimum value is 1. readinessProbe.successThreshold int 1 Minimum consecutive successes for the probe to be considered successful after having failed. Minimum value is 1. readinessProbe.timeoutSeconds int 10 Number of seconds after which the probe times out. Minimum value is 1. realmContext object {\"realms\":[\"POLARIS\"],\"type\":\"default\"} Realm context resolver configuration. realmContext.realms list [\"POLARIS\"] List of valid realms, for use with the default realm context resolver. The first realm in the list is the default realm. Realms not in this list will be rejected. realmContext.type string \"default\" The type of realm context resolver to use. Two built-in types are supported: default and test; test is not recommended for production as it does not perform any realm validation. replicaCount int 1 The number of replicas to deploy (horizontal scaling). Beware that replicas are stateless; don’t set this number \u003e 1 when using in-memory meta store manager. resources object {} Configures the resources requests and limits for polaris pods. We usually recommend not to specify default resources and to leave this as a conscious choice for the user. This also increases chances charts run on environments with little resources, such as Minikube. If you do want to specify resources, uncomment the following lines, adjust them as necessary, and remove the curly braces after ‘resources:’. revisionHistoryLimit string nil The number of old ReplicaSets to retain to allow rollback (if not set, the default Kubernetes value is set to 10). service object {\"annotations\":{},\"clusterIP\":null,\"externalTrafficPolicy\":null,\"internalTrafficPolicy\":null,\"ports\":[{\"name\":\"polaris-http\",\"nodePort\":null,\"port\":8181,\"protocol\":null,\"targetPort\":null}],\"sessionAffinity\":null,\"trafficDistribution\":null,\"type\":\"ClusterIP\"} Polaris main service settings. service.annotations object {} Annotations to add to the service. service.clusterIP string nil You can specify your own cluster IP address If you define a Service that has the .spec.clusterIP set to “None” then Kubernetes does not assign an IP address. Instead, DNS records for the service will return the IP addresses of each pod targeted by the server. This is called a headless service. See https://kubernetes.io/docs/concepts/services-networking/service/#headless-services service.externalTrafficPolicy string nil Controls how traffic from external sources is routed. Valid values are Cluster and Local. The default value is Cluster. Set the field to Cluster to route traffic to all ready endpoints. Set the field to Local to only route to ready node-local endpoints. If the traffic policy is Local and there are no node-local endpoints, traffic is dropped by kube-proxy. service.internalTrafficPolicy string nil Controls how traffic from internal sources is routed. Valid values are Cluster and Local. The default value is Cluster. Set the field to Cluster to route traffic to all ready endpoints. Set the field to Local to only route to ready node-local endpoints. If the traffic policy is Local and there are no node-local endpoints, traffic is dropped by kube-proxy. service.ports list [{\"name\":\"polaris-http\",\"nodePort\":null,\"port\":8181,\"protocol\":null,\"targetPort\":null}] The ports the service will listen on. At least one port is required; the first port implicitly becomes the HTTP port that the application will use for serving API requests. By default, it’s 8181. Note: port names must be unique and no more than 15 characters long. service.ports[0] object {\"name\":\"polaris-http\",\"nodePort\":null,\"port\":8181,\"protocol\":null,\"targetPort\":null} The name of the port. Required. service.ports[0].nodePort string nil The port on each node on which this service is exposed when type is NodePort or LoadBalancer. Usually assigned by the system. If not specified, a port will be allocated if this Service requires one. If this field is specified when creating a Service which does not need it, creation will fail. service.ports[0].port int 8181 The port the service listens on. By default, the HTTP port is 8181. service.ports[0].protocol string nil The IP protocol for this port. Supports “TCP”, “UDP”, and “SCTP”. Default is TCP. service.ports[0].targetPort string nil Number or name of the port to access on the pods targeted by the service. If this is a string, it will be looked up as a named port in the target Pod’s container ports. If this is not specified, the value of the ‘port’ field is used. service.sessionAffinity string nil The session affinity for the service. Valid values are: None, ClientIP. The default value is None. ClientIP enables sticky sessions based on the client’s IP address. This is generally beneficial to Polaris deployments, but some testing may be required in order to make sure that the load is distributed evenly among the pods. Also, this setting affects only internal clients, not external ones. If Ingress is enabled, it is recommended to set sessionAffinity to None. service.trafficDistribution string nil The traffic distribution field provides another way to influence traffic routing within a Kubernetes Service. While traffic policies focus on strict semantic guarantees, traffic distribution allows you to express preferences such as routing to topologically closer endpoints. The only valid value is: PreferClose. The default value is implementation-specific. service.type string \"ClusterIP\" The type of service to create. Valid values are: ExternalName, ClusterIP, NodePort, and LoadBalancer. The default value is ClusterIP. serviceAccount.annotations object {} Annotations to add to the service account. serviceAccount.create bool true Specifies whether a service account should be created. serviceAccount.name string \"\" The name of the service account to use. If not set and create is true, a name is generated using the fullname template. serviceMonitor.enabled bool true Specifies whether a ServiceMonitor for Prometheus operator should be created. serviceMonitor.interval string \"\" The scrape interval; leave empty to let Prometheus decide. Must be a valid duration, e.g. 1d, 1h30m, 5m, 10s. serviceMonitor.labels object {} Labels for the created ServiceMonitor so that Prometheus operator can properly pick it up. serviceMonitor.metricRelabelings list [] Relabeling rules to apply to metrics. Ref https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config. storage object {\"secret\":{\"awsAccessKeyId\":null,\"awsSecretAccessKey\":null,\"gcpToken\":null,\"gcpTokenLifespan\":null,\"name\":null}} Storage credentials for the server. If the following properties are unset, default credentials will be used, in which case the pod must have the necessary permissions to access the storage. storage.secret object {\"awsAccessKeyId\":null,\"awsSecretAccessKey\":null,\"gcpToken\":null,\"gcpTokenLifespan\":null,\"name\":null} The secret to pull storage credentials from. storage.secret.awsAccessKeyId string nil The key in the secret to pull the AWS access key ID from. Only required when using AWS. storage.secret.awsSecretAccessKey string nil The key in the secret to pull the AWS secret access key from. Only required when using AWS. storage.secret.gcpToken string nil The key in the secret to pull the GCP token from. Only required when using GCP. storage.secret.gcpTokenLifespan string nil The key in the secret to pull the GCP token expiration time from. Only required when using GCP. Must be a valid ISO 8601 duration. The default is PT1H (1 hour). storage.secret.name string nil The name of the secret to pull storage credentials from. tasks object {\"maxConcurrentTasks\":null,\"maxQueuedTasks\":null} Polaris asynchronous task executor configuration. tasks.maxConcurrentTasks string nil The maximum number of concurrent tasks that can be executed at the same time. The default is the number of available cores. tasks.maxQueuedTasks string nil The maximum number of tasks that can be queued up for execution. The default is Integer.MAX_VALUE. tolerations list [] A list of tolerations to apply to polaris pods. See https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/. tracing.attributes object {} Resource attributes to identify the polaris service among other tracing sources. See https://opentelemetry.io/docs/reference/specification/resource/semantic_conventions/#service. If left empty, traces will be attached to a service named “Apache Polaris”; to change this, provide a service.name attribute here. tracing.enabled bool false Specifies whether tracing for the polaris server should be enabled. tracing.endpoint string \"http://otlp-collector:4317\" The collector endpoint URL to connect to (required). The endpoint URL must have either the http:// or the https:// scheme. The collector must talk the OpenTelemetry protocol (OTLP) and the port must be its gRPC port (by default 4317). See https://quarkus.io/guides/opentelemetry for more information. tracing.sample string \"1.0d\" Which requests should be sampled. Valid values are: “all”, “none”, or a ratio between 0.0 and “1.0d” (inclusive). E.g. “0.5d” means that 50% of the requests will be sampled. Note: avoid entering numbers here, always prefer a string representation of the ratio. ","categories":"","description":"","excerpt":" A Helm chart for Apache Polaris (incubating).\nHomepage: …","ref":"/in-dev/unreleased/helm/","tags":"","title":"Polaris Helm Chart"},{"body":"This page explains how to configure and use Polaris metastores with either the recommended Relational JDBC or the deprecated EclipseLink persistence backends.\nRelational JDBC This implementation leverages Quarkus for datasource management and supports configuration through environment variables or JVM -D flags at startup. For more information, refer to the Quarkus configuration reference.\nPOLARIS_PERSISTENCE_TYPE=relational-jdbc QUARKUS_DATASOURCE_USERNAME=\u003cyour-username\u003e QUARKUS_DATASOURCE_PASSWORD=\u003cyour-password\u003e QUARKUS_DATASOURCE_JDBC_URL=\u003cjdbc-url-of-postgres\u003e The Relational JDBC metastore currently relies on a Quarkus-managed datasource and supports only PostgresSQL and H2 databases. This limitation is similar to that of EclipseLink, primarily due to underlying schema differences. At this time, official documentation is provided exclusively for usage with PostgreSQL. Please refer to the documentation here: Configure data sources in Quarkus\nAdditionally the retries can be configured via polaris.persistence.relational.jdbc.* properties please ref configuration\nEclipseLink (Deprecated) [!IMPORTANT] Eclipse link is deprecated, its recommend to use Relational JDBC as persistence instead.\nPolaris includes EclipseLink plugin by default with PostgresSQL driver.\nConfigure the polaris.persistence section in your Polaris configuration file (application.properties) as follows:\npolaris.persistence.type=eclipse-link polaris.persistence.eclipselink.configuration-file=/path/to/persistence.xml polaris.persistence.eclipselink.persistence-unit=polaris Alternatively, configuration can also be done with environment variables or system properties. Refer to the [Quarkus Configuration Reference] for more information.\nThe configuration-file option must point to an [EclipseLink configuration file]. This file, named persistence.xml, is used to set up the database connection properties, which can differ depending on the type of database and its configuration.\n[!NOTE] You have to locate the persistence.xml at least two folders down to the root folder, e.g. /deployments/config/persistence.xml is OK, whereas /deployments/persistence.xml will cause an infinity loop. [Quarkus Configuration Reference]: https://quarkus.io/guides/config-reference [EclipseLink configuration file]: https://eclipse.dev/eclipselink/documentation/4.0/solutions/solutions.html#TESTINGJPA002\nPolaris creates and connects to a separate database for each realm. Specifically, the {realm} placeholder in jakarta.persistence.jdbc.url is substituted with the actual realm name, allowing the Polaris server to connect to different databases based on the realm.\n[!NOTE] Some database systems such as Postgres don’t create databases automatically. Database admins need to create them manually before running Polaris server.\nA single persistence.xml can describe multiple persistence units. For example, with both a polaris-dev and polaris persistence unit defined, you could use a single persistence.xml to easily switch between development and production databases. Use the persistence-unit option in the Polaris server configuration to easily switch between persistence units.\nUsing H2 [!IMPORTANT] H2 is an in-memory database and is not suitable for production!\nThe default persistence.xml in Polaris is already configured for H2, but you can easily customize your H2 configuration using the persistence unit template below:\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:h2:file:tmp/polaris_test/filedb_{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"sa\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e To build Polaris with the necessary H2 dependency and start the Polaris service, run the following:\n./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild --rerun \\ -PeclipseLinkDeps=com.h2database:h2:2.3.232 java -Dpolaris.persistence.type=eclipse-link \\ -Dpolaris.persistence.eclipselink.configuration-file=/path/to/persistence.xml \\ -Dpolaris.persistence.eclipselink.persistence-unit=polaris \\ -jar runtime/server/build/quarkus-app/quarkus-run.jar Using Postgres PostgreSQL is included by default in the Polaris server distribution.\nThe following shows a sample configuration for integrating Polaris with Postgres.\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:postgresql://localhost:5432/{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003cproperty name=\"eclipselink.persistence-context.flush-mode\" value=\"auto\"/\u003e \u003cproperty name=\"eclipselink.session.customizer\" value=\"org.apache.polaris.extension.persistence.impl.eclipselink.PolarisEclipseLinkSessionCustomizer\"/\u003e \u003cproperty name=\"eclipselink.transaction.join-existing\" value=\"true\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e ","categories":"","description":"","excerpt":"This page explains how to configure and use Polaris metastores with …","ref":"/in-dev/unreleased/metastores/","tags":"","title":"Metastores"},{"body":"This page documents important configurations for connecting to production database through EclipseLink.\nPolaris Server Configuration Configure the metaStoreManager section in the Polaris configuration (polaris-server.yml by default) as follows:\nmetaStoreManager: type: eclipse-link conf-file: META-INF/persistence.xml persistence-unit: polaris conf-file must point to an EclipseLink configuration file\nBy default, conf-file points to the embedded resource file META-INF/persistence.xml in the polaris-eclipselink module.\nIn order to specify a configuration file outside the classpath, follow these steps.\nPlace persistence.xml into a jar file: jar cvf /tmp/conf.jar persistence.xml Use conf-file: /tmp/conf.jar!/persistence.xml EclipseLink Configuration - persistence.xml The configuration file persistence.xml is used to set up the database connection properties, which can differ depending on the type of database and its configuration.\nCheck out the default persistence.xml for a complete sample for connecting to the file-based H2 database.\nPolaris creates and connects to a separate database for each realm. Specifically, the {realm} placeholder in jakarta.persistence.jdbc.url is substituted with the actual realm name, allowing the Polaris server to connect to different databases based on the realm.\nNote: some database systems such as Postgres don’t create databases automatically. Database admins need to create them manually before running Polaris server.\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:h2:file:tmp/polaris_test/filedb_{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"sa\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e A single persistence.xml can describe multiple persistence units. For example, with both a polaris-dev and polaris persistence unit defined, you could use a single persistence.xml to easily switch between development and production databases. Use persistence-unit in the Polaris server configuration to easily switch between persistence units.\nTo build Polaris with the necessary H2 dependency and start the Polaris service, run the following:\npolaris\u003e ./gradlew --no-daemon --info -PeclipseLink=true -PeclipseLinkDeps=com.h2database:h2:2.3.232 clean shadowJar polaris\u003e java -jar dropwizard/service/build/libs/polaris-dropwizard-service-*.jar server ./polaris-server.yml Postgres The following shows a sample configuration for integrating Polaris with Postgres.\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.jpa.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:postgresql://localhost:5432/{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003cproperty name=\"eclipselink.persistence-context.flush-mode\" value=\"auto\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e To build Polaris with the necessary Postgres dependency and start the Polaris service, run the following:\npolaris\u003e ./gradlew --no-daemon --info -PeclipseLink=true -PeclipseLinkDeps=org.postgresql:postgresql:42.7.4 clean shadowJar polaris\u003e java -jar dropwizard/service/build/libs/polaris-dropwizard-service-*.jar server ./polaris-server.yml ","categories":"","description":"","excerpt":"This page documents important configurations for connecting to …","ref":"/releases/0.9.0/metastores/","tags":"","title":"Metastores"},{"body":"This page explains how to configure and use Polaris metastores with either the recommended Relational JDBC or the deprecated EclipseLink persistence backends.\nRelational JDBC This implementation leverages Quarkus for datasource management and supports configuration through environment variables or JVM -D flags at startup. For more information, refer to the Quarkus configuration reference.\nPOLARIS_PERSISTENCE_TYPE=relational-jdbc QUARKUS_DATASOURCE_DB_KIND=postgresql QUARKUS_DATASOURCE_USERNAME=\u003cyour-username\u003e QUARKUS_DATASOURCE_PASSWORD=\u003cyour-password\u003e QUARKUS_DATASOURCE_JDBC_URL=\u003cjdbc-url-of-postgres\u003e The Relational JDBC metastore currently relies on a Quarkus-managed datasource and supports only PostgresSQL and H2 databases. This limitation is similar to that of EclipseLink, primarily due to underlying schema differences. At this time, official documentation is provided exclusively for usage with PostgreSQL. Please refer to the documentation here: Configure data sources in Quarkus\nAdditionally the retries can be configured via polaris.persistence.relational.jdbc.* properties please ref configuration\nEclipseLink (Deprecated) [!IMPORTANT] Eclipse link is deprecated, its recommend to use Relational JDBC as persistence instead.\nPolaris includes EclipseLink plugin by default with PostgresSQL driver.\nConfigure the polaris.persistence section in your Polaris configuration file (application.properties) as follows:\npolaris.persistence.type=eclipse-link polaris.persistence.eclipselink.configuration-file=/path/to/persistence.xml polaris.persistence.eclipselink.persistence-unit=polaris Alternatively, configuration can also be done with environment variables or system properties. Refer to the [Quarkus Configuration Reference] for more information.\nThe configuration-file option must point to an [EclipseLink configuration file]. This file, named persistence.xml, is used to set up the database connection properties, which can differ depending on the type of database and its configuration.\nNote: You have to locate the persistence.xml at least two folders down to the root folder, e.g. /deployments/config/persistence.xml is OK, whereas /deployments/persistence.xml will cause an infinity loop. [Quarkus Configuration Reference]: https://quarkus.io/guides/config-reference [EclipseLink configuration file]: https://eclipse.dev/eclipselink/documentation/4.0/solutions/solutions.html#TESTINGJPA002\nPolaris creates and connects to a separate database for each realm. Specifically, the {realm} placeholder in jakarta.persistence.jdbc.url is substituted with the actual realm name, allowing the Polaris server to connect to different databases based on the realm.\nNote: some database systems such as Postgres don’t create databases automatically. Database admins need to create them manually before running Polaris server.\nA single persistence.xml can describe multiple persistence units. For example, with both a polaris-dev and polaris persistence unit defined, you could use a single persistence.xml to easily switch between development and production databases. Use the persistence-unit option in the Polaris server configuration to easily switch between persistence units.\nUsing H2 [!IMPORTANT] H2 is an in-memory database and is not suitable for production!\nThe default persistence.xml in Polaris is already configured for H2, but you can easily customize your H2 configuration using the persistence unit template below:\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:h2:file:tmp/polaris_test/filedb_{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"sa\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e To build Polaris with the necessary H2 dependency and start the Polaris service, run the following:\n./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild --rerun \\ -PeclipseLinkDeps=com.h2database:h2:2.3.232 java -Dpolaris.persistence.type=eclipse-link \\ -Dpolaris.persistence.eclipselink.configuration-file=/path/to/persistence.xml \\ -Dpolaris.persistence.eclipselink.persistence-unit=polaris \\ -jar runtime/server/build/quarkus-app/quarkus-run.jar Using Postgres PostgreSQL is included by default in the Polaris server distribution.\nThe following shows a sample configuration for integrating Polaris with Postgres.\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:postgresql://localhost:5432/{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003cproperty name=\"eclipselink.persistence-context.flush-mode\" value=\"auto\"/\u003e \u003cproperty name=\"eclipselink.session.customizer\" value=\"org.apache.polaris.extension.persistence.impl.eclipselink.PolarisEclipseLinkSessionCustomizer\"/\u003e \u003cproperty name=\"eclipselink.transaction.join-existing\" value=\"true\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e ","categories":"","description":"","excerpt":"This page explains how to configure and use Polaris metastores with …","ref":"/releases/1.0.0/metastores/","tags":"","title":"Metastores"},{"body":"This page explains how to configure and use Polaris metastores with either the recommended Relational JDBC or the deprecated EclipseLink persistence backends.\nRelational JDBC This implementation leverages Quarkus for datasource management and supports configuration through environment variables or JVM -D flags at startup. For more information, refer to the Quarkus configuration reference.\nPOLARIS_PERSISTENCE_TYPE=relational-jdbc QUARKUS_DATASOURCE_DB_KIND=postgresql QUARKUS_DATASOURCE_USERNAME=\u003cyour-username\u003e QUARKUS_DATASOURCE_PASSWORD=\u003cyour-password\u003e QUARKUS_DATASOURCE_JDBC_URL=\u003cjdbc-url-of-postgres\u003e The Relational JDBC metastore currently relies on a Quarkus-managed datasource and supports only PostgresSQL and H2 databases. This limitation is similar to that of EclipseLink, primarily due to underlying schema differences. At this time, official documentation is provided exclusively for usage with PostgreSQL. Please refer to the documentation here: Configure data sources in Quarkus\nAdditionally the retries can be configured via polaris.persistence.relational.jdbc.* properties please ref configuration\nEclipseLink (Deprecated) [!IMPORTANT] Eclipse link is deprecated, its recommend to use Relational JDBC as persistence instead.\nPolaris includes EclipseLink plugin by default with PostgresSQL driver.\nConfigure the polaris.persistence section in your Polaris configuration file (application.properties) as follows:\npolaris.persistence.type=eclipse-link polaris.persistence.eclipselink.configuration-file=/path/to/persistence.xml polaris.persistence.eclipselink.persistence-unit=polaris Alternatively, configuration can also be done with environment variables or system properties. Refer to the [Quarkus Configuration Reference] for more information.\nThe configuration-file option must point to an [EclipseLink configuration file]. This file, named persistence.xml, is used to set up the database connection properties, which can differ depending on the type of database and its configuration.\nNote: You have to locate the persistence.xml at least two folders down to the root folder, e.g. /deployments/config/persistence.xml is OK, whereas /deployments/persistence.xml will cause an infinity loop. [Quarkus Configuration Reference]: https://quarkus.io/guides/config-reference [EclipseLink configuration file]: https://eclipse.dev/eclipselink/documentation/4.0/solutions/solutions.html#TESTINGJPA002\nPolaris creates and connects to a separate database for each realm. Specifically, the {realm} placeholder in jakarta.persistence.jdbc.url is substituted with the actual realm name, allowing the Polaris server to connect to different databases based on the realm.\nNote: some database systems such as Postgres don’t create databases automatically. Database admins need to create them manually before running Polaris server.\nA single persistence.xml can describe multiple persistence units. For example, with both a polaris-dev and polaris persistence unit defined, you could use a single persistence.xml to easily switch between development and production databases. Use the persistence-unit option in the Polaris server configuration to easily switch between persistence units.\nUsing H2 [!IMPORTANT] H2 is an in-memory database and is not suitable for production!\nThe default persistence.xml in Polaris is already configured for H2, but you can easily customize your H2 configuration using the persistence unit template below:\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:h2:file:tmp/polaris_test/filedb_{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"sa\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e To build Polaris with the necessary H2 dependency and start the Polaris service, run the following:\n./gradlew \\ :polaris-server:assemble \\ :polaris-server:quarkusAppPartsBuild --rerun \\ -PeclipseLinkDeps=com.h2database:h2:2.3.232 java -Dpolaris.persistence.type=eclipse-link \\ -Dpolaris.persistence.eclipselink.configuration-file=/path/to/persistence.xml \\ -Dpolaris.persistence.eclipselink.persistence-unit=polaris \\ -jar runtime/server/build/quarkus-app/quarkus-run.jar Using Postgres PostgreSQL is included by default in the Polaris server distribution.\nThe following shows a sample configuration for integrating Polaris with Postgres.\n\u003cpersistence-unit name=\"polaris\" transaction-type=\"RESOURCE_LOCAL\"\u003e \u003cprovider\u003eorg.eclipse.persistence.jpa.PersistenceProvider\u003c/provider\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntity\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityActive\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityChangeTracking\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelEntityDropped\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelGrantRecord\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelPrincipalSecrets\u003c/class\u003e \u003cclass\u003eorg.apache.polaris.extension.persistence.impl.eclipselink.models.ModelSequenceId\u003c/class\u003e \u003cshared-cache-mode\u003eNONE\u003c/shared-cache-mode\u003e \u003cproperties\u003e \u003cproperty name=\"jakarta.persistence.jdbc.url\" value=\"jdbc:postgresql://localhost:5432/{realm}\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.user\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.jdbc.password\" value=\"postgres\"/\u003e \u003cproperty name=\"jakarta.persistence.schema-generation.database.action\" value=\"create\"/\u003e \u003cproperty name=\"eclipselink.persistence-context.flush-mode\" value=\"auto\"/\u003e \u003cproperty name=\"eclipselink.session.customizer\" value=\"org.apache.polaris.extension.persistence.impl.eclipselink.PolarisEclipseLinkSessionCustomizer\"/\u003e \u003cproperty name=\"eclipselink.transaction.join-existing\" value=\"true\"/\u003e \u003c/properties\u003e \u003c/persistence-unit\u003e ","categories":"","description":"","excerpt":"This page explains how to configure and use Polaris metastores with …","ref":"/releases/1.0.1/metastores/","tags":"","title":"Metastores"},{"body":" Top ","categories":"","description":"","excerpt":" Top ","ref":"/in-dev/unreleased/polaris-management-service/","tags":"","title":"Apache Polaris Management Service OpenAPI"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/releases/0.9.0/polaris-management-service/","tags":"","title":"Apache Polaris Management Service OpenAPI"},{"body":" Top ","categories":"","description":"","excerpt":" Top ","ref":"/releases/1.0.0/polaris-management-service/","tags":"","title":"Apache Polaris Management Service OpenAPI"},{"body":" Top ","categories":"","description":"","excerpt":" Top ","ref":"/releases/1.0.1/polaris-management-service/","tags":"","title":"Apache Polaris Management Service OpenAPI"},{"body":" Top ","categories":"","description":"","excerpt":" Top ","ref":"/in-dev/unreleased/polaris-catalog-service/","tags":"","title":""},{"body":" Top ","categories":"","description":"","excerpt":" Top ","ref":"/releases/1.0.0/polaris-catalog-service/","tags":"","title":""},{"body":" Top ","categories":"","description":"","excerpt":" Top ","ref":"/releases/1.0.1/polaris-catalog-service/","tags":"","title":""},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/releases/0.9.0/rest-catalog-open-api/","tags":"","title":"Apache Iceberg OpenAPI"},{"body":"This page discusses what can be expected from Apache Polaris as the project evolves.\nUsing Polaris as a Catalog Polaris is primarily intended to be used as a Catalog of Tables and Views. As such, it implements the Iceberg REST Catalog API and its own REST APIs.\nRevisions of the Iceberg REST Catalog API are controlled by the Apache Iceberg community. Polaris attempts to accurately implement this specification. Nonetheless, optional REST Catalog features may or may not be supported immediately. In general, there is no guarantee that Polaris releases always implement the latest version of the Iceberg REST Catalog API.\nAny API under Polaris control that is not in an “experimental” or “beta” state (e.g. the Management API) is maintained as a versioned REST API. New releases of Polaris may include changes to the current version of the API. When that happens those changes are intended to be compatible with prior versions of Polaris clients. Certain endpoints and parameters may be deprecated.\nIn case a major change is required to an API that cannot be implemented in a backward-compatible way, new endpoints (URI paths) may be introduced. New URI “roots” may be introduced too (e.g. api/catalog/v2).\nNote that those “v1”, “v2”, etc. URI path segments are not meant to be 1:1 with Polaris releases or Polaris project version numbers (e.g. a “v2” path segment does not mean that it is added in Polaris 2.0).\nPolaris servers will support deprecated API endpoints / parameters / versions / etc. for some transition period to allow clients to migrate.\nManaging Polaris Database Polaris stores its data in a database, which is sometimes referred to as “Metastore” or “Persistence” in other docs.\nEach Polaris release may support multiple Persistence implementations, for example, “EclipseLink” (deprecated) and “JDBC” (current).\nEach type of Persistence evolves individually. Within each Persistence type, Polaris attempts to support rolling upgrades (both version X and X + 1 servers running at the same time).\nHowever, migrating between different Persistence types is not supported in a rolling upgrade manner (for example, migrating from “EclipseLink” to “JDBC”). Polaris provides tools for migrating between different catalogs and those tools may be used to migrate between different Persistence types as well. Service interruption (downtime) should be expected in those cases.\nUsing Polaris as a Build-Time Dependency Polaris produces several jars. These jars or custom builds of Polaris code may be used in downstream projects according to the terms of the license included into Polaris distributions.\nThe minimal version of the JRE required by Polaris code (compilation target) may be updated in any release. Different Polaris jars may have different minimal JRE version requirements.\nChanges in Java class should be expected at any time regardless of the module name or whether the class / method is public or not.\nThis approach is not meant to discourage the use of Polaris code in downstream projects, but to allow more flexibility in evolving the codebase to support new catalog-level features and improve code efficiency. Maintainers of downstream projects are encouraged to join Polaris mailing lists to monitor project changes, suggest improvements, and engage with the Polaris community in case of specific compatibility concerns.\nSemantic Versioning Polaris strives to follow Semantic Versioning conventions both with respect to REST APIs (beta and experimental APIs excepted), Polaris Policies and user-facing configuration.\nThe following are some examples of Polaris approach to SemVer in REST APIs / configuration. These examples are for illustration purposes and should not be considered to be exhaustive.\nPolaris implementing an optional Iceberg REST Catalog feature that was unimplemented in the previous release is not considered a major change.\nSupporting a new revision of the Iceberg REST Catalog spec in a backward-compatible way is not considered a major change. Specifically, supporting new REST API prefixes (e.g. v2) is not a major change because it does not affect older clients.\nChanging the implementation of an Iceberg REST Catalog feature / endpoint in a non-backward compatible way (e.g. removing or renaming a request parameter) is a major change.\nDropping support for a configuration property with the polaris. name prefix is a major change.\nDropping support for any previously defined Policy type or property is a major change.\nUpgrading Quarkus Runtime to its next major version is a major change (because Quarkus-managed configuration may change).\n","categories":"","description":"","excerpt":"This page discusses what can be expected from Apache Polaris as the …","ref":"/in-dev/unreleased/evolution/","tags":"","title":"Polaris Evolution"},{"body":"This page discusses what can be expected from Apache Polaris as the project evolves.\nUsing Polaris as a Catalog Polaris is primarily intended to be used as a Catalog of Tables and Views. As such, it implements the Iceberg REST Catalog API and its own REST APIs.\nRevisions of the Iceberg REST Catalog API are controlled by the Apache Iceberg community. Polaris attempts to accurately implement this specification. Nonetheless, optional REST Catalog features may or may not be supported immediately. In general, there is no guarantee that Polaris releases always implement the latest version of the Iceberg REST Catalog API.\nAny API under Polaris control that is not in an “experimental” or “beta” state (e.g. the Management API) is maintained as a versioned REST API. New releases of Polaris may include changes to the current version of the API. When that happens those changes are intended to be compatible with prior versions of Polaris clients. Certain endpoints and parameters may be deprecated.\nIn case a major change is required to an API that cannot be implemented in a backward-compatible way, new endpoints (URI paths) may be introduced. New URI “roots” may be introduced too (e.g. api/catalog/v2).\nNote that those “v1”, “v2”, etc. URI path segments are not meant to be 1:1 with Polaris releases or Polaris project version numbers (e.g. a “v2” path segment does not mean that it is added in Polaris 2.0).\nPolaris servers will support deprecated API endpoints / parameters / versions / etc. for some transition period to allow clients to migrate.\nManaging Polaris Database Polaris stores its data in a database, which is sometimes referred to as “Metastore” or “Persistence” in other docs.\nEach Polaris release may support multiple Persistence implementations, for example, “EclipseLink” (deprecated) and “JDBC” (current).\nEach type of Persistence evolves individually. Within each Persistence type, Polaris attempts to support rolling upgrades (both version X and X + 1 servers running at the same time).\nHowever, migrating between different Persistence types is not supported in a rolling upgrade manner (for example, migrating from “EclipseLink” to “JDBC”). Polaris provides tools for migrating between different catalogs and those tools may be used to migrate between different Persistence types as well. Service interruption (downtime) should be expected in those cases.\nUsing Polaris as a Build-Time Dependency Polaris produces several jars. These jars or custom builds of Polaris code may be used in downstream projects according to the terms of the license included into Polaris distributions.\nThe minimal version of the JRE required by Polaris code (compilation target) may be updated in any release. Different Polaris jars may have different minimal JRE version requirements.\nChanges in Java class should be expected at any time regardless of the module name or whether the class / method is public or not.\nThis approach is not meant to discourage the use of Polaris code in downstream projects, but to allow more flexibility in evolving the codebase to support new catalog-level features and improve code efficiency. Maintainers of downstream projects are encouraged to join Polaris mailing lists to monitor project changes, suggest improvements, and engage with the Polaris community in case of specific compatibility concerns.\nSemantic Versioning Polaris strives to follow Semantic Versioning conventions both with respect to REST APIs (beta and experimental APIs excepted), Polaris Policies and user-facing configuration.\nThe following are some examples of Polaris approach to SemVer in REST APIs / configuration. These examples are for illustration purposes and should not be considered to be exhaustive.\nPolaris implementing an optional Iceberg REST Catalog feature that was unimplemented in the previous release is not considered a major change.\nSupporting a new revision of the Iceberg REST Catalog spec in a backward-compatible way is not considered a major change. Specifically, supporting new REST API prefixes (e.g. v2) is not a major change because it does not affect older clients.\nChanging the implementation of an Iceberg REST Catalog feature / endpoint in a non-backward compatible way (e.g. removing or renaming a request parameter) is a major change.\nDropping support for a configuration property with the polaris. name prefix is a major change.\nDropping support for any previously defined Policy type or property is a major change.\nUpgrading Quarkus Runtime to its next major version is a major change (because Quarkus-managed configuration may change).\n","categories":"","description":"","excerpt":"This page discusses what can be expected from Apache Polaris as the …","ref":"/releases/1.0.0/evolution/","tags":"","title":"Polaris Evolution"},{"body":"This page discusses what can be expected from Apache Polaris as the project evolves.\nUsing Polaris as a Catalog Polaris is primarily intended to be used as a Catalog of Tables and Views. As such, it implements the Iceberg REST Catalog API and its own REST APIs.\nRevisions of the Iceberg REST Catalog API are controlled by the Apache Iceberg community. Polaris attempts to accurately implement this specification. Nonetheless, optional REST Catalog features may or may not be supported immediately. In general, there is no guarantee that Polaris releases always implement the latest version of the Iceberg REST Catalog API.\nAny API under Polaris control that is not in an “experimental” or “beta” state (e.g. the Management API) is maintained as a versioned REST API. New releases of Polaris may include changes to the current version of the API. When that happens those changes are intended to be compatible with prior versions of Polaris clients. Certain endpoints and parameters may be deprecated.\nIn case a major change is required to an API that cannot be implemented in a backward-compatible way, new endpoints (URI paths) may be introduced. New URI “roots” may be introduced too (e.g. api/catalog/v2).\nNote that those “v1”, “v2”, etc. URI path segments are not meant to be 1:1 with Polaris releases or Polaris project version numbers (e.g. a “v2” path segment does not mean that it is added in Polaris 2.0).\nPolaris servers will support deprecated API endpoints / parameters / versions / etc. for some transition period to allow clients to migrate.\nManaging Polaris Database Polaris stores its data in a database, which is sometimes referred to as “Metastore” or “Persistence” in other docs.\nEach Polaris release may support multiple Persistence implementations, for example, “EclipseLink” (deprecated) and “JDBC” (current).\nEach type of Persistence evolves individually. Within each Persistence type, Polaris attempts to support rolling upgrades (both version X and X + 1 servers running at the same time).\nHowever, migrating between different Persistence types is not supported in a rolling upgrade manner (for example, migrating from “EclipseLink” to “JDBC”). Polaris provides tools for migrating between different catalogs and those tools may be used to migrate between different Persistence types as well. Service interruption (downtime) should be expected in those cases.\nUsing Polaris as a Build-Time Dependency Polaris produces several jars. These jars or custom builds of Polaris code may be used in downstream projects according to the terms of the license included into Polaris distributions.\nThe minimal version of the JRE required by Polaris code (compilation target) may be updated in any release. Different Polaris jars may have different minimal JRE version requirements.\nChanges in Java class should be expected at any time regardless of the module name or whether the class / method is public or not.\nThis approach is not meant to discourage the use of Polaris code in downstream projects, but to allow more flexibility in evolving the codebase to support new catalog-level features and improve code efficiency. Maintainers of downstream projects are encouraged to join Polaris mailing lists to monitor project changes, suggest improvements, and engage with the Polaris community in case of specific compatibility concerns.\nSemantic Versioning Polaris strives to follow Semantic Versioning conventions both with respect to REST APIs (beta and experimental APIs excepted), Polaris Policies and user-facing configuration.\nThe following are some examples of Polaris approach to SemVer in REST APIs / configuration. These examples are for illustration purposes and should not be considered to be exhaustive.\nPolaris implementing an optional Iceberg REST Catalog feature that was unimplemented in the previous release is not considered a major change.\nSupporting a new revision of the Iceberg REST Catalog spec in a backward-compatible way is not considered a major change. Specifically, supporting new REST API prefixes (e.g. v2) is not a major change because it does not affect older clients.\nChanging the implementation of an Iceberg REST Catalog feature / endpoint in a non-backward compatible way (e.g. removing or renaming a request parameter) is a major change.\nDropping support for a configuration property with the polaris. name prefix is a major change.\nDropping support for any previously defined Policy type or property is a major change.\nUpgrading Quarkus Runtime to its next major version is a major change (because Quarkus-managed configuration may change).\n","categories":"","description":"","excerpt":"This page discusses what can be expected from Apache Polaris as the …","ref":"/releases/1.0.1/evolution/","tags":"","title":"Polaris Evolution"},{"body":" Welcome to the Apache Polaris™ (incubating) web site! Apache Polaris is an open-source, fully-featured catalog for Apache Iceberg™. It implements Iceberg's REST API, enabling seamless multi-engine interoperability across a wide range of platforms, including Apache Doris™, Apache Flink®, Apache Spark™, Dremio® OSS, StarRocks, and Trino. Get Started Join the community!\nChat with users and project developers!\nChat with us\nContributions welcome!\nWe do a Pull Request contributions workflow on GitHub. New users are always welcome!\nPolaris Pull Requests\nJoin our mailing list\nFor announcement and discussions about the project.\nSubscribe\nApache Polaris™ is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF. Apache®, Apache Polaris™, Apache Iceberg™, Apache Spark™ are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. ","categories":"","description":"","excerpt":" Welcome to the Apache Polaris™ (incubating) web site! Apache Polaris …","ref":"/","tags":"","title":"Apache Polaris"},{"body":"The Apache Polaris team is pleased to announce Apache Polaris 1.0.1-incubating.\nThis release is a maintenance release on the 1.0.1-incubating one, fixing a couple of issues on the Helm Chart:\nremove db-kind in Helm Chart add relational-jdbc to Helm Chart This release can be downloaded:\nhttps://polaris.apache.org/downloads/ The artifacts are available on Maven Central.\nThe Docker images are available on Docker Hub:\nhttps://hub.docker.com/r/apache/polaris/tags https://hub.docker.com/r/apache/polaris-admin-tool/tags Enjoy !\nThe Apache Polaris team.\n","categories":"","description":"","excerpt":"The Apache Polaris team is pleased to announce Apache Polaris …","ref":"/blog/2025/08/20/apache-polaris-1.0.1-incubating-has-been-released/","tags":"","title":"Apache Polaris 1.0.1-incubating has been released!"},{"body":"Just a dummy\n","categories":"","description":"","excerpt":"Just a dummy\n","ref":"/blog/2024/10/01/dummy-post/","tags":"","title":"Dummy Post"},{"body":" Release Guide This guide walks you through the release process of the Apache Polaris podling.\nSetup To create a release candidate, you will need:\nyour Apache credentials (for repository.apache.org and dist.apache.org repositories) a GPG key for signing artifacts, published in KEYS file If you haven’t published your GPG key yet, you must publish it before starting the release process:\nsvn co https://dist.apache.org/repos/dist/release/incubator/polaris polaris-dist-release cd polaris-dist-release echo \"\" \u003e\u003e KEYS # append a new line gpg --list-sigs \u003cYOUR KEY ID HERE\u003e \u003e\u003e KEYS # append signatures gpg --armor --export \u003cYOUR KEY ID HERE\u003e \u003e\u003e KEYS # append public key block svn commit -m \"add key for \u003cYOUR NAME HERE\u003e\" To send the key to the Ubuntu key-server, Apache Nexus needs it to validate published artifacts:\ngpg --keyserver hkps://keyserver.ubuntu.com --send-keys \u003cYOUR KEY ID HERE\u003e Dist repository The Apache dist repository (dist.apache.org) is used to populate download.apache.org and archives.apache.org. There are two spaces on dist:\ndev is where you stage the source distribution and other user convenient artifacts (distributions, helm charts, …) release is where the artifacts will be copied when the release vote passed Apache dist is a svn repository, you need your Apache credentials to commit there.\nMaven repository Apache uses Nexus as Maven repository (repository.apache.org) where releases are staged (during vote) and copied to Maven Central when the release vote passed.\nYou have to use Apache credentials on Nexus, configured in ~/.gradle/gradle.properties file using mavenUser and mavenPassword:\napacheUsername=yourApacheId apachePassword=yourPassword Note: an alternative is to use ORG_GRADLE_PROJECT_apacheUsername and ORG_GRADLE_PROJECT_apachePassword environment variables:\nexport ORG_GRADLE_PROJECT_apacheUsername=yourApacheId export ORG_GRADLE_PROJECT_apachePassword=bar PGP signing During release process, the artifacts will be signed with your key, eventually using gpg-agent.\nTo configure gradle to sign the artifacts, you can add the following settings in your ~/.gradle/gradle.properties file:\nsigning.gnupg.keyName=Your Key Name To use gpg instead of gpg2, also set signing.gnupg.executable=gpg.\nFor more information, see the Gradle signing documentation.\nGitHub Repository The release should be executed against https://github.com/apache/polaris.git repository (not a fork). Set it as remote with name apache for release if it’s not already set up.\nCreating a release candidate Initiate a discussion about the release with the community This step can be useful to gather ongoing patches that the community thinks should be in the upcoming release.\nThe communication can be started via a [DISCUSS] mail on the dev@polaris.apache.org mailing list and the desired tickets can be added to the github milestone of the next release.\nNote, creating a milestone in github requires a committer. However, a non-committer can assign tasks to a milestone if added to the list of collaborators in .asf.yaml.\nCreate release branch If it’s the first RC for the release, you have to create a release branch:\ngit branch release/x.y.z git push apache release/x.y.z Go in the branch, and set the target release version:\ngit checkoout release/x.y.z echo \"x.y.z\" \u003e version.txt git commit -a git push Update CHANGELOG.md:\n./gradlew patchChangelog git commit -a git push Note: You should submit a PR to propagate (automated) CHANGELOG updates from the release branch to main.\nIf more changes are cherry-picked for the next RC, and those change introduce CHANGELOG entries, follow this update process:\nManually add an -rcN suffix to the previously generated versioned CHANGELOG section. Rerun the patchChangelog command Manually remove RC sections from the CHANGELOG Submit a PR to propagate CHANGELOG updates from the release branch to main. Note: the CHANGELOG patch commit should probably be the last commit on the release branch when an RC is cut. If more changes are cherry-picked for the next RC, it is best to drop the CHANGELOG patch commit, apply cherry-picks, and re-run patchChangelog.\nNote: You should also submit a PR on main branch to bump the version in the version.txt file.\nCreate release tag On the release branch, you create a tag for the RC:\ngit tag apache-polaris-x.y.z-rci git push apache apache-polaris-x.y.z-rci Switch to the tag:\ngit checkout apache-polaris-x.y.z.rci Verify the build pass This is an optional step, but good to do. The purpose here is to verify the build works fine:\n./gradlew clean build It’s also welcome to verify the regression tests (see regtests/README.md for details).\nBuild and stage the distributions You can now build the source distribution:\n./gradlew build sourceTarball -Prelease -PuseGpgAgent -x test -x intTest The source distribution archives are available in build/distribution folder.\nThe binary distributions (for convenience) are available in:\nruntime/distribution/build/distributions Now, we can stage the artifacts to dist dev repository:\nsvn co https://dist.apache.org/repos/dist/dev/incubator/polaris polaris-dist-dev cd polaris-dist-dev mkdir x.y.z cp /path/to/polaris/github/clone/repo/build/distribution/* x.y.z cp /path/to/polaris/github/clone/repo/runtime/distribution/build/distributions/* x.y.z cp -r /path/to/polaris/github/clone/repo/helm/polaris helm-chart/x.y.z svn add x.y.z svn add helm-chart/x.y.z svn commit -m\"Stage Apache Polaris x.y.z RCx\" Build and stage Maven artifacts You can now build and publish the Maven artifacts on a Nexus staging repository:\n./gradlew publishToApache -Prelease -PuseGpgAgent Next, you have to close the staging repository:\nGo to Nexus and log in In the left menu, click on “Staging Repositories” Select the Polaris repository At the top, select “Close” and follow the instructions In the comment field, use “Apache Polaris x.y.z RCi” Build and staging Docker images You can now publish Docker images on DockerHub:\n./gradlew :polaris-server:assemble :polaris-server:quarkusAppPartsBuild --rerun -Dquarkus.container-image.build=true -Dquarkus.container-image.push=true -Dquarkus.docker.buildx.platform=\"linux/amd64,linux/arm64\" -Dquarkus.container-image.tag=x.y.z-rci ./gradlew :polaris-admin:assemble :polaris-admin:quarkusAppPartsBuild --rerun -Dquarkus.container-image.build=true -Dquarkus.container-image.push=true -Dquarkus.docker.buildx.platform=\"linux/amd64,linux/arm64\" -Dquarkus.container-image.tag=x.y.z-rci Start the vote thread The last step for a release candidate is to create a VOTE thread on the dev mailing list.\nA generated email template is available in the build/distribution folder.\nExample title subject:\n[VOTE] Release Apache Polaris x.y.z (rci) Example content:\nHi everyone, I propose that we release the following RC as the official Apache Polaris x.y.z release. * This corresponds to the tag: apache-polaris-x.y.z-rci * https://github.com/apache/polaris/commits/apache-polaris-x.y.z-rci * https://github.com/apache/polaris/tree/\u003cSHA1\u003e The release tarball, signature, and checksums are here: * https://dist.apache.org/repos/dist/dev/incubator/polaris/x.y.z Helm charts are available on: * https://dist.apache.org/repos/dist/dev/incubator/polaris/helm-chart Docker images: * https://hub.docker.com/r/apache/polaris/tags (x.y.z-rci) * https://hub.docker.com/r/apache/polaris-admin-tool/tags (x.y.z-rci) You can find the KEYS file here: * https://downloads.apache.org/incubator/polaris/KEYS Convenience binary artifacts are staged on Nexus. The Maven repositories URLs are: * https://repository.apache.org/content/repositories/orgapachepolaris-\u003cID\u003e/ Please download, verify, and test. Please vote in the next 72 hours. [ ] +1 Release this as Apache polaris x.y.z [ ] +0 [ ] -1 Do not release this because... Only PPMC members and mentors have binding votes, but other community members are encouraged to cast non-binding votes. This vote will pass if there are 3 binding +1 votes and more binding +1 votes than -1 votes. NB: if this vote passes, a new vote has to be started on the Incubator general mailing list. When a candidate is passed or rejected, reply with the vote result:\n[RESULT][VOTE] Release Apache Polaris x.y.z (rci) Thanks everyone who participated in the vote for Release Apache Polaris x.y.z (rci). The vote result is: +1: a (binding), b (non-binding) +0: c (binding), d (non-binding) -1: e (binding), f (non-binding) A new vote is starting in the Apache Incubator general mailing list. Start a new vote on the Incubator general mailing list As Polaris is an Apache Incubator project, you now have to start a new vote on the Apache Incubator general mailing list.\nYou have to send this email to general@incubator.apache.org:\n[VOTE] Release Apache Polaris x.y.z (rci) Hello everyone, The Apache Polaris community has voted and approved the release of Apache Polaris x.y.z (rci). We now kindly request the IPMC members review and vote for this release. Polaris community vote thread: * https://lists.apache.org/thread/\u003cVOTE THREAD\u003e Vote result thread: * https://lists.apache.org/thread/\u003cVOTE RESULT\u003e The release candidate: * https://dist.apache.org/repos/dist/dev/incubator/polaris/x.y.z Git tag for the release: * https://github.com/apache/polaris/releases/tag/apache-polaris-x.y.z-rci Git commit for the release: * https://github.com/apache/polaris/commit/\u003cCOMMIT\u003e Maven staging repository: * https://repository.apache.org/content/repositories/orgapachepolaris-\u003cID\u003e/ Please download, verify and test. Please vote in the next 72 hours. [ ] +1 approve [ ] +0 no opinion [ ] -1 disapprove with the reason To learn more about apache Polaris, please see https://polaris.apache.org/ Checklist for reference: [ ] Download links are valid. [ ] Checksums and signatures. [ ] LICENSE/NOTICE files exist [ ] No unexpected binary files [ ] All source files have ASF headers [ ] Can compile from source Binding votes are the votes from the IPMC members. Similar to the previous vote, send the result on the Incubator general mailing list:\n[RESULT][VOTE] Release Apache Polaris x.y.z (rci) Hi everyone, This vote passed with the following result: Finishing the release After the release votes passed, you need to release the last candidate’s artifacts.\nPublishing the release First, copy the distribution from the dist dev space to the dist release space:\nsvn mv https://dist.apache.org/repos/dist/dev/incubator/polaris/x.y.z https://dist.apache.org/repos/dist/release/incubator/polaris svn mv https://dist.apache.org/repos/dist/dev/incubator/polaris/helm-chart/x.y.z https://dist.apache.org/repos/dist/release/incubator/polaris/helm-chart Next, add a release tag to the git repository based on the candidate tag:\ngit tag -a apache-polaris-x.y.z apache-polaris-x.y.z-rci Update GitHub with the release: https://github.com/apache/polaris/releases/tag/apache-polaris-x.y.z\nThen release the candidate repository on Nexus.\nPublishing docs Open a PR against branch versioned-docs to publish the documentation Open a PR against the main branch to update website Add download links and release notes in Download page Add the release in the website menu Announcing the release To announce the release, wait until Maven Central has mirrored the artifacts.\nSend a mail to dev@iceberg.apache.org and announce@apache.org:\n[ANNOUNCE] Apache Polaris x.y.z The Apache Polaris team is pleased to announce Apache Polaris x.y.z. \u003cAdd Quick Description of the Release\u003e This release can be downloaded https://www.apache.org/dyn/closer.cgi/incubator/polaris/apache-polaris-x.y.z. Release notes: https://polaris.apache.org/blog/apache-polaris-x.y.z Artifacts are available on Maven Central. Apache Polaris is an open-source, fully-featured catalog for Apache Iceberg™. It implements Iceberg's REST API, enabling seamless multi-engine interoperability across a wide range of platforms, including Apache Doris™, Apache Flink®, Apache Spark™, Dremio®, StarRocks, and Trino. Enjoy ! How to verify a release Validating distributions Release vote email includes links to:\nDistribution archives (source, admin, server) on dist.apache.org Signature files (.asc) Checksum files (.sha512) KEYS file After downloading the distributions archives, signatures, checksums, and KEYS file, here are the instructions on how to verify signatures, checksums.\nVerifying signatures First, import the keys in your local keyring:\ncurl https://downloads.apache.org/incubator/polaris/KEYS -o KEYS gpg --import KEYS Next, verify all .asc files:\ngpg --verify apache-polaris-[...].asc Verifying checksums shasum -a 512 --check apache-polaris-[...].sha512 Verifying build and test In the source distribution:\n./gradlew build Voting Votes are cast by replying on the vote email on the dev mailing list, with either +1, 0, -1.\nIn addition to your vote, it’s customary to specify if your vote is binding or non-binding. Only members of the PPMC and mentors have formally binding votes, and IPMC on the vote on the Incubator general mailing list. If you’re unsure, you can specify that your vote is non-binding. You can find more details on https://www.apache.org/foundation/voting.html.\n","categories":"","description":"","excerpt":" Release Guide This guide walks you through the release process of the …","ref":"/release-guide/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"== Apache Polaris version [unreleased]\nDownload from …\n","categories":"","description":"","excerpt":"== Apache Polaris version [unreleased]\nDownload from …\n","ref":"/in-dev/release_index/","tags":"","title":"POLARIS VERSION INDEX MD TEMPLATE"},{"body":"","categories":"","description":"","excerpt":"","ref":"/releases/","tags":"","title":"Releases"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]