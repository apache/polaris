{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ab2c33-c072-49e9-93de-da24759113f7",
   "metadata": {},
   "source": [
    "# Bootstrap the client with ROOT credentials\n",
    "Using the python client generated from our OpenAPI spec, we generate a token from our root user's credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f982815a-2b48-46ab-96a6-20dad7ec1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.catalog.api.iceberg_catalog_api import IcebergCatalogAPI\n",
    "from polaris.catalog.api.iceberg_o_auth2_api import IcebergOAuth2API\n",
    "from polaris.catalog.api_client import ApiClient as CatalogApiClient\n",
    "from polaris.catalog.api_client import Configuration as CatalogApiClientConfiguration\n",
    "\n",
    "polaris_credential = 'root:s3cr3t' # pragma: allowlist secret\n",
    "\n",
    "client_id, client_secret = polaris_credential.split(\":\")\n",
    "client = CatalogApiClient(CatalogApiClientConfiguration(username=client_id,\n",
    "                                 password=client_secret,\n",
    "                                 host='http://polaris:8181/api/catalog'))\n",
    "\n",
    "oauth_api = IcebergOAuth2API(client)\n",
    "token = oauth_api.get_token(scope='PRINCIPAL_ROLE:ALL',\n",
    "                            client_id=client_id,\n",
    "                          client_secret=client_secret,\n",
    "                          grant_type='client_credentials',\n",
    "                          _headers={'realm': 'default-realm'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21f4a1-4129-4dd8-9a6c-fa6eeabfa56e",
   "metadata": {},
   "source": [
    "# Create our first catalog\n",
    "\n",
    "* Creates a catalog named `polaris_catalog` that writes to a specified location in the Local Filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7a311a-9a55-4ff7-a40e-db3c74c53b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolarisCatalog(type='INTERNAL', name='polaris_demo', properties=CatalogProperties(default_base_location='file:///tmp/polaris/', additional_properties={}), create_timestamp=1748370726917, last_update_timestamp=1748370726917, entity_version=1, storage_config_info=FileStorageConfigInfo(storage_type='FILE', allowed_locations=['file:///tmp', 'file:///tmp/polaris/']))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polaris.management import *\n",
    "\n",
    "client = ApiClient(Configuration(access_token=token.access_token,\n",
    "                                   host='http://polaris:8181/api/management/v1'))\n",
    "root_client = PolarisDefaultApi(client)\n",
    "\n",
    "storage_conf = FileStorageConfigInfo(storage_type=\"FILE\", allowed_locations=[\"file:///tmp\"])\n",
    "catalog_name = 'polaris_demo'\n",
    "catalog = Catalog(name=catalog_name, type='INTERNAL', properties={\"default-base-location\": \"file:///tmp/polaris/\"},\n",
    "                storage_config_info=storage_conf)\n",
    "catalog.storage_config_info = storage_conf\n",
    "root_client.create_catalog(create_catalog_request=CreateCatalogRequest(catalog=catalog))\n",
    "resp = root_client.get_catalog(catalog_name=catalog.name)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521039f-c25d-4baa-96ae-a4408c0fced0",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e42c12-4e01-4577-bdf5-90c2704a5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a principal with the given name\n",
    "def create_principal(api, principal_name):\n",
    "  principal = Principal(name=principal_name, type=\"SERVICE\")\n",
    "  try:\n",
    "    principal_result = api.create_principal(CreatePrincipalRequest(principal=principal))\n",
    "    return principal_result\n",
    "  except ApiException as e:\n",
    "    if e.status == 409:\n",
    "      return api.rotate_credentials(principal_name=principal_name)\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "# Create a catalog role with the given name\n",
    "def create_catalog_role(api, catalog, role_name):\n",
    "  catalog_role = CatalogRole(name=role_name)\n",
    "  try:\n",
    "    api.create_catalog_role(catalog_name=catalog.name, create_catalog_role_request=CreateCatalogRoleRequest(catalog_role=catalog_role))\n",
    "    return api.get_catalog_role(catalog_name=catalog.name, catalog_role_name=role_name)\n",
    "  except ApiException as e:\n",
    "    return api.get_catalog_role(catalog_name=catalog.name, catalog_role_name=role_name)\n",
    "  else:\n",
    "    raise e\n",
    "\n",
    "# Create a principal role with the given name\n",
    "def create_principal_role(api, role_name):\n",
    "  principal_role = PrincipalRole(name=role_name)\n",
    "  try:\n",
    "    api.create_principal_role(CreatePrincipalRoleRequest(principal_role=principal_role))\n",
    "    return api.get_principal_role(principal_role_name=role_name)\n",
    "  except ApiException as e:\n",
    "    return api.get_principal_role(principal_role_name=role_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c250ca-7161-418e-bc52-8bbd88a3e57c",
   "metadata": {},
   "source": [
    "# Create a new Principal, Principal Role, and Catalog Role\n",
    "The new Principal belongs to the `engineer` principal role, which has `CATALOG_MANAGE_CONTENT` privileges on the `polaris_catalog`. \n",
    "\n",
    "\n",
    "`CATALOG_MANAGE_CONTENT` has create/list/read/write privileges on all entities within the catalog. The same privilege could be granted to a namespace, in which case, the engineers could create/list/read/write any entity under that namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ceb5ca-f977-46c7-b2a6-07dda59e8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the engineer_principal\n",
    "engineer_principal = create_principal(root_client, \"principle\")\n",
    "\n",
    "# Create the principal role\n",
    "engineer_role = create_principal_role(root_client, \"engineer\")\n",
    "\n",
    "# Create the catalog role\n",
    "manager_catalog_role = create_catalog_role(root_client, catalog, \"manage_catalog\")\n",
    "\n",
    "# Grant the catalog role to the principal role\n",
    "# All principals in the principal role have the catalog role's privileges\n",
    "root_client.assign_catalog_role_to_principal_role(principal_role_name=engineer_role.name,\n",
    "                                                  catalog_name=catalog.name,\n",
    "                                                  grant_catalog_role_request=GrantCatalogRoleRequest(catalog_role=manager_catalog_role))\n",
    "\n",
    "# Assign privileges to the catalog role\n",
    "# Here, we grant CATALOG_MANAGE_CONTENT\n",
    "root_client.add_grant_to_catalog_role(catalog.name, manager_catalog_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.CATALOG_MANAGE_CONTENT)))\n",
    "\n",
    "# Assign the principal role to the principal\n",
    "root_client.assign_principal_role(engineer_principal.principal.name, grant_principal_role_request=GrantPrincipalRoleRequest(principal_role=engineer_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04cf15-a327-4ab9-a083-6da4e7dd1623",
   "metadata": {},
   "source": [
    "# Create a reader Principal, Principal Role, and Catalog Role\n",
    "This new principal belongs to the `product_manager` principal role, which is explicitly granted read and list permissions on the catalog.\n",
    "\n",
    "Permissions cascade, so permissions granted at the catalog level are inherited by namespaces and tables within the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51a6433-99c9-46c5-a855-928e30bad6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reader principal\n",
    "reader_principal = create_principal(root_client, \"mlee\")\n",
    "\n",
    "# Create the principal role\n",
    "pm_role = create_principal_role(root_client, \"product_manager\")\n",
    "\n",
    "# Create the catalog role\n",
    "read_only_role = create_catalog_role(root_client, catalog, \"read_only\")\n",
    "\n",
    "# Grant the catalog role to the principal role\n",
    "root_client.assign_catalog_role_to_principal_role(principal_role_name=pm_role.name,\n",
    "                                                  catalog_name=catalog.name,\n",
    "                                                  grant_catalog_role_request=GrantCatalogRoleRequest(catalog_role=read_only_role))\n",
    "\n",
    "# Assign privileges to the catalog role\n",
    "# Here, the catalog role is granted READ and LIST privileges at the catalog level\n",
    "# Privileges cascade down\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.TABLE_LIST)))\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.TABLE_READ_PROPERTIES)))\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.TABLE_READ_DATA)))\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.VIEW_LIST)))\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.VIEW_READ_PROPERTIES)))\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.NAMESPACE_READ_PROPERTIES)))\n",
    "root_client.add_grant_to_catalog_role(catalog.name, read_only_role.name,\n",
    "                                      AddGrantRequest(grant=CatalogGrant(catalog_name=catalog.name,\n",
    "                                                                       type='catalog',\n",
    "                                                                       privilege=CatalogPrivilege.NAMESPACE_LIST)))\n",
    "\n",
    "# Assign the principal role to the principal\n",
    "root_client.assign_principal_role(reader_principal.principal.name, grant_principal_role_request=GrantPrincipalRoleRequest(principal_role=pm_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1e2b3-a0d4-49b5-8e1e-ddb43f98b115",
   "metadata": {},
   "source": [
    "# Create a Spark session with the engineer credentials\n",
    "\n",
    "* Catalog URI points to our Polaris installation\n",
    "* Credential set using the client_id and client_secret generated for the principal\n",
    "* Scope set to `PRINCIPAL_ROLE:ALL`\n",
    "* `X-Iceberg-Access-Delegation` is set to vended-credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd13f24b-9d59-470d-9be1-660c22dde680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .config(\"spark.jars\", \"../polaris_libs/polaris-iceberg-1.9.0-spark-runtime-3.5_2.12-0.11.0-beta-incubating-SNAPSHOT.jar\")\n",
    "  .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-aws-bundle:1.9.0,io.delta:delta-spark_2.12:3.2.1\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "  .config('spark.sql.iceberg.vectorization.enabled', 'false')\n",
    "\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension\")\n",
    "  # Configure the 'polaris' catalog as an Iceberg rest catalog\n",
    "  .config(\"spark.sql.catalog.polaris\", \"org.apache.polaris.spark.SparkCatalog\")\n",
    "  # Specify the rest catalog endpoint       \n",
    "  .config(\"spark.sql.catalog.polaris.uri\", \"http://polaris:8181/api/catalog\")\n",
    "  # Enable token refresh\n",
    "  .config(\"spark.sql.catalog.polaris.token-refresh-enabled\", \"true\")\n",
    "  # specify the client_id:client_secret pair\n",
    "  .config(\"spark.sql.catalog.polaris.credential\", f\"{engineer_principal.credentials.client_id}:{engineer_principal.credentials.client_secret}\")\n",
    "\n",
    "  # Set the warehouse to the name of the catalog we created\n",
    "  .config(\"spark.sql.catalog.polaris.warehouse\", catalog_name)\n",
    "\n",
    "  # Scope set to PRINCIPAL_ROLE:ALL\n",
    "  .config(\"spark.sql.catalog.polaris.scope\", 'PRINCIPAL_ROLE:ALL')\n",
    "\n",
    "  # Enable access credential delegation\n",
    "  .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", 'vended-credentials')\n",
    "\n",
    "  # AWS configuration\n",
    "  .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "  .config(\"spark.sql.catalog.polaris.io-impl\", \"org.apache.iceberg.io.ResolvingFileIO\")\n",
    "  .config(\"spark.sql.catalog.polaris.s3.region\", \"us-west-2\")\n",
    "  .config(\"spark.history.fs.logDirectory\", \"/home/iceberg/spark-events\")).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfef99-4a52-433b-ac1d-c92db5f396a3",
   "metadata": {},
   "source": [
    "# USE polaris\n",
    "Tell Spark to use the Polaris catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e9e5fb-b22e-4d38-bb1e-4ca78c0d0f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.sql.\n: org.apache.iceberg.exceptions.BadRequestException: Malformed request: unauthorized_client: The client is not authorized\n\tat org.apache.iceberg.rest.ErrorHandlers$OAuthErrorHandler.accept(ErrorHandlers.java:278)\n\tat org.apache.iceberg.rest.ErrorHandlers$OAuthErrorHandler.accept(ErrorHandlers.java:252)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:215)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:299)\n\tat org.apache.iceberg.rest.BaseHTTPClient.postForm(BaseHTTPClient.java:111)\n\tat org.apache.iceberg.rest.auth.OAuth2Util.fetchToken(OAuth2Util.java:267)\n\tat org.apache.iceberg.rest.auth.OAuth2Manager.initSession(OAuth2Manager.java:87)\n\tat org.apache.iceberg.rest.auth.OAuth2Manager.initSession(OAuth2Manager.java:40)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:201)\n\tat org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.polaris.spark.SparkCatalog.initRESTCatalog(SparkCatalog.java:120)\n\tat org.apache.polaris.spark.SparkCatalog.initialize(SparkCatalog.java:134)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SetCatalogAndNamespace.mapChildren(v2Commands.scala:941)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUSE polaris\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOW NAMESPACES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.sql.\n: org.apache.iceberg.exceptions.BadRequestException: Malformed request: unauthorized_client: The client is not authorized\n\tat org.apache.iceberg.rest.ErrorHandlers$OAuthErrorHandler.accept(ErrorHandlers.java:278)\n\tat org.apache.iceberg.rest.ErrorHandlers$OAuthErrorHandler.accept(ErrorHandlers.java:252)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:215)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:299)\n\tat org.apache.iceberg.rest.BaseHTTPClient.postForm(BaseHTTPClient.java:111)\n\tat org.apache.iceberg.rest.auth.OAuth2Util.fetchToken(OAuth2Util.java:267)\n\tat org.apache.iceberg.rest.auth.OAuth2Manager.initSession(OAuth2Manager.java:87)\n\tat org.apache.iceberg.rest.auth.OAuth2Manager.initSession(OAuth2Manager.java:40)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:201)\n\tat org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.polaris.spark.SparkCatalog.initRESTCatalog(SparkCatalog.java:120)\n\tat org.apache.polaris.spark.SparkCatalog.initialize(SparkCatalog.java:134)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SetCatalogAndNamespace.mapChildren(v2Commands.scala:941)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE polaris\")\n",
    "spark.sql(\"SHOW NAMESPACES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c5a4a-d469-4364-9249-1a4aeb4d560c",
   "metadata": {},
   "source": [
    "# Create Nested Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54159ab2-5964-49a0-8202-a4b64ee4f9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      namespace|\n",
      "+---------------+\n",
      "|DELTA_NS.PUBLIC|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS DELTA_NS.PUBLIC\")\n",
    "spark.sql(\"SHOW NAMESPACES IN DELTA_NS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5311e-4a40-4bdc-aaee-b5845e06d020",
   "metadata": {},
   "source": [
    "# Create a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4abc8426-7f2a-4f3f-9e26-1f1824f870c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE NAMESPACE DELTA_NS.PUBLIC\")\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS PEOPLE (\n",
    "    id int, name string)\n",
    "USING delta LOCATION 'file:///tmp/delta_tables/people';\n",
    "\"\"\")\n",
    "# You can also use cloud storage like s3. For eample: s3://<bucket-name>/<path-to-table>\n",
    "# Make the corresponding credentials are set up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13356e64-23ca-4804-a1b9-e9f57f4d14ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|      namespace|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|DELTA_NS.PUBLIC|   PEOPLE|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa7c6c-34e0-4bb9-babc-3f3db4778101",
   "metadata": {},
   "source": [
    "# It's Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5a466d-6a67-4f42-a6a6-ac54ec258e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM PEOPLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b297de-ed11-41df-8ed9-e9396c6c4465",
   "metadata": {},
   "source": [
    "# Insert some records\n",
    "Querying again shows some records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ab2991-6de9-4105-9f95-4c9f1c18f426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  3|jonath|\n",
      "|  1|  anna|\n",
      "|  2|   bob|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO PEOPLE VALUES (1, 'anna'), (2, 'bob'), (3, 'jonath')\")\n",
    "spark.sql(\"SELECT * FROM PEOPLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d49be8-cf02-4b81-82ce-31e1ee46630a",
   "metadata": {},
   "source": [
    "# Create Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb5fd0c-20d4-42ce-a823-b6ae43f58313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE NAMESPACE DELTA_NS.PUBLIC\")\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS COUNTRY (code string, name string) USING iceberg\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54904fd6-96b4-4198-b5b7-2b6a9e6eea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|      namespace|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|DELTA_NS.PUBLIC|  COUNTRY|      false|\n",
      "|DELTA_NS.PUBLIC|   PEOPLE|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae216740-a5cb-4f28-b219-1114d7189546",
   "metadata": {},
   "source": [
    "# Insert values for the iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1e639be-5a3a-41c6-a782-dd939bc2eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|code|         name|\n",
      "+----+-------------+\n",
      "|  US|United States|\n",
      "|  CA|       Canada|\n",
      "|  FR|       France|\n",
      "|  IN|        India|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO COUNTRY VALUES ('US', 'United States'), ('CA', 'Canada'), ('FR', 'France'), ('IN', 'India')\")\n",
    "spark.sql(\"SELECT * FROM COUNTRY\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76a0ba-800b-436c-b617-3725286af58c",
   "metadata": {},
   "source": [
    "# Initiate a new Spark session\n",
    "Change the credentials to the PM's read-only credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f3aac79-bf45-4603-bd64-30eeab4bdfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The new spark session inherits everything from the previous session except for the overridden credentials\n",
    "new_spark = spark.newSession()\n",
    "new_spark.conf.set(\"spark.sql.catalog.polaris.credential\", f\"{reader_principal.credentials.client_id}:{reader_principal.credentials.client_secret}\")\n",
    "new_spark.sql(\"USE polaris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba9acb-2e9d-4ffa-a685-8a85c75f3046",
   "metadata": {},
   "source": [
    "# Show Namespace contents\n",
    "We can still `USE NAMESPACE` and `SHOW TABLES`, which require `READ_NAMESPACE_PROPERTIES` and `LIST_TABLES` privileges respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d517424d-8893-4375-ac3b-c532c8682b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|      namespace|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|DELTA_NS.PUBLIC|  COUNTRY|      false|\n",
      "|DELTA_NS.PUBLIC|   PEOPLE|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_spark.sql(\"USE NAMESPACE DELTA_NS.PUBLIC\")\n",
    "new_spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfba50e-ec5d-41dd-8715-78ea1c1f42e2",
   "metadata": {},
   "source": [
    "# Table reads work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fce4b1f-4d71-4d03-8b60-3e9ca6ca6ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|code|         name|\n",
      "+----+-------------+\n",
      "|  US|United States|\n",
      "|  CA|       Canada|\n",
      "|  FR|       France|\n",
      "|  IN|        India|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_spark.sql(\"SELECT * FROM COUNTRY\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38c9c2-1b9a-4be1-b0ab-42e64c398b78",
   "metadata": {},
   "source": [
    "# Drop table fails\n",
    "NOTE: there is currently no write privilege support for non-iceberg tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c57967-bd4e-4ffb-a01d-0b22c334432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spark.sql(\"DROP TABLE COUNTRY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bf0ac-10d4-4b46-a2dd-b921b2ea99d4",
   "metadata": {},
   "source": [
    "# Drop table using original spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ced0c7cc-1ba6-4711-8645-4ce210491e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE COUNTRY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a32de6ed-96c3-493c-9430-fba91b527929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|      namespace|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|DELTA_NS.PUBLIC|   PEOPLE|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e3f75a9-c95c-4a74-b12f-9630013e2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|      namespace|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|DELTA_NS.PUBLIC|   PEOPLE|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b78a8e3-a9e7-4c7f-a9af-7a91d4644e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE PEOPLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c42563bb-399e-4308-aa23-764192c5005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
